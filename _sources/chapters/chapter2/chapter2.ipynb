{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65a1db7-efa1-4060-aa44-04704dd5bbe9",
   "metadata": {},
   "source": [
    "# Chapter 2 - Transformer Architecture\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "At the heart of any large language model lies the transformers. Proposed by in the seminal paper Attention is all you need by {cite:p}`vaswani2023attentionneed`, it has become the backbone for modern LLM. Original Transformer articulated in the paper is a neural network, consisting of the following modules\n",
    "\n",
    "1. Encoder Module\n",
    "    This is made of several self attention module heads followed by a fully conntected layers.\n",
    "    \n",
    "2. Decoder Module\n",
    "    Similar to an encoder module with attention heads followed by fully connected layers. In addtion to user input, this layer also takes in the embedding output from encoding module.\n",
    "    \n",
    " \n",
    " ```{figure} ../../images/chapter2/Transformer.png\n",
    "---\n",
    "height: 150px\n",
    "name: Transformer Architecture\n",
    "---\n",
    "Transformer Architecture\n",
    "```\n",
    "Transormer architecture is a deep learning neural network trained for language translation tasks. The Attention is all you need paper gives an example of training from english to german and french. You can think of it as two neural networks, an encoder and a decoder connected in a cascading manner to perform the translation task.\n",
    "\n",
    "The encoder converts the whole input english text into an embedded vector represenation. The decoder uses this embedded vector and translates one word a time. There are three types of transformers which evolved quickly after this paper was published.\n",
    "\n",
    "1. Encoder - Decoder transformers\n",
    "    They include both encoders and decoders. Translation models uses these architecture.  T5, BART etc. Their pre-training is task dependent. Tasks that involve both understanding and generating data. They first encode an input sequence into an internal represenation and then decode this represenation into an output sequence.\n",
    "\n",
    "2. Encoder only Transformers  \n",
    "    Models like BERT are encoder only transfomer.Their task involves only understanding. They are trained to do masked word prediction. Given a sentence, one of the word will be masked. Encoder only models are heavily used in classification tasks.\n",
    "    \n",
    "3. Decoder only Transformers\n",
    "    In this book we will be looking at decoder only models. Decoder only models are trained to perform text generation. These models are also called as autoregressive models.\n",
    "    \n",
    "\n",
    "The secret sauce in encoder / decoder architecture is the multi-head attention module.It is this attention mechanism module which claims to provide better contextual information and long term dependency features present in the input text data to the model. Let us lookat the following two examples\n",
    "\n",
    "1. The river bank is not accessible to the tourist.\n",
    "2. The robbers had planned a heist of major banks in the city.\n",
    "\n",
    "\n",
    "Both the example sentences have the word \"bank\". The context in the first sentence is \"river\" and the second sentence is \"heist\".  We want the model to treat the word \"bank\" with respect to their context in the sentence. Another example,\n",
    "\n",
    "1. I went to the library yesterday, there i forgot my book. I am returning there today.\n",
    "\n",
    "As an English reader, if I ask you a question \"Were am I returning?\", you know that I am returning to the library. The previous sentence has the clue. This is a trivial example for long term dependency.\n",
    "\n",
    "\n",
    "This is where attention mechanism comes to rescue. Using attention mechanism we inject contextual information and long term dependency in to the model. We saw word embeddings in the previous chapter. Each word or a token had a unique position in the embedding space. However as we saw in the previous examples, these embeddings need to account for the contextual information in the input text. Attention mechanism adds these contextual information to the vector representaion of the token. More about attention in the subsequent sections.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c2b8e-8c24-4800-90bb-4904b0e7796c",
   "metadata": {},
   "source": [
    "## Why LLMs are decoder Transformers\n",
    "\n",
    "Why most of the LLMs are decoder only models and not encoder-decoder model?\n",
    "\n",
    "According to {cite}'wang2022language' Decoder only models trained on next word prediction objective exibit the strongest zero-shot generalization after purely self-supervised pretraining.\n",
    "\n",
    "Models trained with masked language modeling objective, followed by mutitask finetuning perform the best among our experiments.\n",
    "\n",
    "* Factors to consider while choosing between Decoder only or Encoder Decoder models.\n",
    "\n",
    "  1 cost of training\n",
    "      Decoder only models are cheaper to train. They can be trained on large unsupervised corpus\n",
    "      Encoder Decoder needs a lot of labelled data for  multitask finetuning.\n",
    "  2 Emerging Abilities\n",
    "      phenomenon where models display new, sophisticated capabilities not explicitly taught during training, arising naturally as the model scales in size\n",
    "  and complexity.https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\n",
    "\n",
    "  https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f\n",
    "\n",
    "  emergent abilities reduce the performance gap achieved by encoder decoder models over decoder-only ones with multitask finetuning.\n",
    "\n",
    "  3. In-context learning from prompts\n",
    " \n",
    "      prompt engineering methods provide few-shot examples to help LLM understand the context or task. In context information can be seen to have a similar effect as gradient descent that updates the attention weight of the zero-shot prompt.\n",
    "\n",
    "  4. Efficiency Optimization\n",
    " \n",
    "     In decoder only models, the key and value matrices from previous tokens can be reused for subsequent tokens during the decoding process. Since each position only attends to previous tokens the K and V matrices for those tokens remain unchaged. This caching mechanism improves effieciency by avoiding the recomputation of K and V matrices for tokens that have already been processed, facilitates faster generation and lower computation cost during inference.\n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c8f22",
   "metadata": {},
   "source": [
    "Figure 1 GPT-2 Architecture\n",
    "\n",
    "(reference:preprocess../ing)=\n",
    "```{figure} ../images/chapter2/GPT2.drawio.png\n",
    "---\n",
    "height: 150px\n",
    "name: preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0d145-f78b-4e30-94a4-c47793aa7662",
   "metadata": {},
   "source": [
    "## Scaled dot product self-attention\n",
    "\n",
    "\n",
    "In a given input, words that are highly correlated receive higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96bdc224-0c19-4854-8dad-c06dec51fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class SLLMConfig:\n",
    "    # Embedding dimension\n",
    "    d_model: int = 512\n",
    "    # Query key Value projection dimension\n",
    "    d_head: int  = 128\n",
    "    # bias for query,key and value projection matrices\n",
    "    bias: bool = False\n",
    "    dropout: int = 0.0\n",
    "    # Number of input tokens\n",
    "    context_window: int = 32\n",
    "    # Number of attention heads\n",
    "    n_heads: int = 4\n",
    "\n",
    "    \n",
    "config = SLLMConfig()\n",
    "assert config.d_model % config.n_heads == 0\n",
    "\n",
    "    \n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements weighted self attention\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(config.dropout)\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        attn_score = q @ k.transpose(-2,-1)\n",
    "        \n",
    "        if mask == None:\n",
    "            mask = torch.triu(torch.ones(x.shape[-2], x.shape[-2],device=x.device), diagonal=1)\n",
    "        \n",
    "        masked = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(masked / math.sqrt(k.shape[-1]), dim=1)\n",
    "        attn_weights = self.attn_drop(attn_weights)\n",
    "\n",
    "        context_vector = attn_weights @ v\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c0ba31-b97c-480a-8764-7089cf20af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 128])\n",
      "tensor([-0.0328, -1.4182,  1.4332], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.normal(0.5,0.3,size=(config.context_window, config.d_model))\n",
    "print(x.shape)\n",
    "\n",
    "sha = SingleHeadAttention(config)\n",
    "attn = sha.forward(x)\n",
    "print(attn.shape)\n",
    "print(attn[0,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf806b",
   "metadata": {},
   "source": [
    "### Flash Attention\n",
    "\n",
    "Pytorch scaled_dot_product_attention implements a memory optimized version of self-attention called flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb682147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0328, -1.4182,  1.4332], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "q = sha.Wq(x)\n",
    "k = sha.Wk(x)\n",
    "v = sha.Wv(x)\n",
    "\n",
    "\n",
    "context_vector = scaled_dot_product_attention(q,k,v, attn_mask=None\n",
    "                                              , dropout_p =0.0\n",
    "                                              ,is_causal=True\n",
    "                                              ,scale=math.sqrt(k.shape[-1]))\n",
    "\n",
    "print(context_vector[0,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e853a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "tensor([ 0.6260, -0.1517, -0.5864], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sha1 = SingleHeadAttention1(config)\n",
    "attn = sha1.forward(x)\n",
    "print(attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e55bd9-23e7-4310-ba5d-eb97325c1cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14287/570851606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mMultihead\u001b[0m \u001b[0mAttention\u001b[0m \u001b[0mImplementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config) for _ in range(config.n_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attentions = []\n",
    "        for head in self.heads:\n",
    "            attentions.append(head(x))\n",
    "\n",
    "        context_vector = torch.cat(attentions, dim=-1)\n",
    "        context_projected = self.projection_out(context_vector)\n",
    "        return context_projected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afe0c902-6bfd-4e84-a345-95d98d7c63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 512])\n",
      "torch.Size([250, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_747187/514991502.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.Wq.weight)\n",
      "/tmp/ipykernel_747187/514991502.py:18: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.Wk.weight)\n",
      "/tmp/ipykernel_747187/514991502.py:19: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.Wv.weight)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(config)\n",
    "x = torch.normal(0.5,0.3,size=(config.context_window, config.d_model))\n",
    "print(x.shape)\n",
    "context_vec = mha.forward(x)\n",
    "print(context_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40042e61",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Adjust the the output of the activation to have zero mean and a variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfa2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5976, -0.9168, -0.9624,  0.1854],\n",
      "         [-0.7473,  0.5628, -0.8835,  2.3138],\n",
      "         [-0.3804, -0.3160, -0.0926,  1.0782]],\n",
      "\n",
      "        [[-1.5180,  0.4197, -1.1423,  0.1079],\n",
      "         [ 1.2017, -1.3224,  1.7539,  0.1658],\n",
      "         [ 0.3813,  1.2058,  0.6612, -1.0390]]])\n",
      "tensor([[[-0.0539, -0.7481, -0.8472,  1.6491],\n",
      "         [-0.8229,  0.1953, -0.9287,  1.5563],\n",
      "         [-0.7667, -0.6576, -0.2792,  1.7034]],\n",
      "\n",
      "        [[-1.2077,  1.1685, -0.7469,  0.7862],\n",
      "         [ 0.6420, -1.5130,  1.1135, -0.2425],\n",
      "         [ 0.0952,  1.0895,  0.4328, -1.6176]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch = 2\n",
    "context_window = 3\n",
    "d_model = 4\n",
    "\n",
    "embedding = torch.randn(batch, context_window, d_model)\n",
    "print(embedding)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "normalized = layer_norm(embedding)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e68158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[-0.5728,  0.3114,  0.0723],\n",
      "        [-0.5332,  0.4498,  0.3023]])\n",
      "Var tensor([[0.2819, 2.2072, 0.4649],\n",
      "        [0.8866, 1.8291, 0.9168]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.mean(embedding,dim=-1)}\")\n",
    "print(f\"Var {torch.var(embedding,dim=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb184a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[0., 0., 0.],\n",
      "        [0., -0., -0.]], grad_fn=<RoundBackward0>)\n",
      "Var  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.round(torch.mean(normalized,dim=-1))}\")\n",
    "print(f\"Var  {torch.round(torch.var(normalized,dim=-1))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e86883",
   "metadata": {},
   "source": [
    "weight and bias as scaling and shifting values which are trained as a part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0078a429-a7e7-4ce0-9a76-81a9cc3d7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, ndim, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias   = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd4e5219-2dcf-4abc-9b7d-af0a7f42195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20638b56-bd41-4333-a3a3-dc322829c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = LayerNorm(config.d_model, bias=config.bias)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln2 = LayerNorm(config.d_head, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b5704f8-5df6-4589-bd2e-637674ed8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embdgs = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embdgs   = nn.Embedding(config.context_window, config.d_model)\n",
    "        self.droput       = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "        [TransformerBlock(config) for _ in config.n_layers]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(config.d_head)\n",
    "        self.out_head = nn.Linear(config.d_head, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_length = x.shape\n",
    "        token_embds = self.token_embdgs(x)\n",
    "        pos_embds = self.pos_embds(torch.arange(seq_length, device=x.device))\n",
    "        x = token_embds + pos_embds\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "                                   \n",
    "                                   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94962a1-bd17-431b-97e8-62e50c33d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Improvements to multihead attention\n",
    "\n",
    "1. Matrix Multiplication\n",
    "2. Multi Query Attention\n",
    "4. A\n",
    "5. scald dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba35558-d2c8-4e0a-b914-3ef68143e389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f85193-396b-4dc5-b566-9208ff224db9",
   "metadata": {},
   "source": [
    "## Multi Query Attenion\n",
    "\n",
    "Multi-head attention has multiple attention layers in parallel. Each attention layer has its own linear transformations on the queries, keys and values and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf0efb9-6d19-4a51-996d-5e5ab1138a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14287/2525072156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "from einops import einsum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41971474-76f1-4d01-a90c-020aa16ed7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
