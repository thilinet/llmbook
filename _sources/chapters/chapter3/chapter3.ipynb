{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6583ca-37f6-497a-b6c0-60b63a784164",
   "metadata": {},
   "source": [
    "# Chapter 3 - Pre-train a tiny LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gopi/Documents/small_llm/llmbook/chapters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.absolute())\n",
    "\n",
    "print(parent_path)\n",
    "sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14be95a1-c83d-4508-929a-6840782abf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import os\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe22224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from chapter1.simplebooks import get_dataloaders, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea75c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/gopi/Documents/small_llm/llmbook/data/simplebooks/simplebooks-2-raw/\n",
      "Total train tokens 1676477\n",
      "Total validation tokens 189785\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = get_dataloaders(batch_size=12, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "897e9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter2.gptlikemodel import SLLM, SLLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c2aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLLMConfig(d_model=128, d_head=128, bias=False, dropout=0.0, context_window=50, n_heads=2, vocab_size=52000, n_layers=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SLLMConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ee9b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 13,698,976\n",
      "Model size: 52.26 MB\n"
     ]
    }
   ],
   "source": [
    "model = SLLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "167e3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_trim = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_trim)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e149972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Encoded tensor tensor([[ 3111, 18733,  1316,   357, 14783,    14]])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor {encoded_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19924186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output [3111, 18733, 1316, 357, 14783, 14, 39860, 5082, 25350, 7337, 45039]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "\n",
    "print(\"Output\", out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec72409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: wonderful spring is awaited.Still pictured Scar\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38f4b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 52000])\n"
     ]
    }
   ],
   "source": [
    "class LLMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LLMLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), targets.flatten())\n",
    "        \n",
    "model.eval()        \n",
    "logits = model(encoded_tensor)\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a52d753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2436,  0.4534, -0.2164,  ..., -0.0228,  0.1009,  0.0285],\n",
       "         [-0.5120,  0.6210,  0.3816,  ..., -1.3161,  0.2259,  0.6086],\n",
       "         [-0.6945, -0.5430, -0.2874,  ...,  0.3271,  0.0516,  0.0516],\n",
       "         [-1.0814, -0.7064,  0.9559,  ..., -0.3215, -0.2887, -0.0172],\n",
       "         [-0.0137,  0.5215, -1.4794,  ...,  0.3573, -0.6554, -0.3192],\n",
       "         [ 0.0354, -0.8348, -1.0299,  ...,  0.0443,  0.0224, -0.0852]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5351fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_context = \"nothing is awaited t.let\"\n",
    "target_encoded = tokenizer.encode(target_context)\n",
    "target_tensors = torch.tensor(target_encoded).unsqueeze(0)\n",
    "target_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68a5c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12323,   357, 14783,   257,    14,  1499])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensors.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8db45ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2436,  0.4534, -0.2164,  ..., -0.0228,  0.1009,  0.0285],\n",
       "        [-0.5120,  0.6210,  0.3816,  ..., -1.3161,  0.2259,  0.6086],\n",
       "        [-0.6945, -0.5430, -0.2874,  ...,  0.3271,  0.0516,  0.0516],\n",
       "        [-1.0814, -0.7064,  0.9559,  ..., -0.3215, -0.2887, -0.0172],\n",
       "        [-0.0137,  0.5215, -1.4794,  ...,  0.3573, -0.6554, -0.3192],\n",
       "        [ 0.0354, -0.8348, -1.0299,  ...,  0.0443,  0.0224, -0.0852]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.flatten(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eb796fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8142, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(logits.flatten(0,1), target_tensors.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53fe966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_context = \"sum is awaited.let\"\n",
    "target_encoded = tokenizer.encode(target_context)\n",
    "target_tensors = torch.tensor(target_encoded).unsqueeze(0)\n",
    "target_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afa03885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8329, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(logits.flatten(0,1), target_tensors.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7c94327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LLMLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), targets.flatten())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e865271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/8382 [00:27<?, ?it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2794/2794 [01:57<00:00, 26.24it/s]"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer, scheduler = accelerator.prepare(\n",
    "     train_loader, valid_loader, model, optimizer, lr_scheduler\n",
    " )\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        features,target = batch\n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, target)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b72f3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n"
     ]
    }
   ],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "950329f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: wonderful spring is awaited... the. and\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46910f56",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59578eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = str(Path(current_path.parent.parent.absolute(), \"bin\"))\n",
    "\n",
    "# save state dictionary\n",
    "accelerator.wait_for_everyone()\n",
    "accelerator.save_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66acca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ec3c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_in_model\n",
    "\n",
    "new_model = SLLM(config)\n",
    "device = accelerator.device\n",
    "load_checkpoint_in_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d3be7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Decoded text: wonderful spring is awaited... the. and\n"
     ]
    }
   ],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d320bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
