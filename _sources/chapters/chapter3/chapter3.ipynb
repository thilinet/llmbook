{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6583ca-37f6-497a-b6c0-60b63a784164",
   "metadata": {},
   "source": [
    "# Chapter 3 - Pre-train a tiny LLM\n",
    "\n",
    "In this chapter we will train our tiny LLM using simplebooks dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2c8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.absolute())\n",
    "sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd56d36",
   "metadata": {},
   "source": [
    "Get the train test and validation dataloaders from the code we wrote in previous chapter. We will use the gpt2 tokenizer from Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe22224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gopi/Documents/small_llm/llmbook/chapters/chapter1/simplebooks.py /home/gopi/Documents/small_llm/llmbook\n",
      "Loading dataset from /home/gopi/Documents/small_llm/llmbook/data/simplebooks/simplebooks-2-raw/\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Total train tokens 1,676,477\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Total validation tokens 189,785\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Total test tokens 188,639\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from chapter1.simplebooks import get_dataloaders, get_tokenizer\n",
    "\n",
    "\n",
    "# Load train,validation and test datasets\n",
    "train_loader, valid_loader, test_loader = get_dataloaders(batch_size=12, \\\n",
    "                num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f2470",
   "metadata": {},
   "source": [
    "We have close to 1.7 million tokens for training. 193K tokens for validation and 192K tokenz for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee9b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration SLLMConfig(d_model=128, d_head=128, bias=False, dropout=0.0, context_window=50, n_heads=2, vocab_size=52000, n_layers=2)\n",
      "\n",
      "Total parameters: 13,698,976\n",
      "Model size: 52.26 MB\n"
     ]
    }
   ],
   "source": [
    "from chapter2.gptlikemodel import SLLM, SLLMConfig\n",
    "\n",
    "\n",
    "# Initialize the model class\n",
    "config = SLLMConfig()\n",
    "\n",
    "print(f\"Model configuration {config}\\n\")\n",
    "\n",
    "model = SLLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167e3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    Generate output tokens from a given model.\n",
    "    Arguments:\n",
    "        model: \n",
    "            llm model for text generation\n",
    "        idx:\n",
    "            Input token tensor\n",
    "        max_new_tokens:\n",
    "            Number of output tokens to be generated\n",
    "        context_size:\n",
    "            model context window.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_trim = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_trim)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def invoke_model(model, start_context):\n",
    "    \n",
    "    assert len(start_context) > 0 \\\n",
    "        and start_context is not None\n",
    "        \n",
    "    print(f\"Input context: '{start_context}'\\n\")\n",
    "    tokenizer = get_tokenizer()\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    \n",
    "    # convert to tensor and add batch dimension\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    print(f\"Encoded tensor {encoded_tensor} No Tokens: {encoded_tensor.size()[-1]} \\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "    print(f\"Output {out} No Tokens: {out.size()[-1]}\")\n",
    "    \n",
    "    decoded_text = tokenizer.decode(out.squeeze(0))\n",
    "    print(f\"Decoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607383f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: 'wonderful spring is awaited.'\n",
      "\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Encoded tensor tensor([[ 3111, 18733,  1316,   357, 14783,    14]]) No Tokens: 6 \n",
      "\n",
      "Output tensor([[ 3111, 18733,  1316,   357, 14783,    14, 27755, 25517, 26381,  7351,\n",
      "         47860]]) No Tokens: 11\n",
      "Decoded text: 'wonderful spring is awaited.1400 deckhand hollerinSurely'\n"
     ]
    }
   ],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "model = model.to(\"cpu\")\n",
    "invoke_model(model, start_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e57659",
   "metadata": {},
   "source": [
    "The output tokensize is 11, 5 more than the input token size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class LLMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LLMLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), targets.flatten())\n",
    "        return loss\n",
    "\n",
    "        \n",
    "def batch_loss(loss_fn, input_batch,target_batch, model, device):\n",
    "\n",
    "    assert model is not None\n",
    "    assert input_batch is not None \n",
    "    assert target_batch is not None\n",
    "\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_batch)\n",
    "        loss   = loss_fn(logits, target_batch)\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loader_loss(loss_fn, data_loader, model, device=\"cpu\"):\n",
    "\n",
    "    assert data_loader is not None\n",
    "    assert model is not None\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "\n",
    "        features, target = batch\n",
    "        loss = batch_loss(loss_fn, features, target, model, device)\n",
    "        total_loss+=loss\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf187ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Loss 11.040218353271484\n",
      "Batch 2 Loss 11.003350257873535\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "batch_no = 1\n",
    "for batch in train_loader:\n",
    "    \n",
    "    features, target = batch\n",
    "    loss = batch_loss(loss_fn, features, target, model, device)\n",
    "    \n",
    "    print(f\"Batch {batch_no} Loss {loss}\")\n",
    "    batch_no+=1\n",
    "    \n",
    "    if batch_no > 2:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f87355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loss 11.046782493591309\n"
     ]
    }
   ],
   "source": [
    "train_loss = loader_loss(loss_fn, train_loader, model,device)\n",
    "\n",
    "print(f\"Train data loss {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77116bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0be87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Learning rate warmup\n",
    "\n",
    "n_epochs = 10\n",
    "initial_lr = 1e-4\n",
    "min_lr = 1e-6\n",
    "top_lr = 0.01\n",
    "warmup_steps = 20\n",
    "total_training_steps = n_epochs * len(train_loader)\n",
    "device = \"cuda\"\n",
    "progress_bar = tqdm(range(total_training_steps))\n",
    "eval_freq = 500\n",
    "\n",
    "lr_increment = (top_lr - initial_lr) / warmup_steps\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "global_steps = -1\n",
    "tokens_seen = 0\n",
    "\n",
    "track_lrs = []\n",
    "\n",
    "train_losses = []\n",
    "eval_losses  = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    model.train()\n",
    "    for input_batch in train_loader:\n",
    "        \n",
    "        features, target = input_batch\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        global_steps+=1\n",
    "        \n",
    "        if global_steps < warmup_steps:\n",
    "            lr = initial_lr + global_steps * lr_increment\n",
    "        else:\n",
    "            # cosine decay\n",
    "            progress = (global_steps - warmup_steps) / (total_training_steps - warmup_steps)\n",
    "            lr = min_lr + (top_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            \n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] =lr\n",
    "        \n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, target)\n",
    "        \n",
    "        tokens_seen += features.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if global_steps > warmup_steps:\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        track_lrs.append(lr)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        \"\"\"\n",
    "        if global_steps % eval_freq == 0:\n",
    "            model.eval()\n",
    "            eval_loss = loader_loss(loss_fn, valid_loader, model)\n",
    "            model.train()\n",
    "            print(f\"Epoch {epoch} Evaluation Loss {eval_loss} LR {lr}\")\n",
    "            eval_losses.append((epoch, eval_loss))\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"Epoch {epoch} Avg Train Loss {sum(losses)/len(losses)} LR {lr}\")\n",
    "    invoke_model(model, start_context)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb3f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning rate warmup and Cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e533ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the saved model\n",
    "from chapter2.gptlikemodel import SLLM, SLLMConfig\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = os.getcwd()\n",
    "model_path   = str(Path(current_path).parent.parent.absolute())\n",
    "\n",
    "\n",
    "config = SLLMConfig()\n",
    "model = SLLM(config)\n",
    "\n",
    "save_directory = model_path + \"/bin/\"\n",
    "model_name = \"small_llm-v1-52-0.855\"\n",
    "\n",
    "model.load_state_dict(torch.load(save_directory + model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "304d9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: 'It is a'\n",
      "\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Encoded tensor tensor([[679, 357, 259]]) No Tokens: 3 \n",
      "\n",
      "Output tensor([[ 679,  357,  259, 7485,  397, 7870,   13, 7870]]) No Tokens: 8\n",
      "Decoded text: 'It is a yeast?\" Whoo- Whoo'\n"
     ]
    }
   ],
   "source": [
    "start_context = \"It is a\"\n",
    "invoke_model(model, start_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43815f",
   "metadata": {},
   "source": [
    "# Accelarator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer, scheduler = accelerator.prepare(\n",
    "     train_loader, valid_loader, model, optimizer, lr_scheduler\n",
    " )\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        features,target = batch\n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, target)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950329f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46910f56",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = str(Path(current_path.parent.parent.absolute(), \"bin\"))\n",
    "\n",
    "# save state dictionary\n",
    "accelerator.wait_for_everyone()\n",
    "accelerator.save_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_in_model\n",
    "\n",
    "new_model = SLLM(config)\n",
    "device = accelerator.device\n",
    "load_checkpoint_in_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3be7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e78f8",
   "metadata": {},
   "source": [
    "## Temperature, P- and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059e60a",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a372e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
