{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68a3c1a-91be-4390-adb8-54fe640ea3f6",
   "metadata": {},
   "source": [
    "# Chapter 6 - Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60044624-67fa-490c-a523-096ca49048aa",
   "metadata": {},
   "source": [
    "Unlike other computer programs, where we interact with source code, LLM interactions is through text. The input text we provide to LLM is called as Prompt. Different LLMs are trained to take different length of input tokens. This is called as context window/length. GPT-2.5 Turbo has a context length of 16K. GPT-4 on the ohter hand can ingest upto 128K tokens. Using this prompt the language models makes an inference. The process of deriving the ouptut based on the inference is called as completion.\n",
    "\n",
    "A typical prompt is composed of the following elements\n",
    "\n",
    "* A question or an instruction to the language model\n",
    "* Context information\n",
    "* Any other examples\n",
    "\n",
    "We will begin this chapter with introductions history of prompt and prompt engineering. Prompt engineering is a discipline to help us adapt LLMs for diverse tasks. This is a new paradigm in machine learning. Historically, models were trained to perform a single task restiricted to a single domain. Prompt engineering enables us to use a pre-trained language model adapt itself to perform different tasks. We will intially look at the foundational techniques like zero-shot and few-shot learning. For the purpose of demonstration, we will leverage LangChain PromptTemplates and Ollama models.Post pre-training, during instruction fine tuning phase, models get trained to perform multiple tasks through instruction datasets. Hence they may have different prompting syntax. LangChain provides us with an unified way of interacting with different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df003c-026f-43eb-86d9-29e5e0de75da",
   "metadata": {},
   "source": [
    "## Why prompting\n",
    "\n",
    "Today's software market demands a variety of NLP systems. Starting from grammatical correction, text summariziation, question answering, plaguraism detection, language transalation, sentiment analysis, text classification, and many more. Data collection and labeling for each of these tasks are manual labour intensive. Once a system is developed and deployed, the whole process has to be repeated for creating another system. Even if the other system is not drastically different than the former one. \n",
    "\n",
    "A complex model, when trained with a limited dataset is prone to exploit spurious correlations in the training dataset. Consider the case of a large language models. These models are pre-trained with billions of tokens. However while fine tuning for a specific task, the training data is very limited, in order of magnitude much smaller than the initial data used for pre-training.\n",
    "\n",
    "With a few examples humans can learn new NLP tasks. They don't need to be provided with large amount of traning data. A task speciiced in natrual language, along with a couple of examples is suffiecient.\n",
    "\n",
    "### In-context learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44dc2f5e-f757-4206-9df5-bca2e3b8ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "MODEL = \"phi3:latest\"\n",
    "\n",
    "llm = Ollama(model=MODEL, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95cf16d5-a318-404a-b3fe-eebf0ee74ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, cotton will generally float on the surface of water. This is because cotton fibers are light and have a low density compared to water. Cotton's ability to absorb water can also increase its overall volume without significantly increasing its mass, which further contributes to it floating rather than sinking when placed in water. However, if enough cotton becomes submerged due to absorption of water and the weight increases beyond a certain point, then at that moment, it would start to sink, but typically this happens after significant soaking.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = \"Will cotton sink in water\"\n",
    "llm.invoke(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb714b9-03ea-404c-8d91-254a60720c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, cotton will generally sink in water. Cotton is a natural fiber that is not very dense compared to most materials commonly found in nature or manufactured products. However, it's important to note that the amount of cotton and how it's packed together can affect whether it sinks or floats. If you have a large quantity of loose cotton fibers, they may disperse and float on water due to their low density and surface tension effects. But typically, when considering an individual piece of cotton fabric in still water, it will sink because the overall density is greater than that of water.\n",
      "Yes, cotton will generally float on the surface of water. Cotton is a natural fiber that has a low density and high volume relative to its mass, which causes it to be less dense than water (density of water is approximately 1 gram per cubic centimeter at room temperature). When you place cotton in water, it will not sink but instead disperse on the surface. However, if enough cotton fabric were compacted and folded into a small enough mass with high density, theoretically, it could become heavy enough to sink due to gravity overcoming its buoyancy. But under normal circumstances and for everyday observations, cotton will float in water.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"will {item} sink in water?\")\n",
    "print(llm.invoke(prompt.format(item=\"cotton\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c7eb8",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d465e9c0-51fa-41c7-b67b-a0c5604d0f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['article_type', 'number'], template='Give me {number} prompt string to generate realistic {article_type} titles.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_string = \"Give me {number} prompt string to generate realistic {article_type} titles.\"\n",
    "\n",
    "#\n",
    "# from_template is a classmethod\n",
    "# Returns an instance of PromptTemplate\n",
    "synthetic_prompt = PromptTemplate.from_template(prompt_string, template_format=\"f-string\")\n",
    "\n",
    "synthetic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76505df4-1414-4248-8c15-170f9a1d55c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'fadjective', 'fname', 'name'], template='\\n\\nIs {name} is a planet. Tell me its {adjective} properties\\n\\nIs {fname} is a fruit? Tell me its {fadjective} properties\\n\\n_next_')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"Is {name} is a planet. Tell me its {adjective} properties\", \"Is {fname} is a fruit? Tell me its {fadjective} properties\"]\n",
    "prompt = PromptTemplate.from_examples(examples, suffix=\"_next_\", input_variables=[\"name\",\"adjective\",\"fname\",\"fadjective\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980a207-7bdc-4aa0-bbd2-a0122b2c723b",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd5a40-a4e6-4611-804d-450e1f2ceaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "219fa22d-0cef-42c5-938a-e830defcda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples =[ \n",
    "    {\n",
    "    \"context\": \n",
    "        \"\"\"\"\n",
    "        A \"whatpu\" is a small, furry animal native to Tanzania.\n",
    "        \"\"\",\n",
    "    \"question\":\n",
    "        \"\"\"\n",
    "        An example of a sentence that uses the word whatpu is:\n",
    "        \"\"\",\n",
    "    \"answer\":\n",
    "        \"\"\"\n",
    "        We were traveling in Africa and we saw these very cute whatpus.\n",
    "        \"\"\"\n",
    "},\n",
    "    {\n",
    "    \"context\": \n",
    "        \"\"\"\"\n",
    "        To do a \"farduddle\" means to jump up and down really fast.\n",
    "        \"\"\",\n",
    "    \"question\":\n",
    "        \"\"\"\n",
    "        An example of a sentence that uses the word farduddle is:\n",
    "        \"\"\",\n",
    "    \"answer\":\n",
    "        \"\"\"\n",
    "        When we won the game, we all started to farduddle in celebration.\n",
    "        \"\"\"\n",
    "},  \n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "284a7fc4-cf5c-4aa5-8bf4-fac60fa39e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Single Example ***************\n",
      "context: \"\n",
      "        A \"whatpu\" is a small, furry animal native to Tanzania.\n",
      "        \n",
      " question: \n",
      "        An example of a sentence that uses the word whatpu is:\n",
      "        \n",
      "\n",
      "        We were traveling in Africa and we saw these very cute whatpus.\n",
      "        \n",
      "********** Print Complete Prompt ***************\n",
      "context: \"\n",
      "        A \"whatpu\" is a small, furry animal native to Tanzania.\n",
      "        \n",
      " question: \n",
      "        An example of a sentence that uses the word whatpu is:\n",
      "        \n",
      "\n",
      "        We were traveling in Africa and we saw these very cute whatpus.\n",
      "        \n",
      "\n",
      "context: \"\n",
      "        To do a \"farduddle\" means to jump up and down really fast.\n",
      "        \n",
      " question: \n",
      "        An example of a sentence that uses the word farduddle is:\n",
      "        \n",
      "\n",
      "        When we won the game, we all started to farduddle in celebration.\n",
      "        \n",
      "\n",
      "context: \n",
      "    \"Bodega\" is a slang for small convienience store in new york\n",
      "    \n",
      "question:make an example sentence that uses Bodega\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\",\"answer\"]\n",
    "   ,template = \"context: {context}\\n question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"********** Single Example ***************\")\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "print(\"********** Print Complete Prompt ***************\")\n",
    "\n",
    "context=\"\"\"\n",
    "    \"Bodega\" is a slang for small convienience store in new york\n",
    "    \"\"\"\n",
    "question=\"make an example sentence that uses Bodega\"\n",
    "\n",
    "\n",
    "fewshot_prompt = FewShotPromptTemplate(\n",
    "    examples = examples\n",
    "   ,example_prompt = example_prompt\n",
    "   ,suffix=\"context: {context}\\nquestion:{question}\"\n",
    "   ,input_variables=[\"context\",\"question\"]\n",
    ")\n",
    "\n",
    "print(fewshot_prompt.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15ed182f-4124-4d26-b680-81b992ea5cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After realizing I forgot my wallet, I hurried to the nearest bodega to quickly grab some essentials before heading back home.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(fewshot_prompt.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c987e2-354d-4979-861f-51716ec5c88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a37f269-02d5-4161-924e-4f51e5137ff2",
   "metadata": {},
   "source": [
    "## Algorithimical Optimization of LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be7242-3dc2-4206-ba29-aca88f26c290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
