{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6583ca-37f6-497a-b6c0-60b63a784164",
   "metadata": {},
   "source": [
    "# Chapter 3 - Pre-train a tiny LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14be95a1-c83d-4508-929a-6840782abf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import os\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a265182-49cc-44c0-8c4e-7ae3ce6b45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05265338-071e-4c34-af4d-c79f51277f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb48d529f45485eb803f164e72b59c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28549592fd4b48058898d866a2d30ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aba83f5c1674b518f5853b8e54bf70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720fb5a9a3094ffc9ed0df106dde5517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d5cb357664a2585b4dfcd00db78e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adcccd1c3444daf87d3599cf59b01cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb4ef9970b438eb95be28e1b46c18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085b94b091f74833b32d9a46197c82db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "train_dataset      = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d090b937-7d34-4f4f-acaf-95537a6da3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text sample \n",
      "\n",
      "['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.', 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.']\n",
      "\n",
      " Vocabulary size 50257\n",
      "model max length 1024\n",
      "new model max length 50\n",
      "\n",
      " Encodings \n",
      "\n",
      "{'input_ids': [[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257, 17598, 287, 607, 2119, 13, 1375, 2993, 340, 373, 2408, 284, 711, 351, 340, 780, 340, 373, 7786, 13, 20037, 2227, 284, 2648, 262, 17598, 351, 607, 1995, 11, 523, 673, 714, 34249, 257, 4936, 319, 607, 10147, 13, 198, 198, 43, 813, 1816, 284, 607, 1995, 290, 531, 11, 366, 29252, 11, 314, 1043, 428, 17598, 13, 1680, 345, 2648, 340, 351, 502, 290, 34249, 616, 10147, 1701, 2332, 1995, 13541, 290, 531, 11, 366, 5297, 11, 20037, 11, 356, 460, 2648, 262, 17598, 290, 4259, 534, 10147, 526, 198, 198, 41631, 11, 484, 4888, 262, 17598, 290, 384, 19103, 262, 4936, 319, 20037, 338, 10147, 13, 632, 373, 407, 2408, 329, 606, 780, 484, 547, 7373, 290, 5742, 1123, 584, 13, 2293, 484, 5201, 11, 20037, 26280, 607, 1995, 329, 7373, 262, 17598, 290, 18682, 607, 10147, 13, 1119, 1111, 2936, 3772, 780, 484, 550, 4888, 290, 3111, 1978, 13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257], [7454, 2402, 257, 640, 11, 612, 373, 257, 1310, 1097, 3706, 1355, 538, 13, 1355, 538, 6151, 284, 467, 3049, 290, 711, 287, 262, 4252, 13, 1355, 538, 373, 257, 5448, 1097, 780, 339, 1464, 550, 922, 5252, 13, 4599, 5252, 925, 1355, 538, 3772, 290, 1913, 13, 198, 198, 3198, 1110, 11, 1355, 538, 373, 5059, 287, 262, 3952, 618, 339, 2497, 257, 1263, 5509, 13, 383, 5509, 550, 867, 5667, 326, 547, 7463, 13, 1355, 538, 8288, 703, 262, 5667, 2121, 290, 2227, 284, 711, 351, 606, 13, 1355, 538, 10357, 739, 262, 5509, 290, 7342, 262, 5667, 2121, 319, 683, 13, 679, 13818, 290, 307, 538, 276, 465, 12718, 13, 198, 198, 3856, 538, 2826, 351, 262, 7463, 5667, 477, 1110, 13, 1649, 340, 373, 640, 284, 467, 1363, 11, 1355, 538, 2993, 339, 2622, 517, 5252, 13, 679, 1816, 284, 262, 5252, 1295, 290, 1392, 517, 5448, 5252, 13, 2735, 11, 1355, 538, 373, 3492, 284, 467, 3049, 290, 711, 757, 262, 1306, 1110, 13, 843, 1355, 538, 5615, 18177, 1683, 706, 13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "samples = [sample for sample in train_dataset['text'][0:2]]\n",
    "print(\"Input text sample \\n\")\n",
    "print(samples)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"\\n Vocabulary size {tokenizer.vocab_size}\")\n",
    "print(f\"model max length {tokenizer.model_max_length}\")\n",
    "tokenizer.model_max_length = 50\n",
    "print(f\"new model max length {tokenizer.model_max_length}\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "encodings = tokenizer(samples, padding='max_length', max_length=250, truncation_strategy = \"only_first\")\n",
    "print(\"\\n Encodings \\n\")\n",
    "print(encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a17e95f-c312-472f-9025-d91bcc50db82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed47a9f0-e33a-4e4b-8e61-93eb6eb71836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def collate_fn(samples):\n",
    "    encodings = tokenizer(samples, padding='max_length', max_length=250, truncation_strategy = \"only_first\")\n",
    "    x = encodings['input_ids']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806f8834-4e63-41d1-a0d7-65f1b5db9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SLLMConfig:\n",
    "    context_window: int = 250\n",
    "    vocab_size: int = 50304\n",
    "    n_layers: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 256\n",
    "    n_dkv: int = n_embd\n",
    "    head_size: int = n_embd\n",
    "    dropout: float = 0.2\n",
    "    bias: bool = False\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements weighted self attention\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.Wq =  nn.Linear(config.n_embd, config.head_size, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.n_embd, config.head_size, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.n_embd, config.head_size, bias=config.bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(config.dropout)\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform(self.Wq.weights)\n",
    "        nn.init.xavier_uniform(self.Wk.weights)\n",
    "        nn.init.xavier_uniform(self.Wv.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "\n",
    "        attn = q @ k.transpose(-2,-1)\n",
    "        mask = torch.triu(torch.ones(x.shape[-2], x.shape[-2],device=x.device), diagonal=1)\n",
    "        masked = attn.masked_fill(mask.bool(), -torch.inf)\n",
    "        attn = torch.softmax(masked / math.sqrt(k.shape[-1]), dim=1)\n",
    "        attn = attn @ v\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        return attn\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694df231-e384-46db-bcb6-e22f7439108a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
