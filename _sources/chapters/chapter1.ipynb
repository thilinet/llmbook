{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a015049-4ed3-4fb1-bb2e-892035ee3c35",
   "metadata": {},
   "source": [
    "# Chapter 1 - Input to LLMs\n",
    "\n",
    "\n",
    "<rewrite>\n",
    "All generative LLMs are designed to take in some text and then predict what text is most likely to come after it.\n",
    "\n",
    "\"Base\" models are trained on a wide variety of different texts, so they make minimal assumptions about the structure of the text they're completing.\n",
    "\n",
    "Chat models are created from base models by training them on transcripts of dialogue, so they assume that whatever text they are given is a fragment of dialogue. You can chat with them by filling in one side of a conversation and letting the model fill in the other.\n",
    "\n",
    "Instruct models are trained on instruction–response pairs, so if you give it an instruction, the model assumes that it should continue with a response that obeys the instruction.\n",
    "\n",
    "\n",
    "https://langroid.github.io/langroid/blog/2023/09/19/language-models-completion-and-chat-completion/\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LLM models take input as text and produce output as text. However, deep learning networks cannot work with text symbols. Hence the text\n",
    "must be represented in a continuous space. In this chapter, we will look at how to pre-process text input so it's malleable for \n",
    "a large language model consumption.\n",
    "\n",
    "Figure 1 shows the basic blocks of this operation.\n",
    "(reference:preprocessing)=\n",
    "```{figure} ../images/chapter1/input_creation.png\n",
    "---\n",
    "height: 150px\n",
    "name: preprocessing\n",
    "---\n",
    "Input preprocessing blocks\n",
    "```\n",
    "In subsequent sections we will see each of those boxes in detail. Starting with token encoding, followed by word embedding and finally position embedding. In a nutshell this chapter is about how to prepare the data for ingestion into a large language model. Before we understand the nueances involved in each of these steps,\n",
    "let us do a simple outline of what happens in each of these blocks using a no frill example.\n",
    "\n",
    "(reference:nofrill)=\n",
    "### No frill example\n",
    "\n",
    "A causal large language model, also refered to autoregresive model is trained to do the next word prediction. From a vocabulary of words, given a sequence of words, the causal model predicts the most probable next word from the vocabulary. Causal model are trained on a bunch of documents. Documents are composed of words and each word is composed of characters. In our example, word will be lowest denomination of operation.We use the term corpus to refer to these input documents. The lowest denomination in this corpus is word, typically referred called tokens. The set of unique tokens in the corpus is called vocabulary.\n",
    "\n",
    "Let us take a sample paragraph, our corpus for this exercise, and apply a regular expression to split the paragraph by whitespace or special characters. The resultant list of words forms the tokens for this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45396fd9-52f5-4ae4-ae4b-5639bc6e2374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Broadly', ',', 'physics', 'involves', 'the', 'study', 'of', 'everything', 'in', 'physical', 'existence', ',', 'from', 'the', 'smallest', 'subatomic', 'particles', 'to', 'the', 'entire', 'universe', '.', 'Physicists', 'try', 'to', 'develop', 'conceptual', 'and', 'mathematical', 'models', 'that', 'describe', 'interactions', 'between', 'entities', '(', 'both', 'big', 'and', 'small', ')', 'and', 'that', 'can', 'be', 'used', 'to', 'extend', 'our', 'understanding', 'of', 'how', 'the', 'universe', 'works', 'at', 'different', 'scales', '.', 'Are', 'you', 'interested', 'in', 'studying', 'physics', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# /* Text borrowed from https://www.tntech.edu/cas/physics/aboutphys/about-physics.php */\n",
    "text_corpus = \"Broadly, physics involves the study of everything in physical existence,\" + \\\n",
    "              \"from the smallest subatomic particles to the entire universe. Physicists try \" + \\\n",
    "              \"to develop conceptual and mathematical models that describe interactions between entities \" + \\\n",
    "              \"(both big and small) and that can be used to extend our understanding of how the \"+ \\\n",
    "               \"universe works at different scales. Are you interested in studying physics?\"\n",
    "\n",
    "\n",
    "split_expr = r'([?.#$&*^@,)(]|\\s)'\n",
    "tokens = re.split(split_expr, text_corpus)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6faca69-1f9c-4dff-85f6-939c98cc9531",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "In the above example we have a r before the string. This informs python interpreter to treat\n",
    "blackslash as raw character and not as escape character.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8483f-01b7-4a27-90ae-0a395c2bbbb5",
   "metadata": {},
   "source": [
    "The set of unique tokens forms our vocabulary. Further we assign a unique id for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb5d005-0143-4fd5-b25d-28af94365169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'interested': 1, 'to': 2, 'models': 3, 'small': 4, 'at': 5, 'scales': 6, 'universe': 7, ',': 8, 'entities': 9, 'subatomic': 10, 'that': 11, 'interactions': 12, 'be': 13, 'between': 14, 'particles': 15, 'our': 16, 'used': 17, 'can': 18, '.': 19, 'conceptual': 20, 'of': 21, 'involves': 22, 'everything': 23, 'entire': 24, 'smallest': 25, 'big': 26, 'understanding': 27, 'works': 28, 'mathematical': 29, 'studying': 30, 'Physicists': 31, 'the': 32, 'physical': 33, '(': 34, 'different': 35, 'study': 36, ')': 37, 'describe': 38, 'how': 39, 'try': 40, 'physics': 41, 'develop': 42, 'you': 43, 'existence': 44, '?': 45, 'Broadly': 46, 'from': 47, 'both': 48, 'in': 49, 'extend': 50, 'Are': 51}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {token:token_id for token_id, token in enumerate(set(tokens))}\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50568d-355e-4fe0-a690-cc342962a404",
   "metadata": {},
   "source": [
    "With this vocabulary we can now encode any input string into a list of integers / token ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95d38e-483a-43b3-bfcb-e47cbdd013a0",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Size of the vocabulary plays a great part in building the LLM. The challenge is to have a very compact vocabulary and\n",
    "still try to cover maxium amount of tokens in the corpus and also the token expectd in the future. We will discuss\n",
    "more in this chapter about modeling exercise to build a compact vocabulary. The illustration given here is a very\n",
    "simplified example.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74136a59-eb49-4fed-bc0b-985a16d3b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 28, 5, 35, 6]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"universe works at different scales\"\n",
    "\n",
    "tokens = re.split(split_expr, input_text)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "encoded_input = [vocabulary[token.strip()] for token in tokens]\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4f2a9-cdd4-4aa4-97d0-4da1257a7b74",
   "metadata": {},
   "source": [
    "We have succesfully converted our word tokens into token id. Though this is now in number space, neural networks cannot process it. We need the input in a continous space. Here is where word embedding comes in handy. Let us build a embedding lookup table. The keys of this look up table are our integer word token ids. The value are a continous representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5caac78-f66a-40c4-b334-7b33b9fa2ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 51\n",
      "Word Embedding shape (51, 5)\n",
      "[[0.79061896 0.23025439 0.55998829 0.8374295  0.40650912]\n",
      " [0.28080375 0.6848087  0.3396529  0.17248223 0.29623326]\n",
      " [0.07165954 0.76513322 0.47325477 0.7382186  0.76346754]\n",
      " [0.90756471 0.14178423 0.65096936 0.23639402 0.67450719]\n",
      " [0.37767153 0.42207631 0.68086117 0.59773189 0.78474626]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = max(vocabulary.values())\n",
    "print(f\"vocabulary size {vocab_size}\")\n",
    "\n",
    "embedding_size = 5\n",
    "word_embedding = np.random.uniform(size=(vocab_size, embedding_size))\n",
    "print(f\"Word Embedding shape {word_embedding.shape}\")\n",
    "print(word_embedding[0:5,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2e478-4501-4903-941f-045053edaf77",
   "metadata": {},
   "source": [
    "Here we build an embedding lookup table. Our embedding dimension is set to 5. We create a look up table where rows represent the token and \n",
    "the columns represent the embedding for those words. The embeddings are random real numbers representing the words in a continous space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868b16dd-9435-408f-9dd0-87367482caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "[[0.85432497 0.33774429 0.5163072  0.16599111 0.77619878]\n",
      " [0.8367271  0.02581914 0.05084087 0.15658079 0.86089508]\n",
      " [0.25121756 0.13697472 0.44487938 0.13113087 0.51111093]\n",
      " [0.33662536 0.6617542  0.16918677 0.71635738 0.5714265 ]\n",
      " [0.51201298 0.95930028 0.74252266 0.45792642 0.87873272]]\n"
     ]
    }
   ],
   "source": [
    "input_embedding = word_embedding[encoded_input,:]\n",
    "print(input_embedding.shape)\n",
    "print(input_embedding[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801519f-c91a-41c8-a166-095ebb9efeb4",
   "metadata": {},
   "source": [
    "Word positions carry semantic information. In addition to the words, providing the position of the words\n",
    "will be benefial to the model. Similar to word embedding, we will create a look up for the position embedding.\n",
    "Let us assume a simple case here. The input size to our LLM is fixed, say 10. We will call it as the sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8466282-7650-4711-955e-492f028b725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5235984 , 0.60912799, 0.66000757, 0.87457871, 0.50215997],\n",
       "       [0.28703682, 0.38816409, 0.02410332, 0.85221138, 0.70862048]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "position_embedding_lookup = np.random.uniform(size=(sequence_length, embedding_size))\n",
    "\n",
    "position_index =  np.arange(input_embedding.shape[0])\n",
    "position_embedding = position_embedding_lookup[position_index, :]\n",
    "\n",
    "position_embedding[0:2, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1c3d7-867b-40ec-af5d-ddbaf81060f4",
   "metadata": {},
   "source": [
    "The embedding size is same as the word embedding. Finally we can now add the position embedding to word embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e814e57-776f-44e8-9034-7c8b7d399f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = input_embedding + position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980969ba-389f-48cd-a317-bcc503e41985",
   "metadata": {},
   "source": [
    "Typical of any deep learning model, feature values X and label value Y are fed into Large language model. The main job of a casual model is to predict the next given word. So say if we given \"universe\", based on how its trained it should be returning \"works\".\n",
    "\n",
    "\n",
    "Let us see how we can quickly prepare the input X and the label Y for our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d805d4-b44b-4160-a4cd-b42a0f1e3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a feature : [46, 8, 41, 22, 32, 36, 21, 23, 49, 33]\n",
      "a label   : [8, 41, 22, 32, 36, 21, 23, 49, 33, 44]\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(split_expr, text_corpus)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "token_encoding = [vocabulary[token] for token in tokens]\n",
    "\n",
    "feature_batch = []\n",
    "label_batch = []\n",
    "\n",
    "slide = 1\n",
    "for idx in range(len(tokens) - sequence_length ) :\n",
    "    feature = token_encoding[idx:idx + sequence_length]\n",
    "    label =   token_encoding[idx + slide: idx + slide + sequence_length]\n",
    "\n",
    "    feature_batch.append(feature)\n",
    "    label_batch.append(label)\n",
    "\n",
    "print(f\"a feature : {feature_batch[0]}\")\n",
    "print(f\"a label   : {label_batch[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b3d64-c0ce-4067-950e-12bf4aac7487",
   "metadata": {},
   "source": [
    "Givent he token id 7, we want the LLM to predict 35, now given 35 we want it to predict 16 and so on. By sliding the feature 1 level to the right, we get the token ids for the labels. \n",
    "\n",
    "\n",
    ":::{note}\n",
    "Sliding is a design decision. For demonstration purpose we have used a slide of 1. This may lead to overfitting in some cases.\n",
    ":::\n",
    "\n",
    "With these we can further get the embeddings throught he lookup table we have created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6c4c5-b92a-40f9-aaa6-6c069f015f6c",
   "metadata": {},
   "source": [
    "Hopefully this gives a summary of all the steps involved in preparing the input for a LLM. Rest of the chapter will dwell into the details of token encoding and embeddings. Towards the end of the chapter we will introduce transformer and dataset python library from hugging face which provides convienient implementation of all the topics we discuss here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d6b1a-48dc-416c-9ef2-99b4e1a5a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    ":::{note}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a8813-4bcf-49a5-a694-4d51b3d111b1",
   "metadata": {},
   "source": [
    "## A Sample Text corpus\n",
    "\n",
    "Publicly and privately available LLMs leverage the text data available in world wide web to do the pre-training. In the no frill section, we showed how the features and labels needed to train an LLM comes from the same source, sliding the features leaves us with the label. This can done in an unsupervised manner, saving the labor needed to create large training dataset. In the GPT-1 paper {cite}`radford2018improving`, the authors call this training process as unsupervised pre-training. GPT-1 was trained with Bookcorpus dataset {cite}`zhu2015aligning`.\n",
    "\n",
    "Later in this book,  dwelling about RAG, we have shown a Q&A example with [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "\n",
    "Loading input text from desparate sources is a tedious undertaking. LLMs are trained on Terra Bytes of data. Complex data pipelines are writeen to\n",
    "extract and validate the data. Details of those pipelines are beyond the scope of the book. Here is a quote from {cite}`touvron2023llama`, \"Our training corpus includes a new mix of data from publicly available sources, which does not include data\n",
    "from Meta’s products or services. We made an effort to remove data from certain sites known to contain a\n",
    "high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this\n",
    "provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase\n",
    "knowledge and dampen hallucinations.\"  \n",
    "\n",
    "Read the section about safety in pre-training data to get some more details into the pre-processing works which goes on to accidently prevent sensitive information from leaking into training the LLM.\n",
    "\n",
    "To give an idea about loading the corpus, we will use Simplebooks {cite}`nguyen2019simplebooks`. After downloading the dataset, we will show how to leverage hugginface's dataset libary to load the dataset, let us quickly explore the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4cb4d611-56cb-4a53-8219-3e96cc529fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading and extracting simplebooks.zip\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_simplebooks():\n",
    "    \"\"\"\n",
    "    Download simple books dataset\n",
    "    \"\"\"\n",
    "    url = \"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=\"../data/\")\n",
    "    print(f\"Finished downloading and extracting simplebooks.zip\")\n",
    "                      \n",
    "    \n",
    "download_simplebooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48991e-2846-4c3d-aaaf-edb1859e8b5b",
   "metadata": {},
   "source": [
    "#### Structure of simplebooks\n",
    "\n",
    "From project gutenberg, 1573 books were selected, mostly children book and simplebooks dataset was created.\n",
    "Simplebooks, when downloaded comes with datasets in two sizes.Simplebooks-2 is of size 11MB with a vocabulary size of 11,492 and Simplebooks-92\n",
    "of size roughly 400MB with a vocabulary size of 98,304. Simplebooks-2 has 2.2 M tokens. Compared to llama-2 which uses 2 trillion tokens, Simplebooks\n",
    "is a small dataset which can be used write code to study LLMs.\n",
    "\n",
    "    !ls ../data/simplebooks\n",
    "    README.md  simplebooks-2  simplebooks-2-raw  simplebooks-92  simplebooks-92-raw\n",
    "\n",
    "Both simplebooks-2 and simplebooks-92 has folders with raw suffix. The raw suffixed folders have the data with no changes from gutenberg source. The following normalization were performed on raw suffixed folders and the results are in non raw suffixed folders. \n",
    "\n",
    "1. Spacy was used to tokenize each book. Original case and punctuations were preserved.\n",
    "2. @ was added as separator for numbers. So 300,000 becomes 300 @,@ 000.\n",
    "\n",
    "Each of the folder have train, test and validation split and vocabulary files.\n",
    "\n",
    "    !ls ../data/simplebooks/simplebooks-2\n",
    "    test.txt  train.txt  train.vocab  valid.txt\n",
    "    \n",
    "simplebooks-2 and simplebooks-92 have the cleaned up data. The vocabulary built after applying pre-tokenization on the normalized text is also stored. A quick peek at the train files should show the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dd4539d-eef0-4d8a-bf21-f8ee8f515ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More <unk> Tales\n",
      "\n",
      "By\n",
      "\n",
      "Ellen C. <unk>\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "The Girl Monkey And The <unk> Of <unk>\n",
      "\n",
      "\n",
      "One day the king went for a long walk in the woods . When he came back to his own garden , he sent for his family to come down to the lake for a swim .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 ../data/simplebooks/simplebooks-2/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "801b4093-66d1-43e7-a0d6-62b2d416f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Jataka Tales\n",
      "\n",
      "By\n",
      "\n",
      "Ellen C. Babbitt\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "The Girl Monkey And The String Of Pearls\n",
      "\n",
      "\n",
      "One day the king went for a long walk in the woods. When he came back to his own garden, he sent for his family to come down to the lake for a swim.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 ../data/simplebooks/simplebooks-2-raw/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb7b837a-8058-4b4b-9e5a-7618234d5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in vocabulary 11493\n",
      "sample tokens [',', '.', 'the', '\"', 'and']\n",
      "their encodings [131695, 105703, 98932, 97156, 63612]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "with open(\"../data/simplebooks/simplebooks-2/train.vocab\") as f:\n",
    "    rows = ( line.split('\\t') for line in f )\n",
    "    count = 0\n",
    "    for row in rows:\n",
    "        vocabulary[row[0]] = int(row[1].strip())\n",
    "        count+=1\n",
    "        \n",
    "print(f\"Entries in vocabulary {count}\")\n",
    "print(f\"sample tokens {list(vocabulary.keys())[0:5]}\")\n",
    "print(f\"their encodings {list(vocabulary.values())[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc26a9-fe65-4652-8578-6e8d8477f33f",
   "metadata": {},
   "source": [
    "As a part of pre-tokenization some of the uknown words like Jataka, Babbitt, are replaced by a token \"\"<unk>\"\". More about special tokens later in this chapter. Unnecessary white space are removed, look at the sentence\n",
    "\"own garden , \" is cleaned up to \"own garden,\" Let us peek intot he vocabulary creaated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbf83d-e9e6-466e-bf79-db0caed6630c",
   "metadata": {},
   "source": [
    "Hopefully this gives an idea about input text corpus. We saw that Simplebooks used Spacy to clean up the text as a part of their normalization process and used a whitespace tokenization for thier pre-tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41033c3f-ac5d-4c82-a3fa-51d9e8f6bcf5",
   "metadata": {},
   "source": [
    "## Tokenization Pipeline\n",
    "\n",
    "The tokenization begins with raw input text source / corpus and ends with a dictionary of tokens and their associated token ids. Token ids are integers. After this given a new text, the pipeline should be able to spit out the associated tokens. Similarly, given a list of tokens, the pipeline should be able to convert it back to text without any loss. The below figure illustrates the various steps involved in this pipeline.\n",
    "\n",
    "(reference:tokenization)=\n",
    "```{figure} ../images/chapter1/TokenEncoding.jpg\n",
    "---\n",
    "height: 250px\n",
    "name: encoding\n",
    "---\n",
    "Steps in Tokenization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c978067-2bd6-4812-9c0b-8898ae8ef9a3",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "In the simplebooks example, we saw that unncessary whitespaces were removed and numbers were formatted by inserting '@' at different separators. \n",
    "Typicall normalization involves removing unncessary whitespaces, stripping of accents, lower case conversion and similar others. Here is a list of some normalizer s provided by [HuggingFace Tokenizer library](https://huggingface.co/docs/tokenizers/en/components).\n",
    "\n",
    "1. Unicode normalization (NFD, NFKD, NFC and NFKC algorithms)\n",
    "2. Lowecase conversion\n",
    "3. Stripping white spaces and accents\n",
    "4. Replacing common string patterns\n",
    "\n",
    ":::{admonition} Unicode normalization\n",
    "\n",
    "Unicode encoding involves assigning a numerical value called \"code point\" to each character and transforming them into a series of bytes.\n",
    "Issues may arise when a character can be represented by a single code point or a combination of two code points. Unicode normalization\n",
    "is the process of normalizing a unicode encoded string into a canonical form.\n",
    "\n",
    "For the more curious please read the [article](https://www.smashingmagazine.com/2012/06/all-about-unicode-utf8-character-sets/) to get a little history about ASCII, latin-1, unicode.\n",
    "\n",
    ":::\n",
    "\n",
    "Quoting from GPT-1 paper {cite}`radford2018improving`, \"We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer\".\n",
    "\n",
    "1. [ftfy - fixes text for you](https://ftfy.readthedocs.io/en/latest/index.html)\n",
    "2. [Spacy](https://spacy.io/)\n",
    "\n",
    "Going into the details of ftfy and spacy is beyond the scope of this book. Following code snippets demonstrates the basic usage of these packages. We will discuss Spacy in pre-tokenization section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f69583-f781-43b1-ae16-c9873b13ee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LóPEZ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ftfy\n",
    "\n",
    "ftfy.fix_text(\"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837af13-fbbe-4aa3-ab6a-429006e428e1",
   "metadata": {},
   "source": [
    "The string \"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\" is converted to LoPEZ by ftfy. This package can take care of issues with character decoding. Let us look\n",
    "at a spacy example. After installing spacy, download the tokenizer model to run the following code snipped.\n",
    "\n",
    "    conda install ftfy spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "`````{admonition} Mojibake (文字化け, \"Garbled\") \n",
    ":class: tip\n",
    "Garbled text formed as a result of being decoded using a character encoding with which it was not orignally encoded. \n",
    "A funny poem about Mojibake related to characters printed in a shippling label.\n",
    "\n",
    "(reference:mojibake)=\n",
    "Figure 2 A funny mojibake poem\n",
    "\n",
    "```{figure} ../images/chapter1/shipping-label.png\n",
    "---\n",
    "height: 150px\n",
    "name: shipping-label\n",
    "---\n",
    "Mojibake shipping label\n",
    "```\n",
    "ODE TO A SHIPPING LABEL\n",
    "Once there was a little o,\n",
    "with an accent on top like so\n",
    "\n",
    "It started out as UTF8,\n",
    "but the program only knew latin1,\n",
    "and changed the litte o to A for fun.\n",
    "\n",
    "and it goes on. For the complete [poem](https://imgur.com/4J7Il0m)\n",
    "\n",
    "The text in the label is lopez and due to wrong decoding we have a Mojibake. \n",
    "\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638d5a1-32af-4a90-bf40-bdf572175115",
   "metadata": {},
   "source": [
    "### Pre-tokenization\n",
    "\n",
    "\n",
    "Using a set of rules, the text is split into atomic units, tokens. Imaging this as a superset of tokens fed into the vocabulary building exercise. A subset of these tokens make their way into the final vocabulary. An example pre-tokenizer is  a simple whitespace tokenizer. If two words are separated by a whitespace, they will be treated as two tokens.\n",
    "We saw an example of this in the no frill section. Let us write some python code to implement what we have learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3188fe4-8628-4044-af61-c0485fd90e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gopi/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import spacy\n",
    "\n",
    "class SpacyTokenizer():\n",
    "    \"\"\"\n",
    "    Tokenizer based on Spacy library\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def __call__(self, input_text):\n",
    "\n",
    "        assert len(input_text) > 0\n",
    "\n",
    "        doc = nlp(input_text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "\n",
    "class RegexTokenizer():\n",
    "    \"\"\"\n",
    "    Regex Based Tokenizer\n",
    "    Splits text by eitehr whitespace or by one of these\n",
    "    special characters,?.#$&*^@,\n",
    "    \"\"\"\n",
    "    def __call__(self, input_text):\n",
    "\n",
    "        assert input_text is not None\n",
    "        assert len(input_text) > 0\n",
    "\n",
    "        tokenizer_regex = r'([?.#$&*^@,)(]|\\s)'\n",
    "        tokens = re.split(tokenizer_regex, input_text)\n",
    "        tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "\n",
    "        return tokens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7a941-16c4-455c-a5f5-7227e8d31b1d",
   "metadata": {},
   "source": [
    "The regex based tokenizer, uses the regex expression we introduced in no frills section. Spacy tokenizer uses\n",
    "the Spacy library to tokenize. Let us take a sample from our Simplebooks dataset to see these tokenizers in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8a42d9-912b-49fa-bbb6-e6e41b93bcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['More', 'Jataka', 'Tales'],\n",
       " ['By'],\n",
       " ['Ellen', 'C', '.', 'Babbitt'],\n",
       " ['I'],\n",
       " ['The', 'Girl', 'Monkey', 'And', 'The', 'String', 'Of', 'Pearls'],\n",
       " ['One',\n",
       "  'day',\n",
       "  'the',\n",
       "  'king',\n",
       "  'went',\n",
       "  'for',\n",
       "  'a',\n",
       "  'long',\n",
       "  'walk',\n",
       "  'in',\n",
       "  'the',\n",
       "  'woods',\n",
       "  '.',\n",
       "  'When',\n",
       "  'he',\n",
       "  'came',\n",
       "  'back',\n",
       "  'to',\n",
       "  'his',\n",
       "  'own',\n",
       "  'garden',\n",
       "  ',',\n",
       "  'he',\n",
       "  'sent',\n",
       "  'for',\n",
       "  'his',\n",
       "  'family',\n",
       "  'to',\n",
       "  'come',\n",
       "  'down',\n",
       "  'to',\n",
       "  'the',\n",
       "  'lake',\n",
       "  'for',\n",
       "  'a',\n",
       "  'swim',\n",
       "  '.'],\n",
       " ['When',\n",
       "  'they',\n",
       "  'were',\n",
       "  'all',\n",
       "  'ready',\n",
       "  'to',\n",
       "  'go',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'the',\n",
       "  'queen',\n",
       "  'and',\n",
       "  'her',\n",
       "  'ladies',\n",
       "  'left',\n",
       "  'their',\n",
       "  'jewels',\n",
       "  'in',\n",
       "  'charge',\n",
       "  'of',\n",
       "  'the',\n",
       "  'servants',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'went',\n",
       "  'down',\n",
       "  'into',\n",
       "  'the',\n",
       "  'lake',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_simplebooks(path):\n",
    "    for line in open(path, 'r'):\n",
    "        yield line\n",
    "\n",
    "simplebooks_reader = read_simplebooks('../data/simplebooks/simplebooks-2-raw/train.txt')\n",
    "SAMPLE_SIZE = 15\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "\n",
    "simple_books_sample = [tokenizer(line) for idx, line in enumerate(simplebooks_reader) if idx <= SAMPLE_SIZE and len(line) > 1]\n",
    "simple_books_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7e1f1-8529-4528-b7cb-7c1d8c56fd79",
   "metadata": {},
   "source": [
    "With a sample of 15 sentences we are ready to pass it to our tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ec8fb-d44b-4dce-8639-2c02c1469fa4",
   "metadata": {},
   "source": [
    "### Tokenizer models - Dictionary Training\n",
    "\n",
    "One may wonder the need for any subsequent processing in tokenization pipeline. The pre-tokenization output can be used directly to build a Vocabulary. The set of unique tokens gathered after running the tokenizer over the input corpus is the vocabulary. You are not alone. The transformerXL model {cite}`dai2019transformerxl` has a vocabulary size of 250K, compared to Llama which has a size of 32K\n",
    "\n",
    ":::{note}\n",
    "TransformerXL uses space and punctuation to tokenize the text. Their vocabulary size is around 250K. Here is the link\n",
    "to their paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n",
    "::: \n",
    "\n",
    "\n",
    "A compact vocabulary reduces the model complexity and computation needs to train and perform inference. Special tokens and respective token-ids are added for unknown words. Say an input to the language model contains a word not present in the vocabulary, it will be treatd as unknown and the token id assigned for unknown word will be substituted. A good token encoding pipeline should strive to reduce the number of unknown words. Compact vocabulary and reduced unkown words are two opposite contraints.\n",
    "\n",
    "Word based tokenization suffers from very large vocabulary size, large number of out of vocabulary tokens and different meaning for similar words.\n",
    "Character based tokenization suffers from very large sequences and less meaningful individual tokens.\n",
    "\n",
    "The pre-tokenization leaves us with a superset of all the tokens. The Dictionary training phase involves applying an algorithm to finalize the subset of tokens from this superset to be used for encoding.\n",
    "\n",
    "Dont get it confused by machine learning tranining process. By train, this method is suppose to use a bunch of rules to produce an optimum dictionary. \n",
    "Using rules, the tokens are further split to form a compact vocabulary, at the same time reduce the chances of having unknown token ids. \n",
    "\n",
    "The most commonly used dictionary training approaches are\n",
    "\n",
    "1. BPE - Byte Pair Encoding\n",
    "2. WordPiece\n",
    "3. SentencePiece\n",
    "4. Unigram\n",
    "\n",
    "\n",
    "```{admonition} Character Level Encoding\n",
    "The two biggest challenge with word-level tokenization and are the size of the vocabulary and the number of unknown tokens added as a part of encoding.\n",
    "The vocabulary size has to be very large to decrease the number of uknown token, however it does not guarantee greate reduction of unknown tokens. Words are based on characters, how about we tokenize the individual characters and use an encoding for each character?\n",
    "\n",
    "    input_corpus_encoded = [ord(character) for character in text_corpus]\n",
    "    print(input_corpus_encoded)\n",
    "    assert len(text_corpus) == len(input_corpus_encoded)\n",
    "    \n",
    "    [76, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 44, 32, 116, 104, 101, 32, 110, 101, 119, 32, 107, 105, 100, 32, 105, 110, 32, 116, 104, 101, 32, 98, 108, 111, 99, 107, 32, 105, 115, 32, 99, 114, 101, 97, 116, 105, 110, 103, 32, 119, 111, 110, 100, 101, 114, 115, 46, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 78, 117, 109, 101, 114, 111, 117, 115, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 104, 97, 118, 101, 32, 115, 112, 97, 110, 110, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 108, 97, 115, 116, 32, 116, 119, 111, 32, 121, 101, 97, 114, 115, 32, 108, 101, 118, 97, 114, 97, 103, 105, 110, 103, 32, 108, 108, 109, 115, 46]\n",
    "\n",
    "    decoded_input = [chr(token_id) for token_id in input_corpus_encoded]\n",
    "    print(\"\".join(decoded_input))\n",
    "    \n",
    "    Large language models, the new kid in the block is creating wonders.                 Numerous applications have spanned in the last two years levaraging llms.\n",
    "\n",
    "**Challenges with character level encoding**\n",
    "\n",
    "The context of thw words are lost while doing character level encoding. They may be suitable for small toy llm's for unusable for building systems of any practical use.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0aa0c-f1f7-494c-bcaf-bc77de7c6a2a",
   "metadata": {},
   "source": [
    "#### Bye pair encoding\n",
    "\n",
    "Byte pair encoding was first introduced for word segmentation in the paper Neural Machine Translation of Rare Words with Subword Units {cite}`sennrich2016neural`. It is a sub-word level method. The original algorithm is attribtued to Philip Gage. 1994. A New Algorithm for Data Com-pression. C Users J., 12(2):23–38, February. It is a data compression algorithm working iteratively. Say for example, we have the following string\n",
    "\n",
    "aaaabdaaabac\n",
    "\n",
    "Iteratively, let us now replace the most frequent pairs with another symbol not present in the string. For example, we replace the pair 'aa' with Z. The new\n",
    "string will be ZabdZabac. 'Za' is the most frequently occuring pair now. Let us replace it with X. We continue this way till the string reaches the desired size.\n",
    "Below is the python code to demonstrate this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12510aa4-27f2-4058-a408-a2b3fea94e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      " old aaabdaaabac \n",
      " new ZabdZabac \n",
      " aa replaced by Z\n",
      "Iteration 2 \n",
      " old ZabdZabac \n",
      " new XbdXbac \n",
      " Za replaced by X\n",
      "Iteration 3 \n",
      " old XbdXbac \n",
      " new YdYac \n",
      " Xb replaced by Y\n"
     ]
    }
   ],
   "source": [
    "def bpe_compression(input_str, desired_size=5):\n",
    "    iterations = 0\n",
    "    replace = ['Z','X','Y','L']\n",
    "    replacements = []\n",
    "    while len(input_str) > desired_size:\n",
    "        iterations+=1\n",
    "        pairs = Counter([i + j for i, j in zip(input_str, input_str[1:])])\n",
    "        pair,freq = pairs.most_common(1)[0]\n",
    "        if freq <=1 :\n",
    "            break\n",
    "        old_str = input_str\n",
    "        input_str = input_str.replace(pair, replace[iterations - 1])\n",
    "        replacements.append((replace[iterations -1], pair))\n",
    "        print(f\"Iteration {iterations} \\n old {old_str} \\n new {input_str} \\n {pair} replaced by {replace[iterations-1]}\")\n",
    "    return input_str, replacements\n",
    "\n",
    "input_str, replacements = bpe_compression('aaabdaaabac')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207af50-d212-4483-99f8-91f698941638",
   "metadata": {},
   "source": [
    "In order to reconstruct this, we store all the replacements in a stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe6f4be-2ad1-4a80-b7e0-6b8f8aa7ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Z', 'aa'), ('X', 'Za'), ('Y', 'Xb')]\n"
     ]
    }
   ],
   "source": [
    "print(replacements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8df93-5215-4379-a08c-cbf2161fdb59",
   "metadata": {},
   "source": [
    "Now we can pop up the replacements from the stack and retrieve the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23e81f0c-cd25-4f91-a168-897565e57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(replacements) > 0:\n",
    "    replace_str, _str = replacements.pop()\n",
    "    print(replace_str, _str)\n",
    "    input_str = input_str.replace(replace_str, _str)\n",
    "    print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee868276-0c98-42c2-b772-b4e7b98a878f",
   "metadata": {},
   "source": [
    "BPE begins with the output from pre-tokenizer. For each token, a map of token with its constituent characters followed by a end of word symbol and its frequency in the input corpus are retrieved.\n",
    "\n",
    "Let us see the example from {cite}`sennrich2016neural`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6eb7362-eb00-4088-81e2-5ace75401e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6, 'w i d e s t </w>':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00a7e5-606d-41ed-b984-82cf13b8930d",
   "metadata": {},
   "source": [
    "So given a token 'l o w </w>', we get the list of subsequent character pairs and their frequency. In this case it will be\n",
    "\n",
    "'l o', 'o w' and 'w </w>'. Since 'l o' occurs in 'l o w' and 'l o w e r',its frequency will be 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7b01cbb-7a54-4343-a0ac-0741dbb0954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('l', 'o'): 7,\n",
       "             ('o', 'w'): 7,\n",
       "             ('w', '</w>'): 5,\n",
       "             ('w', 'e'): 8,\n",
       "             ('e', 'r'): 2,\n",
       "             ('r', '</w>'): 2,\n",
       "             ('n', 'e'): 6,\n",
       "             ('e', 'w'): 6,\n",
       "             ('e', 's'): 9,\n",
       "             ('s', 't'): 9,\n",
       "             ('t', '</w>'): 9,\n",
       "             ('w', 'i'): 3,\n",
       "             ('i', 'd'): 3,\n",
       "             ('d', 'e'): 3})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_freq_pairs():\n",
    "    pairs = defaultdict(int)\n",
    "    for token,frequency in vocab.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) -1):\n",
    "            pair = (symbols[i],symbols[i+1])\n",
    "            pairs[pair]+=frequency\n",
    "    return pairs\n",
    "\n",
    "pairs = get_freq_pairs()\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6caf53d-52a7-46f5-b1c6-998f8e751603",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = max(pairs, key=pairs.get)\n",
    "best = \" \".join(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e812e3-a101-479a-b1ce-4ee3bbcddfc2",
   "metadata": {},
   "source": [
    "Now let us rebuild our vocabulary with this newly found frequencies. This is the merge operation. For that let us first get the most frequent\n",
    "pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24159ada-d815-4db6-b73c-baae237d8a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e s\n",
      "e s\n",
      "defaultdict(<class 'int'>, {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3})\n"
     ]
    }
   ],
   "source": [
    "def merge(best, vocab_in):\n",
    "    new_vocab = defaultdict(int)\n",
    "\n",
    "    for token,freq in vocab_in.items():\n",
    "        if best in token:\n",
    "            print(best)\n",
    "            best_concated = best.replace(\" \",\"\")\n",
    "            token = token.replace(best, best_concated)\n",
    "        new_vocab[token] = freq\n",
    "\n",
    "    return new_vocab\n",
    "\n",
    "vocab = merge(best, vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de7d4c2d-e449-469c-8e12-8f8559800762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e s\n",
      "e s\n",
      "es t\n",
      "es t\n",
      "est </w>\n",
      "est </w>\n",
      "l o\n",
      "l o\n",
      "lo w\n",
      "lo w\n",
      "defaultdict(<class 'int'>, {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3})\n",
      "[(1, 'e s', 'es'), (2, 'es t', 'est'), (3, 'est </w>', 'est</w>'), (4, 'l o', 'lo'), (5, 'lo w', 'low')]\n"
     ]
    }
   ],
   "source": [
    "vocab  = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "rules = []\n",
    "rule_number = 1\n",
    "for i in range(5):\n",
    "    pairs = get_freq_pairs()\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    rules.append((rule_number,\" \".join(best), \"\".join(best)))\n",
    "    rule_number+=1\n",
    "    best = \" \".join(best)\n",
    "    vocab = merge(best, vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a174da1-63df-4596-9c88-b5cbad32918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 1, '</w>': 2, 'e': 3, 'r': 4, 'n': 5, 'w': 6, 'est</w>': 7, 'i': 8, 'd': 9}\n"
     ]
    }
   ],
   "source": [
    "token_id = 0\n",
    "final_vocab = {}\n",
    "for token,freq in vocab.items():\n",
    "    symbols = token.split()\n",
    "    for symbol in symbols:\n",
    "        if symbol not in final_vocab.keys():\n",
    "            token_id+=1\n",
    "            final_vocab[symbol] = token_id\n",
    "\n",
    "print(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52e70b4a-4c95-42c8-8290-cba29d39f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result l o w e r </w>\n",
      "Rule no 2 pattern es t replacement est result l o w e r </w>\n",
      "Rule no 3 pattern est </w> replacement est</w> result l o w e r </w>\n",
      "Rule no 4 pattern l o replacement lo result lo w e r </w>\n",
      "Rule no 5 pattern lo w replacement low result low e r </w>\n",
      "[1, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "def encode_token(test):\n",
    "    for rule in rules:\n",
    "        r_no = rule[0]\n",
    "        pattern = rule[1]\n",
    "        replacement = rule[2]\n",
    "        test = test.replace(pattern, replacement)\n",
    "        \n",
    "        print(f\"Rule no {r_no} pattern {pattern} replacement {replacement} result {test}\")\n",
    "    \n",
    "    encoded = [final_vocab[item] for item in test.split(\" \")]\n",
    "    print(encoded)\n",
    "\n",
    "test = 'l o w e r </w>'\n",
    "encode_token(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5162721-0504-4ab4-9328-438f253a992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result l o w es t </w>\n",
      "Rule no 2 pattern es t replacement est result l o w est </w>\n",
      "Rule no 3 pattern est </w> replacement est</w> result l o w est</w>\n",
      "Rule no 4 pattern l o replacement lo result lo w est</w>\n",
      "Rule no 5 pattern lo w replacement low result low est</w>\n",
      "[1, 7]\n"
     ]
    }
   ],
   "source": [
    "test = 'l o w e s t </w>'\n",
    "encode_token(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665044c-7e72-4e6f-b747-4e01b5257266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a6ddb-2541-45b8-b5e6-cffa6dbbf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "An implementation is available in https://github.com/openai/tiktoken open OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a8058-d122-4d1c-8169-0aa2b0e689a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b25a973-8def-40ea-b0e7-7e0751d26446",
   "metadata": {},
   "source": [
    "### Post Processing\n",
    "\n",
    "In the above example we used a special token **<UNKN>** to handle words which are not in the vocabulary. Some of the additional special tokens include\n",
    "\n",
    "1. <BOS>, beginning of a sequence, a token to symbolize beginning of a text. This will help LLM understand where the text content begins.\n",
    "2. <EOS>, end of sequence, a token to symbolize where the text begins.\n",
    "\n",
    "LLMs are trained using multiple corpuses. These tokens helps them idenify when a token begins and when it ends\n",
    "Let us add an additional corpus, add <BOS> and <EOS> tokens to the corpus and build the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fb704-a8df-47af-bd63-de81850d41e4",
   "metadata": {},
   "source": [
    "### Toy token encoding pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbade9a2-c961-4a7c-9900-489794c6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SpecialTokens:\n",
    "    self.unknwn_token\n",
    "    self.begin_token\n",
    "    self.end_token\n",
    "\n",
    "\n",
    "class ToyTextEncoder():\n",
    "    \"\"\"\n",
    "    A simple example text encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, special_tokens) -> None:\n",
    "\n",
    "        self.tokens = []\n",
    "        self.vocabulary = {}\n",
    "        self.reverse_vocabulary = {}\n",
    "        self.tokenizer = tokenizer\n",
    "        # assert special tokens\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def __pretokenize(self, input_text: str):\n",
    "\n",
    "        tokens = self.tokenizer(input_text)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def __normalize(self, input_text: str):\n",
    "\n",
    "        input_text = ftfy.fix_text(input_text)\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "        return input_text\n",
    "\n",
    "\n",
    "    def tokenize_text(self, input_text):\n",
    "\n",
    "        assert len(input_text) > 0\n",
    "        normal_text = self.__normalize(input_text)\n",
    "        tokens = self.__pretokenize(normal_text)\n",
    "\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def __build_vocabulary(self):\n",
    "\n",
    "        idx_ = 1\n",
    "        self.vocabulary[self.special_tokens.unkwn_token] = idx_\n",
    "        self.reverse_vocabulary[idx_] = self.special_tokens.unkwn_token\n",
    "        idx_+=1\n",
    "        self.vocabulary[self.special_tokens.begin_token] = idx_\n",
    "        self.reverse_vocabulary[idx_] = self.special_tokens.begin_token\n",
    "        idx+=1\n",
    "        self.vocabulary[self.special_tokens.end_token]   = idx_\n",
    "        self.reverse_vocabulary[idx_] = self.special_tokens.end_token\n",
    "        idx+=1\n",
    "        \n",
    "\n",
    "        for idx, token in enumerate(self.tokens):\n",
    "            self.vocabulary[token] =  idx + idx_\n",
    "            self.reverse_vocabulary[idx + idx_] = token\n",
    "    \n",
    "\n",
    "    def train(self, input_corpus):\n",
    "\n",
    "        if not isinstance(input_corpus, (list,tuple)):\n",
    "            if isinstance(input_corpus, str):\n",
    "                input_corpus = [input_corpus]\n",
    "\n",
    "        for line in input_corpus:\n",
    "            tokens = self.tokenize_text(line)\n",
    "            if len(tokens) > 0:\n",
    "                for token in tokens:\n",
    "                    if token not in self.tokens:\n",
    "                        self.tokens.append(token)\n",
    "\n",
    "\n",
    "        \n",
    "    def encode_text(self, input_text):\n",
    "\n",
    "        encoding = []\n",
    "        tokens = self.tokenize_text(input_text)\n",
    "        if len(tokens) > 0:\n",
    "            for token in tokens:\n",
    "                encoding.append(self.vocabulary[token])\n",
    "\n",
    "        return encoding\n",
    "        \n",
    "    def decode_text(self, input_tokenids):\n",
    "        return [self.reverse_vocabulary[id] for id in input_tokenids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0363bd5b-940b-4664-83f3-ddf7e256b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ToyTextEncoder(tokenizer)\n",
    "encoder.train(simple_books_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28200b-6d05-4fbf-8921-f08085c06cce",
   "metadata": {},
   "source": [
    "Let us peek into the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f43f6d85-a7b0-4d3d-a5cb-d09f079913f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more --> 0\n",
      "jataka --> 1\n",
      "tales --> 2\n",
      "by --> 3\n",
      "ellen --> 4\n",
      "c --> 5\n"
     ]
    }
   ],
   "source": [
    "for idx, (key,value) in enumerate(encoder.vocabulary.items()):\n",
    "    print(f\"{key} --> {value}\")\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8b230-4993-49ac-b785-ae3c36013660",
   "metadata": {},
   "source": [
    "Now we can convieniently encode and decode any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e727de87-57f3-4575-9c07-0eb69eca2ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtales by ellen king\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mencode_text(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"tales by ellen king\"\n",
    "encoder.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c5e69fd3-8240-4210-b5c7-d02cecd8b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tales', 'by', 'ellen']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode_text([2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f2ba0-2914-41af-9e71-4d6f46bb7028",
   "metadata": {},
   "source": [
    "As you can see in the above output, we have lost three tokens. Since the original words were not present in the dictionary, they were replaced by <UNKN> token.\n",
    "During decoding we have lost the original three tokens.\n",
    "\n",
    "::::{important}\n",
    ":::{note}\n",
    "While choosing the tokenizer a key requirment is that no information should be lost during encoding tokens to token-ids.  \n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb97aa-88cb-4d9b-972e-e2406c3db20e",
   "metadata": {},
   "source": [
    "## Training a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc492ae-9036-4fe1-9a33-8811884bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578d860d-2b02-49af-8286-3865d3d1f5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f614af0f2b1a41a4a53af005c68bd125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6bf74a3f204036b78c2781224274da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b326f603b641a1ab132b2cc083e1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('../data/simplebooks/simplebooks-2-raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c503ffe2-5a9d-45d7-bc3c-91dcd576d4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 114696\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 13384\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14830\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b9eaddb-57bb-4d9d-8821-334cfbe9c5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More', 'ĠJ', 'ataka', 'ĠTales']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_train_corpus():\n",
    "    return(\n",
    "        dataset['train'][i:i+100]['text'] for i in range(0, len(dataset['train']),100)\n",
    "    )\n",
    "\n",
    "training_corpus = get_train_corpus()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sample = next(training_corpus)\n",
    "tokens = old_tokenizer.tokenize(sample[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "372b0423-b13c-4cf8-a819-47eddd27ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffb75ec8-df84-4d41-a6f4-dff061baccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More', 'ĠJ', 'at', 'ak', 'a', 'ĠTales']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4577567-95b3-46c1-b6d2-74f2d8dfa7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/simplebooks-tokenizer/tokenizer_config.json',\n",
       " '../data/simplebooks-tokenizer/special_tokens_map.json',\n",
       " '../data/simplebooks-tokenizer/vocab.json',\n",
       " '../data/simplebooks-tokenizer/merges.txt',\n",
       " '../data/simplebooks-tokenizer/added_tokens.json',\n",
       " '../data/simplebooks-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../data/simplebooks-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "725f19e0-4af8-44fc-86bb-7ae153c88f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More', 'ĠJ', 'at', 'ak', 'a', 'ĠTales']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplebooks_tokenizer = AutoTokenizer.from_pretrained(\"../data/simplebooks-tokenizer\")\n",
    "simplebooks_tokenizer.tokenize(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93de5cca-6d08-4b9f-a200-f45b1fe3c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6628, 474, 275, 520, 65, 10578]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplebooks_tokenizer.encode(sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83b229-e615-4e00-85bb-49de02484fb4",
   "metadata": {},
   "source": [
    "### Simple Books Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f0e82-6048-4c31-a7ad-c71929efe054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_train_corpus():\n",
    "    return(\n",
    "        dataset['train'][i:i+100]['text'] for i in range(0, len(dataset['train']),100)\n",
    "    )\n",
    "\n",
    "def get_validation_corpus():\n",
    "    return(\n",
    "        dataset['validation'][i:i+100]['text'] for i in range(0, len(dataset['validation']),100)\n",
    "    )\n",
    "\n",
    "training_corpus = get_train_corpus()\n",
    "validation_corpus = get_validation_corpus()\n",
    "\n",
    "\n",
    "class SimpleBooksDataSet(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus_generator, max_length, stride):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.tokenizer  = AutoTokenizer.from_pretrained(\"../data/simplebooks-tokenizer\")\n",
    "        self.input_ids  = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids  = []\n",
    "\n",
    "        for sample in next(corpus_generator):\n",
    "            \"\"\"Returns 100 sentences\"\"\"\n",
    "            for sentence in sample:\n",
    "                self.token_ids.append(self.tokenizer(sample))\n",
    "\n",
    "        for i in range(0, len(self.token_ids) - max_length,stride):\n",
    "            input_chunk =  self.token_ids[i:i + max_length]\n",
    "            target_chunk = self.token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.apppend(torch.tensor(target_chunk))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "simplebooks_train_ds = SimpleBooksDataSet(training_corpus, max_length=50, stride=5)            r\n",
    "simplebooks_validation_ds = SimpleBooksDataSet(validation_corpus, max_length=50, stride=5)            r\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "train_dataloader = DataLoader()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a61251-8887-4465-9fc0-8dbb1e8d3758",
   "metadata": {},
   "source": [
    "## Word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "420f4363-261e-49ad-bd9d-cf87ecd55a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6628, 474, 275, 520, 65, 10578]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e75fb-92f4-4004-a2e1-201189249be8",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "\n",
    "* Absolute PE\n",
    "\n",
    "Every input token at position i will be associated with a trainable embedding vector.\n",
    "\n",
    "* Relative PE\n",
    "\n",
    "Represents the distance between tokens.\n",
    "\n",
    "\"A women is nothing without her man\"\n",
    "\"A man is nothing without her women\"\n",
    "\n",
    "These two sentences share the same words. They will hence share the same embeddings. For neural network both the sentences mean the same.\n",
    "But we know they convey a different meaning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719055c-7e7f-4ba6-a36c-d38b658edcfe",
   "metadata": {},
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbec15-2fc2-46ff-996b-1025303559f8",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/\n",
    "\n",
    "\n",
    " In attention, we basically take two word embeddings (x and y), pass one through a Query transformation matrix (Q) and the second through a Key transformation matrix (K), and compare how similar the resulting query and key vectors are by their dot product. So, basically, we want the dot product between Qx and Ky, which we write as:\n",
    "\n",
    "(Qx)'(Ky) = x' (Q'Ky). So equivalently we just need to learn one joint Query-Key transformation (Q'K) that transform the secondary inputs y into a new space in which we can compare x.\n",
    "\n",
    "By adding positional encodings e and f to x and y, respectively, we essentially change the dot product to\n",
    "\n",
    "(Q(x+e))' (K(y+f)) = (Qx+Qe)' (Ky+Kf) = (Qx)' Ky + (Qx)' Kf + (Qe)' Ky + (Qe)' Kf = x' (Q'Ky) + x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), where in addition to the original x' (Q'Ky) term, which asks the question \"how much attention should we pay to word x given word y\", we also have x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), which ask the additional questions, \"how much attention should we pay to word x given the position f of word y\", \"how much attention should we pay to y given the position e of word x\", and \"how much attention should we pay to the position e of word x given the position f of word y\".\n",
    "\n",
    "Essentially, the learned transformation matrix Q'K with positional encodings has to do all four of these tasks simultaneously. This is the part that may appear inefficient, since intuitively, there should be a trade-off in the ability of Q'K to do four tasks simultaneously and well.\n",
    "\n",
    " HOWEVER, MY GUESS is that there isn't actually a trade-off when we force Q'K to do all four of these tasks, because of some approximate orthogonality condition that is satisfied of in high dimensions. The intuition for this is that randomly chosen vectors in high dimensions are almost always approximately orthogonal. There's no reason to think that the word vectors and position encoding vectors are related in any way. If the word embeddings form a smaller dimensional subspace and the positional encodings form another smaller dimensional subspace, then perhaps the two subspaces themselves are approximately orthogonal, so presumably these subspaces can be transformed approx. independently through the same learned Q'K transformation (since they basically exist on different axes in high dimensional space). I don't know if this is true, but it seems intuitively possible.\n",
    "\n",
    "If true, this would explain why adding positional encodings, instead of concatenation, is essentially fine. Concatenation would ensure that the positional dimensions are orthogonal to the word dimensions, but my guess is that, because these embedding spaces are so high dimensional, you can get approximate orthogonality for free even when adding, without the costs of concatenation (many more parameters to learn). Adding layers would only help with this, by allowing for nonlinearities.\n",
    "\n",
    "We also ultimately want e and f to behave in some nice ways, so that there's some kind of \"closeness\" in the vector representation with respect to small changes in positions. The sin and cos representation is nice since nearby positions have high similarity in their positional encodings, which may make it easier to learn transformations that \"preserve\" this desired closeness.\n",
    "\n",
    "(Maybe I'm wrong, and the approximate orthogonality arises from stacking multiple layers or non-linearities in the fully-connected parts of the transformer).\n",
    "\n",
    "tl;dr: It is intuitively possible that, in high dimensions, the word vectors form a smaller dimensional subspace within the full embedding space, and the positional vectors form a different smaller dimensional subspace approximately orthogonal to the one spanned by word vectors. Thus despite vector addition, the two subspaces can be manipulated essentially independently of each other by some single learned transformation. Thus, concatenation doesn't add much, but greatly increases cost in terms of parameters to learn. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5aedc78-ca3d-4556-8f47-140161c28944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n = 100\n",
    "d_model = 4\n",
    "context_window = 5\n",
    "\n",
    "position_embedding = torch.zeros(context_window, d_model)\n",
    "\n",
    "\"\"\"\n",
    "Embedding[i, 2k]   = sin(position / n ^ (2k/d_model))\n",
    "Embedding[i, 2k+1] = cos(position / n ^ (2k/d_model))\n",
    "\n",
    "i = position index in the sequence\n",
    "k = dimension index within the embedding vector\n",
    "\"\"\"\n",
    "\n",
    "d_model % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f6184-ddfc-48d2-ad99-86cec15eba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41ede01c-3df9-48b2-9d79-90c484cbe9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(context_window):\n",
    "    for k in range(0, d_model//2) :\n",
    "        # Even positions in embedding vector\n",
    "        even_idx = 2 * k\n",
    "        denominator = torch.tensor(n ** (even_idx / d_model))\n",
    "        position_embedding[i, even_idx] = torch.sin(i/denominator)\n",
    "        # Odd positions in embedding vector\n",
    "        odd_idx = 2*k + 1\n",
    "        position_embedding[i, odd_idx] = torch.cos(i/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7d072510-c3e0-4324-8e56-788b6abb9906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0998,  0.9950],\n",
       "        [ 0.9093, -0.4161,  0.1987,  0.9801],\n",
       "        [ 0.1411, -0.9900,  0.2955,  0.9553],\n",
       "        [-0.7568, -0.6536,  0.3894,  0.9211]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b243f-6f0a-4bcd-b80d-e06fc7c10cc7",
   "metadata": {},
   "source": [
    "Position embedding should be monotonic. Teh closer you are, the more your influence on the current token. But sinusoid embeddings totally\n",
    "violate that.\n",
    "\n",
    "ALIBI based encoding is monotonic and seems to do better than sinusoidal encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47330be2-a1c6-4c7f-b82d-1164769addd8",
   "metadata": {},
   "source": [
    "## Conclustion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2ed1-b07a-4192-b7bc-3f697062b817",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
