{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a015049-4ed3-4fb1-bb2e-892035ee3c35",
   "metadata": {},
   "source": [
    "# Chapter 1 - Input to LLMs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LLM models take input as text and produce output as text. However, deep learning networks cannot work with text symbols. Hence the text must be represented in a continuous space. In this chapter, we will look at how to pre-process text input so it's malleable for a large language model to consume.\n",
    "\n",
    "Figure 1 shows the basic blocks of this operation.\n",
    "(reference:preprocessing)=\n",
    "```{figure} ../../images/chapter1/input_creation.png\n",
    "---\n",
    "height: 150px\n",
    "name: preprocessing\n",
    "---\n",
    "Input preprocessing blocks\n",
    "```\n",
    "\n",
    "Before we understand the nueances involved in each of these steps,\n",
    "let us do a simple outline of what happens in each of these blocks using a no frill example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a26d88",
   "metadata": {},
   "source": [
    "(reference:nofrill)=\n",
    "### No frill example\n",
    "\n",
    "A causal large language model, also refered to autoregresive model is trained to do the next word prediction. From a vocabulary of words, given a sequence of words, the causal model predicts the most probable next word from the vocabulary. Causal model are trained on a bunch of documents. Documents are composed of words and each word is composed of characters. In our example, word will be lowest denomination of operation.We use the term corpus to refer to these input documents. The lowest denomination in this corpus is word, typically referred called tokens. The set of unique tokens in the corpus is called vocabulary.\n",
    "\n",
    "Let us take a sample paragraph, our corpus for this exercise, and apply a regular expression to split the paragraph by whitespace or special characters. The resultant list of words forms the tokens for this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45396fd9-52f5-4ae4-ae4b-5639bc6e2374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Broadly', ',', 'physics', 'involves', 'the', 'study', 'of', 'everything', 'in', 'physical', 'existence', ',', 'from', 'the', 'smallest', 'subatomic', 'particles', 'to', 'the', 'entire', 'universe', '.', 'Physicists', 'try', 'to', 'develop', 'conceptual', 'and', 'mathematical', 'models', 'that', 'describe', 'interactions', 'between', 'entities', '(', 'both', 'big', 'and', 'small', ')', 'and', 'that', 'can', 'be', 'used', 'to', 'extend', 'our', 'understanding', 'of', 'how', 'the', 'universe', 'works', 'at', 'different', 'scales', '.', 'Are', 'you', 'interested', 'in', 'studying', 'physics', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# /* Text borrowed from https://www.tntech.edu/cas/physics/aboutphys/about-physics.php */\n",
    "text_corpus = \"Broadly, physics involves the study of everything in physical existence,\" + \\\n",
    "\"from the smallest subatomic particles to the entire universe. Physicists try \" + \\\n",
    "\"to develop conceptual and mathematical models that describe interactions between entities \" + \\\n",
    "\"(both big and small) and that can be used to extend our understanding of how the \"+ \\\n",
    "\"universe works at different scales. Are you interested in studying physics?\"\n",
    "\n",
    "\n",
    "split_expr = r'([?.#$&*^@,)(]|\\s)'\n",
    "tokens = re.split(split_expr, text_corpus)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6faca69-1f9c-4dff-85f6-939c98cc9531",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "In the above example we have a r before the string. This informs python interpreter to treat\n",
    "blackslash as raw character and not as escape character.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8483f-01b7-4a27-90ae-0a395c2bbbb5",
   "metadata": {},
   "source": [
    "The set of unique tokens forms our vocabulary. Further we assign a unique id for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb5d005-0143-4fd5-b25d-28af94365169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'interested': 1, 'to': 2, 'models': 3, 'small': 4, 'at': 5, 'scales': 6, 'universe': 7, ',': 8, 'entities': 9, 'subatomic': 10, 'that': 11, 'interactions': 12, 'be': 13, 'between': 14, 'particles': 15, 'our': 16, 'used': 17, 'can': 18, '.': 19, 'conceptual': 20, 'of': 21, 'involves': 22, 'everything': 23, 'entire': 24, 'smallest': 25, 'big': 26, 'understanding': 27, 'works': 28, 'mathematical': 29, 'studying': 30, 'Physicists': 31, 'the': 32, 'physical': 33, '(': 34, 'different': 35, 'study': 36, ')': 37, 'describe': 38, 'how': 39, 'try': 40, 'physics': 41, 'develop': 42, 'you': 43, 'existence': 44, '?': 45, 'Broadly': 46, 'from': 47, 'both': 48, 'in': 49, 'extend': 50, 'Are': 51}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {token:token_id for token_id, token in enumerate(set(tokens))}\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50568d-355e-4fe0-a690-cc342962a404",
   "metadata": {},
   "source": [
    "With this vocabulary we can now encode any input string into a list of integers / token ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95d38e-483a-43b3-bfcb-e47cbdd013a0",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Size of the vocabulary plays a great part in building the LLM. The challenge is to have a compact vocabulary and\n",
    "still try to cover maxium amount of tokens in the corpus. We will discuss\n",
    "more in this chapter about modeling exercise to build a compact vocabulary. The illustration given here is a very\n",
    "simplified example.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74136a59-eb49-4fed-bc0b-985a16d3b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 28, 5, 35, 6]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"universe works at different scales\"\n",
    "\n",
    "tokens = re.split(split_expr, input_text)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "encoded_input = [vocabulary[token.strip()] for token in tokens]\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4f2a9-cdd4-4aa4-97d0-4da1257a7b74",
   "metadata": {},
   "source": [
    "We have succesfully converted our word tokens into token id. Though this is now in number space, neural networks cannot process it. We need the input in a continous space. Here is where word embedding comes in handy. Let us build a embedding lookup table. The keys of this look up table are our integer word token ids. The value are a continous representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5caac78-f66a-40c4-b334-7b33b9fa2ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 51\n",
      "Word Embedding shape (51, 5)\n",
      "[[0.79061896 0.23025439 0.55998829 0.8374295  0.40650912]\n",
      " [0.28080375 0.6848087  0.3396529  0.17248223 0.29623326]\n",
      " [0.07165954 0.76513322 0.47325477 0.7382186  0.76346754]\n",
      " [0.90756471 0.14178423 0.65096936 0.23639402 0.67450719]\n",
      " [0.37767153 0.42207631 0.68086117 0.59773189 0.78474626]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = max(vocabulary.values())\n",
    "print(f\"vocabulary size {vocab_size}\")\n",
    "\n",
    "embedding_size = 5\n",
    "word_embedding = np.random.uniform(size=(vocab_size, embedding_size))\n",
    "print(f\"Word Embedding shape {word_embedding.shape}\")\n",
    "print(word_embedding[0:5,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2e478-4501-4903-941f-045053edaf77",
   "metadata": {},
   "source": [
    "Here we build an embedding lookup table. Our embedding dimension is set to 5. We create a look up table where rows represent the token and \n",
    "the columns represent the embedding for those words. The embeddings are random real numbers representing the words in a continous space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868b16dd-9435-408f-9dd0-87367482caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "[[0.85432497 0.33774429 0.5163072  0.16599111 0.77619878]\n",
      " [0.8367271  0.02581914 0.05084087 0.15658079 0.86089508]\n",
      " [0.25121756 0.13697472 0.44487938 0.13113087 0.51111093]\n",
      " [0.33662536 0.6617542  0.16918677 0.71635738 0.5714265 ]\n",
      " [0.51201298 0.95930028 0.74252266 0.45792642 0.87873272]]\n"
     ]
    }
   ],
   "source": [
    "input_embedding = word_embedding[encoded_input,:]\n",
    "print(input_embedding.shape)\n",
    "print(input_embedding[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801519f-c91a-41c8-a166-095ebb9efeb4",
   "metadata": {},
   "source": [
    "Word positions carry semantic information. In addition to the words, providing the position of the words\n",
    "will be benefial to the model. Similar to word embedding, we will create a look up for the position embedding.\n",
    "Let us assume a simple case here. The input size to our LLM is fixed, say 10. We will call it as the sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8466282-7650-4711-955e-492f028b725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5235984 , 0.60912799, 0.66000757, 0.87457871, 0.50215997],\n",
       "       [0.28703682, 0.38816409, 0.02410332, 0.85221138, 0.70862048]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "position_embedding_lookup = np.random.uniform(size=(sequence_length, embedding_size))\n",
    "\n",
    "position_index =  np.arange(input_embedding.shape[0])\n",
    "position_embedding = position_embedding_lookup[position_index, :]\n",
    "\n",
    "position_embedding[0:2, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1c3d7-867b-40ec-af5d-ddbaf81060f4",
   "metadata": {},
   "source": [
    "The embedding size is same as the word embedding. Finally we can now add the position embedding to word embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e814e57-776f-44e8-9034-7c8b7d399f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = input_embedding + position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980969ba-389f-48cd-a317-bcc503e41985",
   "metadata": {},
   "source": [
    "Typical of any deep learning model, feature values X and label value Y are fed into Large language model. The main job of a casual model is to predict the next given word. \n",
    "\n",
    "\n",
    "Let us see how we can quickly prepare the input X and the label Y for our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d805d4-b44b-4160-a4cd-b42a0f1e3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a feature : [46, 8, 41, 22, 32, 36, 21, 23, 49, 33]\n",
      "a label   : [8, 41, 22, 32, 36, 21, 23, 49, 33, 44]\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(split_expr, text_corpus)\n",
    "tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "token_encoding = [vocabulary[token] for token in tokens]\n",
    "\n",
    "feature_batch = []\n",
    "label_batch = []\n",
    "\n",
    "slide = 1\n",
    "for idx in range(len(tokens) - sequence_length ) :\n",
    "    feature = token_encoding[idx:idx + sequence_length]\n",
    "    label =   token_encoding[idx + slide: idx + slide + sequence_length]\n",
    "\n",
    "    feature_batch.append(feature)\n",
    "    label_batch.append(label)\n",
    "\n",
    "print(f\"a feature : {feature_batch[0]}\")\n",
    "print(f\"a label   : {label_batch[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b3d64-c0ce-4067-950e-12bf4aac7487",
   "metadata": {},
   "source": [
    "Givent the token id 7, we want the LLM to predict 35, now given 35 we want it to predict 16 and so on. By sliding the feature 1 level to the right, we get the token ids for the labels. \n",
    "\n",
    "\n",
    ":::{note}\n",
    "Sliding is a design decision. For demonstration purpose we have used a slide of 1. This may lead to overfitting in some cases.\n",
    ":::\n",
    "\n",
    "With these we can further get the embeddings throught he lookup table we have created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6c4c5-b92a-40f9-aaa6-6c069f015f6c",
   "metadata": {},
   "source": [
    "Hopefully this gives a summary of all the steps involved in preparing the input for a LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e49ad",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The examples are trivialized in this chapter. The goal is to understand the datapipeline.In realworld the pipelines are much complex. To quote from llama3 description,\n",
    "\n",
    "\"Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages. \"\n",
    "\n",
    "writing a pipline to process fifteen trillion tokens is a work of a large team of data engineers with sophisticated hardware.\n",
    "\n",
    "\"To ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.\"\n",
    "\n",
    "Further to ensure good quality training data a lot of pre-processing work needs to be performed. Covering all of htem is outside the scope of this work. However we have touched upon some of the essential pre-processing topics.\n",
    "\n",
    "https://ai.meta.com/blog/meta-llama-3/\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a8813-4bcf-49a5-a694-4d51b3d111b1",
   "metadata": {},
   "source": [
    "## A Sample Text corpus\n",
    "\n",
    "Publicly and privately available LLMs leverage the text data available in world wide web to do the pre-training. In the no frill section, we showed how the features and labels needed to train an LLM comes from the same source, sliding the features leaves us with the label. This can done in an unsupervised manner, saving the labor needed to create large training dataset. In the GPT-1 paper {cite}`radford2018improving`, the authors call this training process as unsupervised pre-training. GPT-1 was trained with Bookcorpus dataset {cite}`zhu2015aligning`.\n",
    "\n",
    "\n",
    "Loading input text from desparate sources is a tedious undertaking. LLMs are trained on Terra Bytes of data. Complex data pipelines are orchestrated to\n",
    "extract and validate the data. Details of those pipelines are beyond the scope of the book. Here is a quote from {cite}`touvron2023llama`, \"Our training corpus includes a new mix of data from publicly available sources, which does not include data\n",
    "from Meta’s products or services. We made an effort to remove data from certain sites known to contain a\n",
    "high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this\n",
    "provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase\n",
    "knowledge and dampen hallucinations.\"  \n",
    "\n",
    "\n",
    "    \n",
    "To give an idea about loading the corpus, we will use Simplebooks {cite}`nguyen2019simplebooks`. After downloading the dataset, we will show how to leverage hugginface's dataset libary to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4cb4d611-56cb-4a53-8219-3e96cc529fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading and extracting simplebooks.zip\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_simplebooks(destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Download simple books dataset\n",
    "    \"\"\"\n",
    "    url = \"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=destination)\n",
    "    print(f\"Finished downloading and extracting simplebooks.zip\")\n",
    "                      \n",
    "    \n",
    "download_simplebooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48991e-2846-4c3d-aaaf-edb1859e8b5b",
   "metadata": {},
   "source": [
    "### Structure of simplebooks\n",
    "\n",
    "From project gutenberg, 1573 books were selected, mostly children book and simplebooks dataset was created.\n",
    "Simplebooks, when downloaded comes with datasets in two sizes.Simplebooks-2 is of size 11MB with a vocabulary size of 11,492 and Simplebooks-92\n",
    "of size roughly 400MB with a vocabulary size of 98,304. Simplebooks-2 has 2.2 M tokens. Compared to llama-2 which uses 2 trillion tokens, Simplebooks\n",
    "is a small dataset which can be used write code to study LLMs.\n",
    "\n",
    "    !ls ../data/simplebooks\n",
    "    README.md  simplebooks-2  simplebooks-2-raw  simplebooks-92  simplebooks-92-raw\n",
    "\n",
    "Both simplebooks-2 and simplebooks-92 has folders with raw suffix. The raw suffixed folders have the data with no changes from gutenberg source. The following normalization were performed on raw suffixed folders and the results are in non raw suffixed folders. \n",
    "\n",
    "1. Spacy was used to tokenize each book. Original case and punctuations were preserved.\n",
    "2. @ was added as separator for numbers. So 300,000 becomes 300 @,@ 000.\n",
    "\n",
    "Each of the folder have train, test and validation split and vocabulary files.\n",
    "\n",
    "    !ls ../data/simplebooks/simplebooks-2\n",
    "    test.txt  train.txt  train.vocab  valid.txt\n",
    "    \n",
    "simplebooks-2 and simplebooks-92 have the cleaned up data. The vocabulary built after applying pre-tokenization on the normalized text is also stored. A quick peek at the train files should show the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4dd4539d-eef0-4d8a-bf21-f8ee8f515ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More <unk> Tales\r\n",
      "\r\n",
      "By\r\n",
      "\r\n",
      "Ellen C. <unk>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ../../data/simplebooks/simplebooks-2/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "801b4093-66d1-43e7-a0d6-62b2d416f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Jataka Tales\n",
      "\n",
      "By\n",
      "\n",
      "Ellen C. Babbitt\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "The Girl Monkey And The String Of Pearls\n",
      "\n",
      "\n",
      "One day the king went for a long walk in the woods. When he came back to his own garden, he sent for his family to come down to the lake for a swim.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 ../data/simplebooks/simplebooks-2-raw/train.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc26a9-fe65-4652-8578-6e8d8477f33f",
   "metadata": {},
   "source": [
    "As a part of pre-tokenization some of the uknown words like Jataka, Babbitt, are replaced by a token \"<unk>\". More about special tokens later in this chapter. Unnecessary white space are removed, look at the sentence\n",
    "\"own garden , \" is cleaned up to \"own garden,\" Let us peek into the vocabulary creaated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb7b837a-8058-4b4b-9e5a-7618234d5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in vocabulary 11493\n",
      "sample tokens [',', '.', 'the', '\"', 'and']\n",
      "their encodings [131695, 105703, 98932, 97156, 63612]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "with open(\"../data/simplebooks/simplebooks-2/train.vocab\") as f:\n",
    "    rows = ( line.split('\\t') for line in f )\n",
    "    count = 0\n",
    "    for row in rows:\n",
    "        vocabulary[row[0]] = int(row[1].strip())\n",
    "        count+=1\n",
    "        \n",
    "print(f\"Entries in vocabulary {count}\")\n",
    "print(f\"sample tokens {list(vocabulary.keys())[0:5]}\")\n",
    "print(f\"their encodings {list(vocabulary.values())[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbf83d-e9e6-466e-bf79-db0caed6630c",
   "metadata": {},
   "source": [
    "Hopefully this gives an idea about input text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41033c3f-ac5d-4c82-a3fa-51d9e8f6bcf5",
   "metadata": {},
   "source": [
    "## Tokenization Pipeline\n",
    "\n",
    "The tokenization begins with raw input text source / corpus and ends with a dictionary of tokens and their associated token ids. Token ids are integers. After this given a new text, the pipeline should be able to spit out the associated tokens. Similarly, given a list of tokens, the pipeline should be able to convert it back to text without any loss. The below figure illustrates the various steps involved in this pipeline.\n",
    "\n",
    "(reference:tokenization)=\n",
    "```{figure} ../../images/chapter1/TokenEncoding.jpg\n",
    "---\n",
    "height: 250px\n",
    "name: encoding\n",
    "---\n",
    "Steps in Tokenization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c978067-2bd6-4812-9c0b-8898ae8ef9a3",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "In the simplebooks example, we saw that unncessary whitespaces were removed and numbers were formatted by inserting '@' at different separators. \n",
    "Typicall normalization involves removing unncessary whitespaces, stripping of accents, lower case conversion and similar others. Here is a list of some normalizer s provided by [HuggingFace Tokenizer library](https://huggingface.co/docs/tokenizers/en/components).\n",
    "\n",
    "1. Unicode normalization (NFD, NFKD, NFC and NFKC algorithms)\n",
    "2. Lowecase conversion\n",
    "3. Stripping white spaces and accents\n",
    "4. Replacing common string patterns\n",
    "\n",
    ":::{admonition} Unicode normalization\n",
    "\n",
    "Unicode encoding involves assigning a numerical value called \"code point\" to each character and transforming them into a series of bytes.\n",
    "Issues may arise when a character can be represented by a single code point or a combination of two code points. Unicode normalization\n",
    "is the process of normalizing a unicode encoded string into a canonical form.\n",
    "\n",
    "For the more curious please read the [article](https://www.smashingmagazine.com/2012/06/all-about-unicode-utf8-character-sets/) to get a little history about ASCII, latin-1, unicode.\n",
    "\n",
    ":::\n",
    "\n",
    "Quoting from GPT-1 paper {cite}`radford2018improving`, \"We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer\".\n",
    "\n",
    "1. [ftfy - fixes text for you](https://ftfy.readthedocs.io/en/latest/index.html)\n",
    "2. [Spacy](https://spacy.io/)\n",
    "\n",
    "Going into the details of ftfy and spacy is beyond the scope of this book. Following code snippets demonstrates the basic usage of these packages. We will discuss Spacy in pre-tokenization section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f69583-f781-43b1-ae16-c9873b13ee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LóPEZ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ftfy\n",
    "\n",
    "ftfy.fix_text(\"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837af13-fbbe-4aa3-ab6a-429006e428e1",
   "metadata": {},
   "source": [
    "The string \"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\" is converted to LoPEZ by ftfy. This package can take care of issues with character decoding. Let us look\n",
    "at a spacy example. After installing spacy, download the tokenizer model to run the following code snipped.\n",
    "\n",
    "    conda install ftfy spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "`````{admonition} Mojibake (文字化け, \"Garbled\") \n",
    ":class: tip\n",
    "Garbled text formed as a result of being decoded using a character encoding with which it was not orignally encoded. \n",
    "A funny poem about Mojibake related to characters printed in a shippling label.\n",
    "\n",
    "(reference:mojibake)=\n",
    "Figure 2 A funny mojibake poem\n",
    "\n",
    "```{figure} ../images/chapter1/shipping-label.png\n",
    "---\n",
    "height: 150px\n",
    "name: shipping-label\n",
    "---\n",
    "Mojibake shipping label\n",
    "```\n",
    "ODE TO A SHIPPING LABEL\n",
    "Once there was a little o,\n",
    "with an accent on top like so\n",
    "\n",
    "It started out as UTF8,\n",
    "but the program only knew latin1,\n",
    "and changed the litte o to A for fun.\n",
    "\n",
    "and it goes on. For the complete [poem](https://imgur.com/4J7Il0m)\n",
    "\n",
    "The text in the label is lopez and due to wrong decoding we have a Mojibake. \n",
    "\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638d5a1-32af-4a90-bf40-bdf572175115",
   "metadata": {},
   "source": [
    "### Pre-tokenization\n",
    "\n",
    "\n",
    "Using a set of rules, the text is split into atomic units, tokens. Imaging this as a superset of tokens fed into the vocabulary building exercise. A subset of these tokens make their way into the final vocabulary. An example pre-tokenizer is  a simple whitespace tokenizer. If two words are separated by a whitespace, they will be treated as two tokens.\n",
    "We saw an example of this in the no frill section. Let us write some python code to implement what we have learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3188fe4-8628-4044-af61-c0485fd90e3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftfy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75303/18816868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mftfy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSpacyTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ftfy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import spacy\n",
    "\n",
    "class SpacyTokenizer():\n",
    "    \"\"\"\n",
    "    Tokenizer based on Spacy library\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def __call__(self, input_text):\n",
    "\n",
    "        assert len(input_text) > 0\n",
    "\n",
    "        doc = nlp(input_text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "\n",
    "class RegexTokenizer():\n",
    "    \"\"\"\n",
    "    Regex Based Tokenizer\n",
    "    Splits text by eitehr whitespace or by one of these\n",
    "    special characters,?.#$&*^@,\n",
    "    \"\"\"\n",
    "    def __call__(self, input_text):\n",
    "\n",
    "        assert input_text is not None\n",
    "        assert len(input_text) > 0\n",
    "\n",
    "        tokenizer_regex = r'([?.#$&*^@,)(]|\\s)'\n",
    "        tokens = re.split(tokenizer_regex, input_text)\n",
    "        tokens = [token.strip() for token in tokens if len(token.strip()) > 0]\n",
    "\n",
    "        return tokens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7a941-16c4-455c-a5f5-7227e8d31b1d",
   "metadata": {},
   "source": [
    "The regex based tokenizer, uses the regex expression we introduced in no frills section. Spacy tokenizer uses\n",
    "the Spacy library to tokenize. Let us take a sample from our Simplebooks dataset to see these tokenizers in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8a42d9-912b-49fa-bbb6-e6e41b93bcaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75303/3361626792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mSAMPLE_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegexTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def read_simplebooks(path):\n",
    "    for line in open(path, 'r'):\n",
    "        yield line\n",
    "\n",
    "simplebooks_reader = read_simplebooks('../data/simplebooks/simplebooks-2-raw/train.txt')\n",
    "SAMPLE_SIZE = 6\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "\n",
    "simple_books_sample = [tokenizer(line) for idx, line in enumerate(simplebooks_reader) if idx <= SAMPLE_SIZE and len(line) > 1]\n",
    "simple_books_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7e1f1-8529-4528-b7cb-7c1d8c56fd79",
   "metadata": {},
   "source": [
    "With a sample of 15 sentences we are ready to pass it to our tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ec8fb-d44b-4dce-8639-2c02c1469fa4",
   "metadata": {},
   "source": [
    "### Tokenizer models - Dictionary Training\n",
    "\n",
    "One may wonder the need for any subsequent processing in tokenization pipeline. The pre-tokenization output can be used directly to build a Vocabulary. The set of unique tokens gathered after running the tokenizer over the input corpus is the vocabulary. The transformerXL model {cite}`dai2019transformerxl` has a vocabulary size of 250K, compared to Llama which has a size of 32K\n",
    "\n",
    ":::{note}\n",
    "TransformerXL uses space and punctuation to tokenize the text. Their vocabulary size is around 250K. Here is the link\n",
    "to their paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n",
    "::: \n",
    "\n",
    "\n",
    "A compact vocabulary reduces the model complexity and computation needs to train and perform inference. Special tokens and respective token-ids are added for unknown words. Say an input to the language model contains a word not present in the vocabulary, it will be treatd as unknown and the token id assigned for unknown word will be substituted. A good token encoding pipeline should strive to reduce the number of unknown words. Compact vocabulary and reduced unkown words are two opposite contraints.\n",
    "\n",
    "Word based tokenization suffers from very large vocabulary size and large number of out of vocabulary tokens.\n",
    "Character based tokenization suffers from very large sequences and less meaningful individual tokens.\n",
    "\n",
    "The pre-tokenization leaves us with a superset of all the tokens. The Dictionary training phase involves applying an algorithm to finalize the subset of tokens from this superset to be used for encoding.\n",
    "\n",
    "\n",
    "```{note}\n",
    "The tokenizer which uses recursive rules to produce vocabulary are commonly called as sub-word tokenizers.\n",
    "```\n",
    "\n",
    "Dont get it confused by machine learning tranining process. By train, this method is suppose to use a bunch of rules to produce an optimum dictionary. \n",
    "Using rules, the tokens are further split to form a compact vocabulary, at the same time reduce the chances of having unknown token ids. \n",
    "\n",
    "The most commonly used dictionary training approaches are\n",
    "\n",
    "1. BPE - Byte Pair Encoding\n",
    "2. WordPiece\n",
    "3. SentencePiece\n",
    "4. Unigram\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Character Level Encoding\n",
    "The two biggest challenge with word-level tokenization and are the size of the vocabulary and the number of unknown tokens added as a part of encoding.\n",
    "The vocabulary size has to be very large to decrease the number of uknown token, however it does not guarantee great reduction of unknown tokens. Words are based on characters, how about we tokenize the individual characters and use an encoding for each character?\n",
    "\n",
    "    input_corpus_encoded = [ord(character) for character in text_corpus]\n",
    "    print(input_corpus_encoded)\n",
    "    assert len(text_corpus) == len(input_corpus_encoded)\n",
    "    \n",
    "    [76, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 115, 44, 32, 116, 104, 101, 32, 110, 101, 119, 32, 107, 105, 100, 32, 105, 110, 32, 116, 104, 101, 32, 98, 108, 111, 99, 107, 32, 105, 115, 32, 99, 114, 101, 97, 116, 105, 110, 103, 32, 119, 111, 110, 100, 101, 114, 115, 46, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 78, 117, 109, 101, 114, 111, 117, 115, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 104, 97, 118, 101, 32, 115, 112, 97, 110, 110, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 108, 97, 115, 116, 32, 116, 119, 111, 32, 121, 101, 97, 114, 115, 32, 108, 101, 118, 97, 114, 97, 103, 105, 110, 103, 32, 108, 108, 109, 115, 46]\n",
    "\n",
    "    decoded_input = [chr(token_id) for token_id in input_corpus_encoded]\n",
    "    print(\"\".join(decoded_input))\n",
    "    \n",
    "    Large language models, the new kid in the block is creating wonders.                 Numerous applications have spanned in the last two years levaraging llms.\n",
    "\n",
    "**Challenges with character level encoding**\n",
    "\n",
    "The context of thw words are lost while doing character level encoding. They may be suitable for small toy llm's for unusable for building systems of any practical use.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "In this book we will cover Byte pair encoding. Curious readers can go through https://huggingface.co/docs/transformers/en/tokenizer_summary to get a summary of other subword algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0aa0c-f1f7-494c-bcaf-bc77de7c6a2a",
   "metadata": {},
   "source": [
    "#### Bye pair encoding\n",
    "\n",
    "Byte pair encoding was first introduced for word segmentation in the paper Neural Machine Translation of Rare Words with Subword Units {cite}`sennrich2016neural`. It is a sub-word level method. The original algorithm is attribtued to Philip Gage. 1994. A New Algorithm for Data Com-pression. C Users J., 12(2):23–38, February. It is a data compression algorithm working iteratively. Say for example, we have the following string\n",
    "\n",
    "aaaabdaaabac\n",
    "\n",
    "Iteratively, let us now replace the most frequent pairs with another symbol not present in the string. For example, we replace the pair 'aa' with Z. The new\n",
    "string will be ZabdZabac. 'Za' is the most frequently occuring pair now. Let us replace it with X. We continue this way till the string reaches the desired size.\n",
    "Below is the python code to demonstrate this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12510aa4-27f2-4058-a408-a2b3fea94e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      " old aaabdaaabac \n",
      " new ZabdZabac \n",
      " aa replaced by Z\n",
      "Iteration 2 \n",
      " old ZabdZabac \n",
      " new XbdXbac \n",
      " Za replaced by X\n",
      "Iteration 3 \n",
      " old XbdXbac \n",
      " new YdYac \n",
      " Xb replaced by Y\n"
     ]
    }
   ],
   "source": [
    "def bpe_compression(input_str, desired_size=5):\n",
    "    iterations = 0\n",
    "    replace = ['Z','X','Y','L']\n",
    "    replacements = []\n",
    "    while len(input_str) > desired_size:\n",
    "        iterations+=1\n",
    "        pairs = Counter([i + j for i, j in zip(input_str, input_str[1:])])\n",
    "        pair,freq = pairs.most_common(1)[0]\n",
    "        if freq <=1 :\n",
    "            break\n",
    "        old_str = input_str\n",
    "        input_str = input_str.replace(pair, replace[iterations - 1])\n",
    "        replacements.append((replace[iterations -1], pair))\n",
    "        print(f\"Iteration {iterations} \\n old {old_str} \\n new {input_str} \\n {pair} replaced by {replace[iterations-1]}\")\n",
    "    return input_str, replacements\n",
    "\n",
    "input_str, replacements = bpe_compression('aaabdaaabac')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207af50-d212-4483-99f8-91f698941638",
   "metadata": {},
   "source": [
    "In order to reconstruct this, we store all the replacements in a stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe6f4be-2ad1-4a80-b7e0-6b8f8aa7ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Z', 'aa'), ('X', 'Za'), ('Y', 'Xb')]\n"
     ]
    }
   ],
   "source": [
    "print(replacements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8df93-5215-4379-a08c-cbf2161fdb59",
   "metadata": {},
   "source": [
    "Now we can pop up the replacements from the stack and retrieve the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23e81f0c-cd25-4f91-a168-897565e57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(replacements) > 0:\n",
    "    replace_str, _str = replacements.pop()\n",
    "    print(replace_str, _str)\n",
    "    input_str = input_str.replace(replace_str, _str)\n",
    "    print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee868276-0c98-42c2-b772-b4e7b98a878f",
   "metadata": {},
   "source": [
    "BPE begins with the output from pre-tokenizer. For each token, a map of token with its constituent characters followed by a end of word symbol and its frequency in the input corpus are retrieved.\n",
    "\n",
    "Let us see the example from {cite}`sennrich2016neural`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6eb7362-eb00-4088-81e2-5ace75401e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6, 'w i d e s t </w>':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00a7e5-606d-41ed-b984-82cf13b8930d",
   "metadata": {},
   "source": [
    "So given a token 'l o w </w>', we get the list of subsequent character pairs and their frequency. In this case it will be\n",
    "\n",
    "'l o', 'o w' and 'w </w>'. Since 'l o' occurs in 'l o w' and 'l o w e r',its frequency will be 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b01cbb-7a54-4343-a0ac-0741dbb0954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('l', 'o'): 7,\n",
       "             ('o', 'w'): 7,\n",
       "             ('w', '</w>'): 5,\n",
       "             ('w', 'e'): 8,\n",
       "             ('e', 'r'): 2,\n",
       "             ('r', '</w>'): 2,\n",
       "             ('n', 'e'): 6,\n",
       "             ('e', 'w'): 6,\n",
       "             ('e', 's'): 9,\n",
       "             ('s', 't'): 9,\n",
       "             ('t', '</w>'): 9,\n",
       "             ('w', 'i'): 3,\n",
       "             ('i', 'd'): 3,\n",
       "             ('d', 'e'): 3})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_freq_pairs():\n",
    "    pairs = defaultdict(int)\n",
    "    for token,frequency in vocab.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) -1):\n",
    "            pair = (symbols[i],symbols[i+1])\n",
    "            pairs[pair]+=frequency\n",
    "    return pairs\n",
    "\n",
    "pairs = get_freq_pairs()\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6caf53d-52a7-46f5-b1c6-998f8e751603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e s'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = max(pairs, key=pairs.get)\n",
    "best = \" \".join(best)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e812e3-a101-479a-b1ce-4ee3bbcddfc2",
   "metadata": {},
   "source": [
    "In this iteration we have selected 'es' as the best pair. Now let us rebuild our vocabulary with this newly found frequencies. This is the merge operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24159ada-d815-4db6-b73c-baae237d8a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3})\n"
     ]
    }
   ],
   "source": [
    "def merge(best, vocab_in):\n",
    "    new_vocab = defaultdict(int)\n",
    "\n",
    "    for token,freq in vocab_in.items():\n",
    "        if best in token:\n",
    "            best_concated = best.replace(\" \",\"\")\n",
    "            token = token.replace(best, best_concated)\n",
    "        new_vocab[token] = freq\n",
    "\n",
    "    return new_vocab\n",
    "\n",
    "vocab = merge(best, vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28639a1f",
   "metadata": {},
   "source": [
    "As you can see from the above output, 'e' and 's' are now merged as 'es'. We have the output from the first iteration. Similary we can run multiple iteration. In the below example, we run the iterations 5 times. Every time we get the most frequent pair, find the best pair, the one with highest frequency, perform the merge oepration and simulataneously store the merges as rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7d4c2d-e449-469c-8e12-8f8559800762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3})\n",
      "[(1, 'e s', 'es'), (2, 'es t', 'est'), (3, 'est </w>', 'est</w>'), (4, 'l o', 'lo'), (5, 'lo w', 'low')]\n"
     ]
    }
   ],
   "source": [
    "vocab  = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "rules = []\n",
    "rule_number = 1\n",
    "for i in range(5):\n",
    "    pairs = get_freq_pairs()\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    rules.append((rule_number,\" \".join(best), \"\".join(best)))\n",
    "    rule_number+=1\n",
    "    best = \" \".join(best)\n",
    "    vocab = merge(best, vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1203686",
   "metadata": {},
   "source": [
    "With the updated frequency and rules to merge, we can finally create our subword dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a174da1-63df-4596-9c88-b5cbad32918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 1, '</w>': 2, 'e': 3, 'r': 4, 'n': 5, 'w': 6, 'est</w>': 7, 'i': 8, 'd': 9}\n"
     ]
    }
   ],
   "source": [
    "token_id = 0\n",
    "final_vocab = {}\n",
    "for token,freq in vocab.items():\n",
    "    symbols = token.split()\n",
    "    for symbol in symbols:\n",
    "        if symbol not in final_vocab.keys():\n",
    "            token_id+=1\n",
    "            final_vocab[symbol] = token_id\n",
    "\n",
    "print(final_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbbc47",
   "metadata": {},
   "source": [
    "Our final vocabulary is ready. You can compare it with our pre-tokenization frequence table. We had words like lower, lowest in our pre-tokenization dictionary. We now have a compact subword vocabulary. Let us try to use this vocabulary and our merge rules to tokenize a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e70b4a-4c95-42c8-8290-cba29d39f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result l o w e r </w>\n",
      "Rule no 2 pattern es t replacement est result l o w e r </w>\n",
      "Rule no 3 pattern est </w> replacement est</w> result l o w e r </w>\n",
      "Rule no 4 pattern l o replacement lo result lo w e r </w>\n",
      "Rule no 5 pattern lo w replacement low result low e r </w>\n",
      "[1, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "def encode_token(test):\n",
    "    for rule in rules:\n",
    "        r_no = rule[0]\n",
    "        pattern = rule[1]\n",
    "        replacement = rule[2]\n",
    "        test = test.replace(pattern, replacement)\n",
    "        \n",
    "        print(f\"Rule no {r_no} pattern {pattern} replacement {replacement} result {test}\")\n",
    "    \n",
    "    encoded = [final_vocab[item] for item in test.split(\" \")]\n",
    "    print(encoded)\n",
    "\n",
    "test = 'l o w e r </w>'\n",
    "encode_token(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04df8f",
   "metadata": {},
   "source": [
    "Rule No 1,2 and 3 does not apply to our example. Rule number 4, where characters l and o are replaced by lo. Further according to Rule no 5, lo and w are further merged as low. Finally we perform a lookup for low, e and r in our vocabulary and encode this text. A single world lower in this case is encoded into four integer tokens. Let us see another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5162721-0504-4ab4-9328-438f253a992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result l o w es t </w>\n",
      "Rule no 2 pattern es t replacement est result l o w est </w>\n",
      "Rule no 3 pattern est </w> replacement est</w> result l o w est</w>\n",
      "Rule no 4 pattern l o replacement lo result lo w est</w>\n",
      "Rule no 5 pattern lo w replacement low result low est</w>\n",
      "[1, 7]\n"
     ]
    }
   ],
   "source": [
    "test = 'l o w e s t </w>'\n",
    "encode_token(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff67b2",
   "metadata": {},
   "source": [
    "We see the word lowest is encoded as 2 tokens. An implementation of byte-pair encoding is available in https://github.com/openai/tiktoken. This was released by OpenAI.\n",
    "\n",
    "Google has released SentencePiece,https://github.com/google/sentencepiece. A tokenizer which uses both byte-pair and Unigram algorithms. During pre-tokenization we typcially tend to use whitespace to split the raw text. But there are languagtes where the words can't be split by whitespace. SentencePiece claims to be handy in those cases. It is a langauage agnostic subword tokenization algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222dd95",
   "metadata": {},
   "source": [
    "### Post Processing\n",
    "\n",
    "Let us look a simple example to illustrate the need for post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea066bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result speed\n",
      "Rule no 2 pattern es t replacement est result speed\n",
      "Rule no 3 pattern est </w> replacement est</w> result speed\n",
      "Rule no 4 pattern l o replacement lo result speed\n",
      "Rule no 5 pattern lo w replacement low result speed\n",
      "Given word not available in the dictionary\n"
     ]
    }
   ],
   "source": [
    "test = \"speed\"\n",
    "try:\n",
    "    encode_token(test)\n",
    "except KeyError:\n",
    "    print(f\"Given word not available in the dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cc1fd",
   "metadata": {},
   "source": [
    "Here is an example where the encoding failed, as the encoding algorithm didnt know how to split this word into subwords. During training the dictionary we did not encounter this word. In these cases, a special token is added to the vocabulary. Let us rewrite our encode function to handle this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1569957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule no 1 pattern e s replacement es result speed\n",
      "Rule no 2 pattern es t replacement est result speed\n",
      "Rule no 3 pattern est </w> replacement est</w> result speed\n",
      "Rule no 4 pattern l o replacement lo result speed\n",
      "Rule no 5 pattern lo w replacement low result speed\n",
      "[999]\n"
     ]
    }
   ],
   "source": [
    "final_vocab['UNKN'] = 999\n",
    "\n",
    "def encode_token_v1(test):\n",
    "    for rule in rules:\n",
    "        r_no = rule[0]\n",
    "        pattern = rule[1]\n",
    "        replacement = rule[2]\n",
    "        test = test.replace(pattern, replacement)\n",
    "        \n",
    "        print(f\"Rule no {r_no} pattern {pattern} replacement {replacement} result {test}\")\n",
    "    \n",
    "    encoded =[]\n",
    "    for item in test.split(\" \"):\n",
    "        if item not in final_vocab.keys():\n",
    "            encoded.append(final_vocab['UNKN'])\n",
    "        else:\n",
    "            encoded.append(final_vocab[item])\n",
    "    print(encoded)\n",
    "    \n",
    "encode_token_v1(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25a973-8def-40ea-b0e7-7e0751d26446",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the above example we used a special token **<UNKN>** to handle words which are not in the vocabulary. Some of the additional special tokens include\n",
    "\n",
    "1. <BOS>, beginning of a sequence, a token to symbolize beginning of a text. This will help LLM understand where the text content begins.\n",
    "2. <EOS>, end of sequence, a token to symbolize where the text begins.\n",
    "\n",
    "LLMs are trained using multiple corpuses. These tokens helps them idenify when a token begins and when it ends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f2ba0-2914-41af-9e71-4d6f46bb7028",
   "metadata": {},
   "source": [
    "::::{important}\n",
    ":::{note}\n",
    "While choosing the tokenizer algorithm a key requirment is that no information should be lost during encoding tokens to token-ids.  \n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb97aa-88cb-4d9b-972e-e2406c3db20e",
   "metadata": {},
   "source": [
    "## HuggingFace Libraries\n",
    "\n",
    "Now that we understand the data preparation pipeline, let us introduce the readers to HuggingFace ecosystem and how we can leverage it for building input data pipelines to train LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "578d860d-2b02-49af-8286-3865d3d1f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.parent.absolute())\n",
    "\n",
    "\n",
    "destination = parent_path + '/data/simplebooks/simplebooks-2-raw/'\n",
    "\n",
    "\n",
    "def download_simplebooks(destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Download simple books dataset\n",
    "    \n",
    "    Args:\n",
    "        destination: download folder string\n",
    "    \"\"\"\n",
    "    url = \"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=\"destination\")\n",
    "    print(f\"Finished downloading and extracting simplebooks.zip\")\n",
    "\n",
    "\n",
    "\n",
    "def load_simple_books(destination: str) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(destination)\n",
    "    \n",
    "    train        = dataset['train']['text']\n",
    "    test         = dataset['test']['text']\n",
    "    validation   = dataset['validation']['text'] \n",
    "    \n",
    "    return {\"train\": train, \"test\": test, \"validation\": validation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3191b",
   "metadata": {},
   "source": [
    "Function download_simplebooks downloads the raw input and stores it in the destination folder. The following function load_simple_books uses load_dataset function from datasets library to load this data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "137313a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 114696\n",
      "test rows: 14830\n",
      "validation rows: 13384\n"
     ]
    }
   ],
   "source": [
    "dataset = load_simple_books(destination)\n",
    "\n",
    "for key,values in dataset.items():\n",
    "    print(f\"{key} rows: {len(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978b4c1",
   "metadata": {},
   "source": [
    "As you can see our raw data is now stored as dictionary in memory. Let us now use AutoTokenizer class from Huggingface transformers libary to load subword tokenization algorithm employed by GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b9eaddb-57bb-4d9d-8821-334cfbe9c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS> The Girl Monkey And The String Of Pearls <EOS>', '<BOS> One day the king went for a long walk in the woods. When he came back to his own garden, he sent for his family to come down to the lake for a swim. <EOS>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " 'ĠThe',\n",
       " 'ĠGirl',\n",
       " 'ĠMonkey',\n",
       " 'ĠAnd',\n",
       " 'ĠThe',\n",
       " 'ĠString',\n",
       " 'ĠOf',\n",
       " 'ĠPear',\n",
       " 'ls']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\"\n",
    "                                               ,padding='max_length'\n",
    "                                               ,truncation=\"only_second\"\n",
    "                                               ,max_length=10\n",
    "                                               ,padding_side =\"left\"\n",
    "                                               ,bos_token=\"<BOS>\"\n",
    "                                               ,eos_token=\"<EOS>\"\n",
    "                                               ,pad_token=\"<PAD\"\n",
    "                                              )\n",
    "encode_input = []\n",
    "for sample in dataset['train'][10:15]:\n",
    "    if len(sample) >0:\n",
    "        sample = \"<BOS> \" + sample + \" <EOS>\"\n",
    "        encode_input.append(sample)\n",
    "\n",
    "print(encode_input)\n",
    "\n",
    "tokens = gpt2_tokenizer.tokenize(encode_input)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd3022e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[50259, 464, 7430, 26997, 843, 383, 10903, 3226, 11830, 7278], [3198, 1110, 262, 5822, 1816, 329, 257, 890, 2513, 287]], 'attention_mask': [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer(encode_input ,padding='max_length'\n",
    "                                               ,truncation=True\n",
    "                                               ,max_length=10\n",
    "                                               ,return_attention_mask=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca020f5",
   "metadata": {},
   "source": [
    "### Train a custom dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "372b0423-b13c-4cf8-a819-47eddd27ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Jataka Tales\n",
      "The Story Of A Lamb On Wheels\n"
     ]
    }
   ],
   "source": [
    "def data_generator(dataset):\n",
    "    for idx, row in enumerate(dataset):\n",
    "        yield row\n",
    "\n",
    "        \n",
    "print(next(data_generator(dataset['train'])))\n",
    "print(next(data_generator(dataset['validation'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "545d866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 52000\n",
    "generator = data_generator(dataset['train'])\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(generator, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4577567-95b3-46c1-b6d2-74f2d8dfa7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/simplebooks-tokenizer/tokenizer_config.json',\n",
       " '../data/simplebooks-tokenizer/special_tokens_map.json',\n",
       " '../data/simplebooks-tokenizer/vocab.json',\n",
       " '../data/simplebooks-tokenizer/merges.txt',\n",
       " '../data/simplebooks-tokenizer/added_tokens.json',\n",
       " '../data/simplebooks-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../data/simplebooks-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "725f19e0-4af8-44fc-86bb-7ae153c88f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More', 'ĠJataka', 'ĠTales']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplebooks_tokenizer = AutoTokenizer.from_pretrained(\"../data/simplebooks-tokenizer\")\n",
    "simplebooks_tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93de5cca-6d08-4b9f-a200-f45b1fe3c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6483, 28923, 10076]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplebooks_tokenizer.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0da9d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6483, 28923, 10076], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplebooks_tokenizer(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83b229-e615-4e00-85bb-49de02484fb4",
   "metadata": {},
   "source": [
    "### Simple Books Pytorch Dataset\n",
    "\n",
    "Let us put together what we have learned till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d21f0e82-6048-4c31-a7ad-c71929efe054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.parent.absolute())\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    \n",
    "    tokenizer_path = parent_path + \"/data/simplebooks-tokenizer\"\n",
    "    print(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "    simplebooks_tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path))\n",
    "    return simplebooks_tokenizer\n",
    "\n",
    "\n",
    "def download_simplebooks(destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Download simple books dataset\n",
    "    \n",
    "    Args:\n",
    "        destination: download folder string\n",
    "    \"\"\"\n",
    "    url = \"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=\"destination\")\n",
    "    print(f\"Finished downloading and extracting simplebooks.zip\")\n",
    "\n",
    "\n",
    "def load_simple_books(destination: str) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(destination)\n",
    "    \n",
    "    train        = dataset['train']['text']\n",
    "    test         = dataset['test']['text']\n",
    "    validation   = dataset['validation']['text'] \n",
    "    \n",
    "    return {\"train\": train, \"test\": test, \"validation\": validation}\n",
    "                                         \n",
    "\n",
    "\n",
    "\n",
    "class SimpleBooksDataSet(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, max_length, stride, context='train'):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        path = parent_path + \"/data/simplebooks-tokenizer\"\n",
    "        self.tokenizer  = AutoTokenizer.from_pretrained(path)\n",
    "        self.input_ids  = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids  = []\n",
    "\n",
    "        for sample in corpus:\n",
    "            if len(sample) > 0:\n",
    "                self.token_ids.extend(self.tokenizer.encode(sample, truncation=True, max_length=max_length))\n",
    "        \n",
    "        print(f\"Total {context} tokens {len(self.token_ids)}\")\n",
    "        \n",
    "        for i in range(0, len(self.token_ids) - max_length + 1,stride):\n",
    "            input_chunk =  self.token_ids[i:i + max_length]\n",
    "            target_chunk = self.token_ids[i + 1: i + max_length + 1]\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "    \n",
    "def stack_collate(data):\n",
    "    features, target = zip(*data)\n",
    "    X = torch.stack(features)\n",
    "    y = torch.stack(target)\n",
    "    return X, y\n",
    "    \n",
    "def get_dataloaders(batch_size=16, num_workers=4):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    destination = parent_path + '/data/simplebooks/simplebooks-2-raw/'\n",
    "    \n",
    "    print(f\"Loading dataset from {destination}\")\n",
    "    \n",
    "    dataset     = load_simple_books(destination)\n",
    "    \n",
    "    training_corpus   = dataset['train']\n",
    "    validation_corpus = dataset['validation']\n",
    "\n",
    "    \n",
    "    train_ds      = SimpleBooksDataSet(training_corpus, max_length=50, stride=50)            \n",
    "    validation_ds = SimpleBooksDataSet(validation_corpus,max_length=50, stride=50, context='validation')  \n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_ds, batch_size=batch_size, collate_fn=stack_collate,\n",
    "                                  shuffle=True,drop_last=True,num_workers=num_workers)\n",
    "\n",
    "    validation_dataloader = DataLoader(dataset=validation_ds, batch_size=batch_size, collate_fn=stack_collate,\n",
    "                                  shuffle=False,drop_last=True,num_workers=num_workers)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13f05082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/gopi/Documents/small_llm/llmbook/data/simplebooks/simplebooks-2-raw/\n",
      "Total train tokens 1676477\n",
      "Total validation tokens 189785\n",
      "tensor([[  653,  1055,    12,  2165,  3359,    12,   581,    26,     2,  1640,\n",
      "           372,   434,   925,  2502,    31,  2577,   372,   434,   925,  2502,\n",
      "            31,   295,   448,   585,   392,   260,   925,   381,  1594,    14,\n",
      "          1098,  2012,   271,   260,  8612,   536,   260,   666,   396,  2978,\n",
      "            14,   935,   357,    12,   351,   341,   552,  2978,    12,   410],\n",
      "        [ 1633,   271,   421,  3481,   589,    12,   270,   351,   404,   348,\n",
      "            12,   921,   344,   259,  1392,    12,  1121,   283,   341,  1370,\n",
      "           260,  1539,    14,  3481, 10928,   432,  3149,   341,   624,   822,\n",
      "           303,   260,   922,    14,   590,   469,  2388,   466,  1156,  3618,\n",
      "           351,   433,    12,   270,   559,   561,   922,   618,   270,   594]])\n",
      "tensor([[ 1055,    12,  2165,  3359,    12,   581,    26,     2,  1640,   372,\n",
      "           434,   925,  2502,    31,  2577,   372,   434,   925,  2502,    31,\n",
      "           295,   448,   585,   392,   260,   925,   381,  1594,    14,  1098,\n",
      "          2012,   271,   260,  8612,   536,   260,   666,   396,  2978,    14,\n",
      "           935,   357,    12,   351,   341,   552,  2978,    12,   410,   260],\n",
      "        [  271,   421,  3481,   589,    12,   270,   351,   404,   348,    12,\n",
      "           921,   344,   259,  1392,    12,  1121,   283,   341,  1370,   260,\n",
      "          1539,    14,  3481, 10928,   432,  3149,   341,   624,   822,   303,\n",
      "           260,   922,    14,   590,   469,  2388,   466,  1156,  3618,   351,\n",
      "           433,    12,   270,   559,   561,   922,   618,   270,   594,  6557]])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, validation_dataloder = get_dataloaders(batch_size=2, num_workers=0)\n",
    "\n",
    "\n",
    "for x,y in train_dataloader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a61251-8887-4465-9fc0-8dbb1e8d3758",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "Words are represented in a continous space. The idea is in this new vector space, the words semantically close to each other should be also close in vector space and we should be able to use standard distance functions, like euclidean and cosine to find the similarity between words.embedding. It is easy to imagine a 2 dimensional space. In this 2d space, each word is represented by a co-ordinate.\n",
    "\n",
    "Look at the following figure\n",
    "\n",
    "\n",
    "Figure 3 Word Embedding.\n",
    "(reference:word_embedding)=\n",
    "```{figure} ../../images/chapter1/word_embedding.drawio.png\n",
    "---\n",
    "height: 250px\n",
    "name: preprocessing\n",
    "---\n",
    "Word Embedding example\n",
    "```\n",
    "\n",
    "In the figure, word King and Man are represented in a 2D continous space. With this represenation, we can now compare these two words. Words semantically close to each other should be close to each other in this 2D vector space. For illustration purpose we had kept the vector dimension to 2. In LLMs these are much larger than 2. GPT uses 12288 dimension embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "420f4363-261e-49ad-bd9d-cf87ecd55a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1568e-01,  1.9251e-01,  1.6439e-01,  ...,  8.1474e-01,\n",
       "           2.1049e+00, -9.9629e-01],\n",
       "         [-2.4471e-01, -2.6456e-01, -8.3230e-01,  ...,  5.6894e-01,\n",
       "           1.2458e-01, -6.7888e-02],\n",
       "         [ 1.3079e+00, -2.0478e+00,  2.9307e+00,  ...,  2.2039e-01,\n",
       "          -9.5471e-01, -7.1041e-01],\n",
       "         ...,\n",
       "         [ 1.4433e+00,  9.8329e-01, -1.4033e-01,  ...,  2.0992e-02,\n",
       "          -2.5070e-01, -1.9180e-01],\n",
       "         [ 2.3441e-01, -1.5142e+00, -1.3133e-01,  ...,  1.2781e+00,\n",
       "           1.3794e+00, -1.2916e-01],\n",
       "         [ 4.1121e-04, -3.6518e-01,  1.5661e+00,  ...,  2.3526e+00,\n",
       "           5.3088e-01, -6.4193e-01]],\n",
       "\n",
       "        [[-6.3845e-02,  8.5167e-01,  1.4399e-01,  ...,  4.2325e-01,\n",
       "           4.8302e-01,  1.4194e+00],\n",
       "         [ 1.5958e+00, -1.0533e+00,  5.0496e-01,  ..., -2.6729e-01,\n",
       "           6.8486e-01,  2.4605e+00],\n",
       "         [-9.4344e-01, -4.9438e-01, -1.1369e+00,  ..., -8.5355e-01,\n",
       "          -8.4444e-01,  1.6266e+00],\n",
       "         ...,\n",
       "         [ 2.0965e-01, -4.5694e-01,  1.0964e+00,  ..., -7.0590e-01,\n",
       "          -1.0948e+00,  1.0518e+00],\n",
       "         [-9.4344e-01, -4.9438e-01, -1.1369e+00,  ..., -8.5355e-01,\n",
       "          -8.4444e-01,  1.6266e+00],\n",
       "         [-5.9748e-01, -8.2814e-02,  5.2005e-01,  ..., -2.1455e+00,\n",
       "          -6.2619e-02, -5.2003e-01]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "encoded_input, encoded_taret = next(iter(train_dataloader))\n",
    "embedding_dim  = 32\n",
    "vocab_size = 52000\n",
    "\n",
    "wte = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "embedded_input = wte(encoded_input)\n",
    "# (batch_size, context_window, embedding_dimension)\n",
    "print(embedded_input.shape)\n",
    "embedded_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea559",
   "metadata": {},
   "source": [
    "nn.Embedding from pytroch is a trainable lookup table. We intialized it using the size of our vocabulary and expected dimension for embedding. Each word in our vocabulary is a row in this lookup. When we pass the token ids to this lookup we get the embedding vector for that token id. Each token is now a 32 dimension vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28fcb72",
   "metadata": {},
   "source": [
    "Initially these embeddings are random. As the model trains, the word embeddings are learned. It is a design choice to load a pre-learned embedding and either keep them outside the model learning. During the learning process, distribution semantics of the words are leveraged to place semantically similar words close to each other in the new embedding vector space. According to distributional semantics, words with similar meanings are more likely to occur in similar context. When a large corpus is used for training, we hope to provide visibility to such contexts to our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e75fb-92f4-4004-a2e1-201189249be8",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "\n",
    "\"A women is nothing without her man\"\n",
    "\"A man is nothing without her women\"\n",
    "\n",
    "These two sentences share the same words. They will hence share the same embeddings. For neural network both the sentences mean the same. But we know they convey a different meaning. The model needs to be aware of the position of the tokens in the input. This is where position embedding comes to play.\n",
    "\n",
    "A simple solution is to have a embedding dictionary similar to word embedding. A dictionary with an entry for each position.\n",
    "\n",
    "\n",
    "```note\n",
    "context window defines the maximum size of the input an LLM can ingest, the maximum number of tokens it can ingest for it to generate a response. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "69c4c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 2 context_window 50\n",
      "torch.Size([2, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_window = 50\n",
    "embedding_dim = 32\n",
    "\n",
    "pe = nn.Embedding(context_window, embedding_dim)\n",
    "\n",
    "input_length = len(encoded_input[-1])\n",
    "batch_size = encoded_input.shape[0]\n",
    "print(f\"Batch size {batch_size} context_window {input_length}\")\n",
    "positions = torch.tensor(range(input_length)).repeat(2,1)\n",
    "print(positions.shape)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d4742fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0659,  0.3402,  0.4823,  ...,  0.0778, -1.2551,  0.1716],\n",
       "         [-1.8011,  0.0123, -0.1828,  ..., -0.1942, -0.7024,  0.6973],\n",
       "         [ 0.2235, -1.4827,  0.3727,  ...,  0.9452,  2.1436,  0.4244],\n",
       "         ...,\n",
       "         [-0.1704,  0.2706, -0.5948,  ..., -1.7537, -0.5171,  1.7966],\n",
       "         [ 0.8878,  1.1431, -0.0761,  ..., -0.0164, -0.9832, -0.6537],\n",
       "         [ 0.0917, -0.4515,  0.7106,  ..., -0.0086, -1.1809,  0.0525]],\n",
       "\n",
       "        [[-0.0659,  0.3402,  0.4823,  ...,  0.0778, -1.2551,  0.1716],\n",
       "         [-1.8011,  0.0123, -0.1828,  ..., -0.1942, -0.7024,  0.6973],\n",
       "         [ 0.2235, -1.4827,  0.3727,  ...,  0.9452,  2.1436,  0.4244],\n",
       "         ...,\n",
       "         [-0.1704,  0.2706, -0.5948,  ..., -1.7537, -0.5171,  1.7966],\n",
       "         [ 0.8878,  1.1431, -0.0761,  ..., -0.0164, -0.9832, -0.6537],\n",
       "         [ 0.0917, -0.4515,  0.7106,  ..., -0.0086, -1.1809,  0.0525]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding = pe(positions)\n",
    "print(position_embedding.shape)\n",
    "position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a709438",
   "metadata": {},
   "source": [
    "Finally we add the word token embedding and position embedding to feed as input to the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "302fe6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 32])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_transformer = embedded_input + position_embedding\n",
    "input_to_transformer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dfdbc",
   "metadata": {},
   "source": [
    "We encoded the absolute position of each toke thus the name absolute positoinal embedding. However one drawback here is each position's embedding is independent of the other. From a models perspective it will not know how far is position 500 from 2. To summarize position embedding should be monotonic. The more two tokens are closer to each other, they should influence each other.\n",
    "\n",
    "\n",
    "### Relative Position embedding\n",
    "\n",
    "Relative position embedding leverages the distance between pairs of tokens. These techniques aler the attention mechanism. More about attention mechanism in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d05155",
   "metadata": {},
   "source": [
    "## Pre-training data engineering\n",
    "\n",
    "```{figure} ../../images/chapter1/data_pipeline.png\n",
    "---\n",
    "height: 350px\n",
    "name: Data Engineering\n",
    "---\n",
    "Data Engineering pipeline\n",
    "```\n",
    "\n",
    "The typical approach is concat-and-chunk. They convert text datasets with variable document lengths into sequences with a fixed target length. First we randomly shuffle and concatenate all tokenized documents. Consequtive concatenated documents are separated by a special token '<EOT>', allowing models to discover document boundaries. We then chunk the concatenated sequence into subsequences with a target sequence length. For example 2048 and 4096 for llama1 and llama2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### Data sources\n",
    "    \n",
    "    Common datasources include The Pile, RefinedWeb, RedPajma and DOLMA.\n",
    "\n",
    "### Data pipeline\n",
    "\n",
    "Filtration, deduplication, diversifacation.\n",
    "    \n",
    "1. A classifier to classify a document as high quality or low quality. Then documents are passed through this classifier and only high quality documents are filtered.\n",
    "    \n",
    "2. MinHashLSH techniques are used for deduplication. Deduplication is done with the document and across the documents.\n",
    "    \n",
    "3. For diversification, other curated, tailored datasets are included.\n",
    "    \n",
    "\n",
    "### Data quality\n",
    "\n",
    "### Data Bias\n",
    "\n",
    "### Privacy and eithical cosiderations\n",
    "    \n",
    "### Synthetic data generation through LLM\n",
    "    \n",
    "\n",
    "    Microsoft Phi models were mostly trained on synthetic data.\n",
    "    Cosmopedia a dataset consisting of synthetic textbooks, blog posts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. Has 30 million files and 25 billion tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47330be2-a1c6-4c7f-b82d-1164769addd8",
   "metadata": {},
   "source": [
    "## Conclustion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2ed1-b07a-4192-b7bc-3f697062b817",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
