{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577f4d2e-3a06-48ce-859f-9ca23622973a",
   "metadata": {},
   "source": [
    "# Chapter 9 - Rag based applications\n",
    "\n",
    "Building a chatbot using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea49650-4edd-4018-a7ed-cb3ce0eec6be",
   "metadata": {},
   "source": [
    "## What is RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47724eb3-433e-4586-aa52-779642409c03",
   "metadata": {},
   "source": [
    "## QandA with RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b253d-02c5-47fe-990f-5b9305c51d0c",
   "metadata": {},
   "source": [
    "### Extract text from a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b690cf5-d8a7-4dcf-ba31-02a830e4d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/64317/pg64317-images.html\" \n",
    "\n",
    "response = requests.get(url)\n",
    "page_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "a    = soup.find('div', attrs={\"class\" : \"container\"})\n",
    "text = a.parent.parent.get_text()\n",
    "start = re.escape(\"*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\")\n",
    "end = re.escape(\"*** END OF THE PROJECT GUTENBERG EBOOK\")\n",
    "text = re.search('{}(.*){}'.format(start, end), text, re.S).group(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5afa22-2c3b-424c-9caa-5a82ef529ee7",
   "metadata": {},
   "source": [
    "### Create documents to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c316c20-4580-4ae7-91f1-2e85c2a2f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "doc =  Document(page_content=text, metadata={\"source\": \"local\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74439dfa-595e-4c00-bb17-6501a87ca8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The Great GatsbybyF. Scott Fitzgerald\\n\\nTable of Contents\\n\\n\\nI\\n\\n\\nII\\n\\n\\nIII\\n\\n\\nIV\\n\\n\\nV\\n\\n\\nVI\\n\\n\\nVII\\n\\n\\nVIII\\n\\n\\nIX\\n\\n\\n\\n\\n\\r\\nOnce again\\r\\nto\\r\\nZelda\\r\\n\\n\\n\\n\\n\\nThen wear the gold hat, if that will move her;\\n\\nIf you can bounce high, bounce for her too,\\n\\nTill she cry “Lover, gold-hatted, high-bouncing lover,\\n\\nI must have you!”\\n\\n\\nThomas Parke d’Invilliers\\n\\n\\n\\n\\nI\\n\\r\\nIn my younger and more vulnerable years my father gave me some advice that I’ve been turning over in my mind ever since.\\r\\n\\n\\r\\n“Whenever you feel like criticizing anyone,” he told me, “just remember that all the people in this world haven’t had the advantages that you’ve had.”', metadata={'source': 'local', 'start_index': 2})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents([doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38ae80-4778-42fc-8ae8-4b31a517cb7e",
   "metadata": {},
   "source": [
    "### Index the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0fdb69e-fecd-4a53-bc71-58e1c66cedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af2757-0d0e-4b92-bebd-5dfda73fffcb",
   "metadata": {},
   "source": [
    "## Similarity based retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53efaf5e-ae84-4941-b4c9-946795510e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac138870-48a0-4c09-9608-5a37d6fc8f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With fenders spread like wings we scattered light through half Astoria—only half, for as we twisted among the pillars of the elevated I heard the familiar “jug-jug-spat!” of a motorcycle, and a frantic policeman rode alongside.\n",
      "\n",
      "\n",
      "“All right, old sport,” called Gatsby. We slowed down. Taking a white card from his wallet, he waved it before the man’s eyes.\n",
      "\n",
      "\n",
      "“Right you are,” agreed the policeman, tipping his cap. “Know you next time, Mr. Gatsby. Excuse me!”\n",
      "\n",
      "\n",
      "“What was that?” I inquired. “The picture of Oxford?”\n",
      "\n",
      "\n",
      "“I was able to do the commissioner a favour once, and he sends me a Christmas card every year.”\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3eb269-08d3-4568-a315-58a163dc7caf",
   "metadata": {},
   "source": [
    "### Q&A Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e896468-397e-4662-94bb-8baf054bad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 57s, sys: 15.7 s, total: 5min 13s\n",
      "Wall time: 53.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It happened, and that’s all I know.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# HuggingFace Pipeline\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n",
    "## Langchain pipeline\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "template = \"\"\"You are an assistant for question answering task.Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "%time rag_chain.invoke(\"Where did the accident happen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92191b9-5e9a-4050-87b6-9c31bee2a5a4",
   "metadata": {},
   "source": [
    "## Chat with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c7b0585-5db2-417b-b053-b5d4a2e2da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b948bb7a-5f84-494e-8373-8132a2963901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom Buchanan was in love with Daisy for several years before he knew that she was involved in some criminal activity. At first, he suspected nothing, but one day he saw Daisy leave her home with another woman and he discovered that Daisy had a driver’s license. When Tom confronted Daisy about it, she attempted to run away and tore open the back of her neck, killing her instantly.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"Who is Daisy?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"How did she die?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b626423-72c1-4bf5-b70d-a598d2a94d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def echo(message, history):\n",
    "    question = message\n",
    "    ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "    return ai_msg\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(fn=echo, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"The Great Gatsby Bot\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9aa7db-8d00-4cbb-859b-a8a12c899e41",
   "metadata": {},
   "source": [
    "## Dolly Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a1a6c3d-25dd-47b0-a713-766e2bf8773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"cpu\", return_full_text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384e8752-f859-4d6e-8733-3af337144948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972957ac-8c64-45df-bb8c-cd3a596e9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fission breaks down an atom into smaller atoms, while fusion together the smaller atomic nuclei to form one larger atomic nucleus.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511be35f-821e-49f9-98a2-a691c8790bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington was president from 1789 to 1797.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\n",
    "and Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n",
    "\n",
    "print(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eee23d-8e96-44b7-b58e-7dc3df7fa820",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    Improve the quality of the embeddings: You're currently using the sentence-transformers/all-mpnet-base-v2 model for embeddings. While this is a good general-purpose model, you might get better results with a model that's more specialized for your specific task. For example, you could try using a model that's been fine-tuned on a dataset of similar documents to your operation manual.\n",
    "\n",
    "    Optimize the search: You're currently using the FAISS vector store for similarity search. While FAISS is a good choice for large-scale similarity search, it might not be the most efficient choice for your specific task. You could try using a different vector store, such as Pinecone or Weaviate, to see if they provide better performance.\n",
    "\n",
    "    Optimize the text splitting: You're currently splitting the text into chunks of 1000 characters with an overlap of 200 characters. This might be too fine-grained, resulting in a lot of redundant computations. You could try increasing the chunk size and reducing the overlap to improve efficiency.\n",
    "\n",
    "    Optimize the prompt: You're currently using a generic prompt template. You might get better results by customizing the prompt to better match the style and content of your operation manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2c033a-d2ab-43bb-9d51-b3516c82a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_74360/4080736814.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3/dist-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_74360/4080736814.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/home/gopi/.local/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3/dist-packages/bottleneck/__init__.py\", line 2, in <module>\n",
      "    from .reduce import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020e558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
