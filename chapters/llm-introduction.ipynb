{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831f32a6",
   "metadata": {},
   "source": [
    "# LLM Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c65c0-3022-40c5-8803-26ed1f3acee1",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama enables us to run open source LLMs locally. A tool that enables local deployment of large language models. A great tool for experimetning with LLMs. No need cloud hosting.\n",
    "http://github.com/ollama/ollama. Refer to the website for installation.\n",
    "\n",
    "Advantages for using Ollama,\n",
    "\n",
    "1. Quick iteratvie development without needing to deploy model changes\n",
    "2. Privacy and security - data does not leave your machine.\n",
    "3. cost - more cost effective than making API calls.\n",
    "4. control - more control over the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e80020-d6e3-41c0-b8c4-7a1a7cf84bfc",
   "metadata": {},
   "source": [
    "    curl  -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    >>> Installing ollama to /usr/local/bin...\n",
    "    >>> Creating ollama user...\n",
    "    >>> Adding ollama user to render group...\n",
    "    >>> Adding ollama user to video group...\n",
    "    >>> Adding current user to ollama group...\n",
    "    >>> Creating ollama systemd service...\n",
    "    >>> Enabling and starting ollama service...\n",
    "    Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.\n",
    "    >>> NVIDIA GPU installed.\n",
    "\n",
    "    sudo systemctl status ollama\n",
    "    ● ollama.service - Ollama Service\n",
    "         Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled)\n",
    "         Active: active (running) since Sat 2024-05-11 19:59:42 EDT; 5min ago\n",
    "       Main PID: 1053781 (ollama)\n",
    "          Tasks: 18 (limit: 18827)\n",
    "         Memory: 480.5M\n",
    "         CGroup: /system.slice/ollama.service\n",
    "                 └─1053781 /usr/local/bin/ollama serve\n",
    "    \n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: Couldn't find '/usr/share/ollama/.ollama/id_ed25519'. Generating new private key.\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: Your new public key is:\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPkwrlZDwOjnkF2xAiAWVxs8CrIfsqSnNcs3adQQv9xC\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: 2024/05/11 19:59:42 routes.go:1006: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_>\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:42.753-04:00 level=INFO source=images.go:704 msg=\"total blobs: 0\"\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:42.753-04:00 level=INFO source=images.go:711 msg=\"total unused blobs removed: 0\"\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:42.753-04:00 level=INFO source=routes.go:1052 msg=\"Listening on 127.0.0.1:11434 (version 0.1.36)\"\n",
    "    May 11 19:59:42 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:42.753-04:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama769771873/runners\n",
    "    May 11 19:59:44 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:44.699-04:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60002]\"\n",
    "    May 11 19:59:45 gopi-G5-MD ollama[1053781]: time=2024-05-11T19:59:45.929-04:00 level=INFO source=types.go:71 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"15.4 GiB\" avail>\n",
    "    lines 1-19/19 (END)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be934afd-2196-473d-ba30-8012c07329f4",
   "metadata": {},
   "source": [
    "## List of available models\n",
    "\n",
    "    ollama list\n",
    "    NAME\tID\tSIZE\tMODIFIED \n",
    "\n",
    "## Pull a model\n",
    "\n",
    "     ollama pull phi3\n",
    "    pulling manifest \n",
    "    pulling 4fed7364ee3e... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 2.3 GB                         \n",
    "    pulling c608dc615584... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  149 B                         \n",
    "    pulling fa8235e5b48f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB                         \n",
    "    pulling d47ab88b61ba... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  140 B                         \n",
    "    pulling f7eda1da5a81... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  485 B                         \n",
    "    verifying sha256 digest \n",
    "    writing manifest \n",
    "    removing any unused layers \n",
    "    success \n",
    "\n",
    "\n",
    "    (base) gopi@gopi-G5-MD:~/Documents/small_llm$ ollama list\n",
    "    NAME       \tID          \tSIZE  \tMODIFIED       \n",
    "    phi3:latest\ta2c89ceaed85\t2.3 GB\t19 seconds ago\t\n",
    "\n",
    "\n",
    "Now listing the model shows phi3 model. The models are stored as blob in /usr/share/ollama/.ollama\n",
    "\n",
    "Ollama bundles the model into a single package defined by a Modelfile. Let us see the modelfile for phi3.\n",
    "\n",
    "    >>> /show modelfile\n",
    "    # Modelfile generate by \"ollama show\"\n",
    "    # To build a new Modelfile based on this, replace FROM with:\n",
    "    # FROM phi3:latest\n",
    "    \n",
    "    FROM /usr/share/ollama/.ollama/models/blobs/sha256-4fed7364ee3e0c7cb4fe0880148bfdfcd1b630981efa0802a6b62ee52e7da97e\n",
    "    TEMPLATE \"{{ if .System }}<|system|>\n",
    "    {{ .System }}<|end|>\n",
    "    {{ end }}{{ if .Prompt }}<|user|>\n",
    "    {{ .Prompt }}<|end|>\n",
    "    {{ end }}<|assistant|>\n",
    "    {{ .Response }}<|end|>\n",
    "    \"\n",
    "    PARAMETER stop <|user|>\n",
    "    PARAMETER stop <|assistant|>\n",
    "    PARAMETER stop <|system|>\n",
    "    PARAMETER stop <|end|>\n",
    "    PARAMETER stop <|endoftext|>\n",
    "    PARAMETER num_keep 4\n",
    "    LICENSE \"\"\"Microsoft.\n",
    "    Copyright (c) Microsoft Corporation.\n",
    "    \n",
    "    MIT License\n",
    "    \n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "    of this software and associated documentation files (the \"Software\"), to deal\n",
    "    in the Software without restriction, including without limitation the rights\n",
    "    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "    copies of the Software, and to permit persons to whom the Software is\n",
    "    furnished to do so, subject to the following conditions:\n",
    "    \n",
    "    The above copyright notice and this permission notice shall be included in all\n",
    "    copies or substantial portions of the Software.\n",
    "    \n",
    "    THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "    SOFTWARE.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c45969-764a-497f-b218-4d0f94b0175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interacting with LLM\n",
    "\n",
    "    ollama run phi3:latest\n",
    "    >>> Why is the sky blue in color?\n",
    "     The sky appears blue to our eyes because of a phenomenon called Rayleigh scattering. As sunlight travels through Earth's atmosphere, it encounters molecules and small particles that are much \n",
    "    smaller than its wavelength. These particles scatter shorter-wavelength light (blue and violet) more effectively than longer-wavelength light (red, orange).\n",
    "    \n",
    "    However, our eyes are less sensitive to violet light, and the Sun emits less violet light compared to blue light; that's why we see a predominantly blue sky during daytime. At sunrise or sunset, \n",
    "    when the light path through the atmosphere is longer, even more scattering of red and orange wavelengths occurs, which makes the sky appear in shades of red, pink, and orange.\n",
    "    \n",
    "    The overall color perception can also be affected by atmospheric conditions such as pollution or dust particles. But under normal circumstances on a clear day, Rayleigh scattering is responsible \n",
    "    for the blue coloration of our sky.\n",
    "    \n",
    "    >>> Send a message (/? for help)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6f04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
