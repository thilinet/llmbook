{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6583ca-37f6-497a-b6c0-60b63a784164",
   "metadata": {},
   "source": [
    "# Chapter 3 - Pre-train a tiny LLM\n",
    "\n",
    "In this chapter we will train our tiny LLM using simplebooks dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2c8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.absolute())\n",
    "sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf021",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd56d36",
   "metadata": {},
   "source": [
    "Get the train test and validation dataloaders from the code we wrote in previous chapter. We will use the gpt2 tokenizer from Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe22224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from chapter1.simplebooks import get_dataloaders, get_tokenizer\n",
    "\n",
    "\n",
    "# Load train,validation and test datasets\n",
    "train_loader, valid_loader, test_loader = get_dataloaders(batch_size=12, \\\n",
    "                num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678449",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f2470",
   "metadata": {},
   "source": [
    "We have close to 1.7 million tokens for training. 193K tokens for validation and 192K tokenz for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter2.gptlikemodel import SLLM, SLLMConfig\n",
    "\n",
    "\n",
    "# Initialize the model class\n",
    "config = SLLMConfig()\n",
    "\n",
    "print(f\"Model configuration {config}\\n\")\n",
    "\n",
    "model = SLLM(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bac87",
   "metadata": {},
   "source": [
    "## Query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167e3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chapter1.simplebooks import get_tokenizer\n",
    "\n",
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    Generate output tokens from a given model.\n",
    "    Arguments:\n",
    "        model: \n",
    "            llm model for text generation\n",
    "        idx:\n",
    "            Input token tensor\n",
    "        max_new_tokens:\n",
    "            Number of output tokens to be generated\n",
    "        context_size:\n",
    "            model context window.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_trim = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_trim)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def invoke_model(model, start_context):\n",
    "    \n",
    "    assert len(start_context) > 0 \\\n",
    "        and start_context is not None\n",
    "        \n",
    "    print(f\"Input context: '{start_context}'\\n\")\n",
    "    tokenizer = get_tokenizer()\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    \n",
    "    # convert to tensor and add batch dimension\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    print(f\"Encoded tensor {encoded_tensor} No Tokens: {encoded_tensor.size()[-1]} \\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "    print(f\"Output {out} No Tokens: {out.size()[-1]}\")\n",
    "    \n",
    "    decoded_text = tokenizer.decode(out.squeeze(0))\n",
    "    print(f\"Decoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607383f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "model = model.to(\"cpu\")\n",
    "invoke_model(model, start_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e57659",
   "metadata": {},
   "source": [
    "The output tokensize is 11, 5 more than the input token size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930b6a5",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class LLMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LLMLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), targets.flatten())\n",
    "        return loss\n",
    "\n",
    "        \n",
    "def batch_loss(loss_fn, input_batch,target_batch, model, device):\n",
    "\n",
    "    assert model is not None\n",
    "    assert input_batch is not None \n",
    "    assert target_batch is not None\n",
    "\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_batch)\n",
    "        loss   = loss_fn(logits, target_batch)\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loader_loss(loss_fn, data_loader, model, device=\"cpu\"):\n",
    "\n",
    "    assert data_loader is not None\n",
    "    assert model is not None\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "\n",
    "        features, target = batch\n",
    "        loss = batch_loss(loss_fn, features, target, model, device)\n",
    "        total_loss+=loss\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf187ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "batch_no = 1\n",
    "for batch in train_loader:\n",
    "    \n",
    "    features, target = batch\n",
    "    loss = batch_loss(loss_fn, features, target, model, device)\n",
    "    \n",
    "    print(f\"Batch {batch_no} Loss {loss}\")\n",
    "    batch_no+=1\n",
    "    \n",
    "    if batch_no > 2:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f87355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = loader_loss(loss_fn, train_loader, model,device)\n",
    "\n",
    "print(f\"Train data loss {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40703f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0be87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Learning rate warmup\n",
    "\n",
    "n_epochs = 10\n",
    "initial_lr = 1e-4\n",
    "min_lr = 1e-6\n",
    "top_lr = 0.01\n",
    "warmup_steps = 20\n",
    "total_training_steps = n_epochs * len(train_loader)\n",
    "device = \"cuda\"\n",
    "progress_bar = tqdm(range(total_training_steps))\n",
    "eval_freq = 500\n",
    "\n",
    "lr_increment = (top_lr - initial_lr) / warmup_steps\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "global_steps = -1\n",
    "tokens_seen = 0\n",
    "\n",
    "track_lrs = []\n",
    "\n",
    "train_losses = []\n",
    "eval_losses  = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    model.train()\n",
    "    for input_batch in train_loader:\n",
    "        \n",
    "        features, target = input_batch\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        global_steps+=1\n",
    "        \n",
    "        if global_steps < warmup_steps:\n",
    "            lr = initial_lr + global_steps * lr_increment\n",
    "        else:\n",
    "            # cosine decay\n",
    "            progress = (global_steps - warmup_steps) / (total_training_steps - warmup_steps)\n",
    "            lr = min_lr + (top_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            \n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] =lr\n",
    "        \n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, target)\n",
    "        \n",
    "        tokens_seen += features.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if global_steps > warmup_steps:\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        track_lrs.append(lr)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        \"\"\"\n",
    "        if global_steps % eval_freq == 0:\n",
    "            model.eval()\n",
    "            eval_loss = loader_loss(loss_fn, valid_loader, model)\n",
    "            model.train()\n",
    "            print(f\"Epoch {epoch} Evaluation Loss {eval_loss} LR {lr}\")\n",
    "            eval_losses.append((epoch, eval_loss))\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"Epoch {epoch} Avg Train Loss {sum(losses)/len(losses)} LR {lr}\")\n",
    "    invoke_model(model, start_context)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae24fa",
   "metadata": {},
   "source": [
    "## Learning rate warmup and Cosine decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd0ebe",
   "metadata": {},
   "source": [
    "## Load saved model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e533ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the saved model\n",
    "from chapter2.gptlikemodel import SLLM, SLLMConfig\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = os.getcwd()\n",
    "model_path   = str(Path(current_path).parent.parent.absolute())\n",
    "\n",
    "\n",
    "config = SLLMConfig()\n",
    "model = SLLM(config)\n",
    "\n",
    "save_directory = model_path + \"/bin/\"\n",
    "model_name = \"small_llm-v1-52-0.855\"\n",
    "\n",
    "model.load_state_dict(torch.load(save_directory + model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304d9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: 'It is a'\n",
      "\n",
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Encoded tensor tensor([[679, 357, 259]]) No Tokens: 3 \n",
      "\n",
      "Output tensor([[  679,   357,   259,  7485,    14, 10148, 26114,     2]]) No Tokens: 8\n",
      "Decoded text: 'It is a yeast. Page wisps\"'\n"
     ]
    }
   ],
   "source": [
    "start_context = \"It is a\"\n",
    "invoke_model(model, start_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989726e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Improvements to invoking the model for text generation\n",
    "\n",
    "* curently greedy search\n",
    "* Beam search\n",
    "* temperature\n",
    "* top-k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217d66e",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/introducing-csearch\n",
    "Deterministic methods, e.g. greedy search and beam search, generate text by selecting the text continuation with the highest likelihood measured by the language model. However, as widely discussed in previous studies [3][4], deterministic methods often lead to the problem of model degeneration, i.e., the generated text is unnatural and contains undesirable repetitions.\n",
    "\n",
    "To address the issues posed by deterministic methods, stochastic methods generate text by introducing randomness during the decoding process. Two widely-used stochastic methods are (i) top-k sampling [3] and (ii) nucleus sampling (also called top-p sampling) [4].\n",
    "\n",
    "While nucleus sampling can generate text free of repetitions, the semantic coherence of the generated text is not well-maintained. For instance, the generated phrase 'AI is not journalism' is incoherent with respect to the given prefix, i.e. 'DeepMind Company'.\n",
    "\n",
    "We note that this semantic inconsistency problem can partially be remedied by lowering the temperature. However, reducing the temperature brings nucleus sampling closer to greedy search, which can be seen as a trade-off between greedy search and nucleus sampling. Generally, it is challenging to find a prompt and model-independent temperature that avoids both the pitfalls of greedy search and nucleus sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f95c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def greedy_search(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    return idx_next\n",
    "\n",
    "\n",
    "def generate_text(model, idx, max_new_tokens\n",
    "                  , context_size\n",
    "                  , search_fn=greedy_search\n",
    "                  , temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate output tokens from a given model.\n",
    "    Arguments:\n",
    "        model: \n",
    "        \n",
    "            llm model for text generation\n",
    "        idx:\n",
    "            Input token tensor\n",
    "        max_new_tokens:\n",
    "            Number of output tokens to be generated\n",
    "        context_size:\n",
    "            model context window.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_trim = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_trim)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        idx_next = search_fn(logits=logits,temperature=temperature)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e647b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(model,tokenizer \n",
    "                 ,start_context\n",
    "                 ,search_fn=greedy_search\n",
    "                ,temperature=1.0):\n",
    "    \n",
    "    assert len(start_context) > 0 \\\n",
    "        and start_context is not None\n",
    "        \n",
    "    print(f\"Input context: '{start_context}'\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = generate_text(model, encoded_tensor, 5\n",
    "                            , context_size=50\n",
    "                            ,search_fn=search_fn\n",
    "                           ,temperature=temperature)\n",
    "    \n",
    "    decoded_text = tokenizer.decode(out.squeeze(0))\n",
    "    print(f\"Decoded text: '{decoded_text}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac3ac4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n",
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a yeast?\" Whoo- Whoo'\n",
      "\n",
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a yeast?\" Whoo- Whoo'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "\n",
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=greedy_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e94981b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_search(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.multinomial(probas, num_samples=1)\n",
    "    return idx_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f657397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a campfire!\" Painted Weasel.'\n",
      "\n",
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a lifeboat, yer, shy'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=probabilistic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2edd11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ Temperature 0.10 values [0.047 0.019 0.935]\n",
      "\ta choosen 3 times out of 50 trials\n",
      "\ttree choosen 1 times out of 50 trials\n",
      "\tspace choosen 46 times out of 50 trials\n",
      "@ Temperature 0.20 values [0.163 0.104 0.732]\n",
      "\ta choosen 9 times out of 50 trials\n",
      "\ttree choosen 3 times out of 50 trials\n",
      "\tspace choosen 38 times out of 50 trials\n",
      "@ Temperature 0.30 values [0.224 0.166 0.61 ]\n",
      "\ta choosen 7 times out of 50 trials\n",
      "\ttree choosen 9 times out of 50 trials\n",
      "\tspace choosen 34 times out of 50 trials\n",
      "@ Temperature 0.40 values [0.256 0.203 0.541]\n",
      "\ta choosen 13 times out of 50 trials\n",
      "\ttree choosen 9 times out of 50 trials\n",
      "\tspace choosen 28 times out of 50 trials\n",
      "@ Temperature 0.50 values [0.273 0.228 0.498]\n",
      "\ta choosen 19 times out of 50 trials\n",
      "\ttree choosen 13 times out of 50 trials\n",
      "\tspace choosen 18 times out of 50 trials\n",
      "@ Temperature 0.60 values [0.285 0.245 0.47 ]\n",
      "\ta choosen 12 times out of 50 trials\n",
      "\ttree choosen 8 times out of 50 trials\n",
      "\tspace choosen 30 times out of 50 trials\n",
      "@ Temperature 0.70 values [0.294 0.258 0.448]\n",
      "\ta choosen 15 times out of 50 trials\n",
      "\ttree choosen 16 times out of 50 trials\n",
      "\tspace choosen 19 times out of 50 trials\n",
      "@ Temperature 0.80 values [0.299 0.268 0.433]\n",
      "\ta choosen 19 times out of 50 trials\n",
      "\ttree choosen 14 times out of 50 trials\n",
      "\tspace choosen 17 times out of 50 trials\n",
      "@ Temperature 0.90 values [0.302 0.273 0.424]\n",
      "\ta choosen 12 times out of 50 trials\n",
      "\ttree choosen 12 times out of 50 trials\n",
      "\tspace choosen 26 times out of 50 trials\n",
      "@ Temperature 1.00 values [0.306 0.28  0.414]\n",
      "\ta choosen 13 times out of 50 trials\n",
      "\ttree choosen 13 times out of 50 trials\n",
      "\tspace choosen 24 times out of 50 trials\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = [\"a\",\"tree\",\"space\"]\n",
    "\n",
    "logits = np.asarray([0.2,0.11,0.5])\n",
    "temp_range = np.linspace(0,1,11)\n",
    "\n",
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "for temperature in temp_range:\n",
    "    if temperature > 0:\n",
    "        b = np.round(logits * 1/temperature,2)\n",
    "        b_norm = np.round(softmax(b),3)\n",
    "        print(f\"@ Temperature {temperature:.2f} values {b_norm}\")\n",
    "\n",
    "        experiments = 50\n",
    "        idxs = np.random.multinomial(experiments, b_norm)\n",
    "        \n",
    "        for word,choosen_freq in zip(words, idxs):\n",
    "            print(f\"\\t{word} choosen {choosen_freq} times out of {experiments} trials\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6be983fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scaling(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    temperature = kwargs['temperature']\n",
    "    probas = torch.softmax(logits/temperature, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    return idx_next\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33373750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a yeast. Page rap increased'\n",
      "\n",
      "Input context: 'It is a'\n",
      "Decoded text: 'It is a campfire. Page a fa'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    temperature =0.7\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=probabilistic_search, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43815f",
   "metadata": {},
   "source": [
    "## Accelarator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ad412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer, scheduler = accelerator.prepare(\n",
    "     train_loader, valid_loader, model, optimizer, lr_scheduler\n",
    " )\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "loss_fn = LLMLoss()\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        features,target = batch\n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, target)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950329f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46910f56",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = str(Path(current_path.parent.parent.absolute(), \"bin\"))\n",
    "\n",
    "# save state dictionary\n",
    "accelerator.wait_for_everyone()\n",
    "accelerator.save_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_in_model\n",
    "\n",
    "new_model = SLLM(config)\n",
    "device = accelerator.device\n",
    "load_checkpoint_in_model(model, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3be7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"wonderful spring is awaited.\"\n",
    "tokenizer = get_tokenizer()\n",
    "encoded = tokenizer.encode(start_context)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "out = generate_text(model, encoded_tensor, 5, context_size=50)\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a372e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
