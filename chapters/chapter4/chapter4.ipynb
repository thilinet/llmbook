{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0173d462",
   "metadata": {},
   "source": [
    "# Chapter 4 Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0f5de",
   "metadata": {},
   "source": [
    "## Improvements to invoking the model for text generation\n",
    "\n",
    "* curently greedy search\n",
    "* Beam search\n",
    "* temperature\n",
    "* top-k\n",
    "\n",
    "\n",
    "\n",
    "https://huggingface.co/blog/introducing-csearch Deterministic methods, e.g. greedy search and beam search, generate text by selecting the text continuation with the highest likelihood measured by the language model. However, as widely discussed in previous studies [3][4], deterministic methods often lead to the problem of model degeneration, i.e., the generated text is unnatural and contains undesirable repetitions.\n",
    "\n",
    "To address the issues posed by deterministic methods, stochastic methods generate text by introducing randomness during the decoding process. Two widely-used stochastic methods are (i) top-k sampling [3] and (ii) nucleus sampling (also called top-p sampling) [4].\n",
    "\n",
    "While nucleus sampling can generate text free of repetitions, the semantic coherence of the generated text is not well-maintained. For instance, the generated phrase 'AI is not journalism' is incoherent with respect to the given prefix, i.e. 'DeepMind Company'.\n",
    "\n",
    "We note that this semantic inconsistency problem can partially be remedied by lowering the temperature. However, reducing the temperature brings nucleus sampling closer to greedy search, which can be seen as a trade-off between greedy search and nucleus sampling. Generally, it is challenging to find a prompt and model-independent temperature that avoids both the pitfalls of greedy search and nucleus sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03866751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def greedy_search(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    return idx_next\n",
    "\n",
    "\n",
    "def generate_text(model, idx, max_new_tokens\n",
    "                  , context_size\n",
    "                  , search_fn=greedy_search\n",
    "                  , temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate output tokens from a given model.\n",
    "    Arguments:\n",
    "        model: \n",
    "        \n",
    "            llm model for text generation\n",
    "        idx:\n",
    "            Input token tensor\n",
    "        max_new_tokens:\n",
    "            Number of output tokens to be generated\n",
    "        context_size:\n",
    "            model context window.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_trim = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_trim)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        idx_next = search_fn(logits=logits,temperature=temperature)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93eaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(model,tokenizer \n",
    "                 ,start_context\n",
    "                 ,search_fn=greedy_search\n",
    "                ,temperature=1.0):\n",
    "    \n",
    "    assert len(start_context) > 0 \\\n",
    "        and start_context is not None\n",
    "        \n",
    "    print(f\"Input context: '{start_context}'\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = generate_text(model, encoded_tensor, 5\n",
    "                            , context_size=50\n",
    "                            ,search_fn=search_fn\n",
    "                           ,temperature=temperature)\n",
    "    \n",
    "    decoded_text = tokenizer.decode(out.squeeze(0))\n",
    "    print(f\"Decoded text: '{decoded_text}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "\n",
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=greedy_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63339264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_search(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.multinomial(probas, num_samples=1)\n",
    "    return idx_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b34d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=probabilistic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = [\"a\",\"tree\",\"space\"]\n",
    "\n",
    "logits = np.asarray([0.2,0.11,0.5])\n",
    "temp_range = np.linspace(0,1,11)\n",
    "\n",
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "for temperature in temp_range:\n",
    "    if temperature > 0:\n",
    "        b = np.round(logits * 1/temperature,2)\n",
    "        b_norm = np.round(softmax(b),3)\n",
    "        print(f\"@ Temperature {temperature:.2f} values {b_norm}\")\n",
    "\n",
    "        experiments = 50\n",
    "        idxs = np.random.multinomial(experiments, b_norm)\n",
    "        \n",
    "        for word,choosen_freq in zip(words, idxs):\n",
    "            print(f\"\\t{word} choosen {choosen_freq} times out of {experiments} trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scaling(**kwargs):\n",
    "    logits = kwargs['logits']\n",
    "    temperature = kwargs['temperature']\n",
    "    probas = torch.softmax(logits/temperature, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    return idx_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    start_context = \"It is a\"\n",
    "    temperature =0.7\n",
    "    invoke_model(model,tokenizer,start_context,search_fn=probabilistic_search, temperature=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
