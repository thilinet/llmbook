{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a5f508-c751-431d-b8e9-f15e3d047983",
   "metadata": {},
   "source": [
    "# Chapter 6 - Parameter Effiecient Fine Tuning\n",
    "\n",
    "LORA, QLORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e0da1",
   "metadata": {},
   "source": [
    "Parameter effiecient fine tuning is a family of techniques to adapt a pretrained model for new task in an effiecient manner; these techniques are tailored to use less compute power, far less memory and storage.\n",
    "\n",
    "LoRA stands for low rank adaptations. A technique to adapt a pretrained model for a specific downstream NLP task. In the previous chapter we looked at full fine tuning technique of a pretrained model. \n",
    "\n",
    "* reduced number of weight updates\n",
    "* separate the fine-tuned weight from pretrained weights. This allows to have smaller model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d58564c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "current_path = Path(os.getcwd())\n",
    "parent_path  = str(current_path.parent.absolute())\n",
    "sys.path.append(parent_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e489a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter2.gptlikemodel import SLLM, SLLMConfig\n",
    "\n",
    "\n",
    "def print_no_parameters(model, custom_msg=\"\"):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,} {custom_msg}\")\n",
    "    \n",
    "\n",
    "def get_model():\n",
    "    # Initialize the model class\n",
    "    config = SLLMConfig()\n",
    "    print(f\"Model configuration {config}\\n\")\n",
    "    model = SLLM(config)\n",
    "    print_no_parameters(model)\n",
    "\n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7e186af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration SLLMConfig(d_model=128, d_head=128, bias=False, dropout=0.0, context_window=50, n_heads=2, vocab_size=52000, n_layers=2)\n",
      "\n",
      "Total parameters: 13,698,976 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SLLM(\n",
       "  (embedding_block): EmbeddingsBlock(\n",
       "    (token_embdgs): Embedding(52000, 128)\n",
       "    (pos_embdgs): Embedding(50, 128)\n",
       "    (droput): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (mha): MultiHeadAttentionV1(\n",
       "        (projection_out): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (Wq): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (Wk): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (Wv): Linear(in_features=128, out_features=256, bias=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (ln_1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=128, out_features=52000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,config = get_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484974c",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "Low Rank Adaptation. Replace an existing weight matrices in a model with low rank counter part. For example, consider the following matrix.\n",
    "\n",
    "gradient descent weight update\n",
    "\n",
    "w = w + learning_rate * delta_w\n",
    "where delta_w is the gradients of weights w.r.t loss.\n",
    "\n",
    "once the weights are calculate, x is projected onto w.\n",
    "\n",
    "x*w\n",
    "\n",
    "we can rewrite the above equation as\n",
    "\n",
    "x * (w + learning_rate * delta_w)\n",
    "\n",
    "x*w + learning_rate * (x * delta_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b678b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self,in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        \n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        \n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora  = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b8d09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration SLLMConfig(d_model=128, d_head=128, bias=False, dropout=0.0, context_window=50, n_heads=2, vocab_size=52000, n_layers=2)\n",
      "\n",
      "Total parameters: 13,698,976 \n",
      "Total parameters: 13,698,976 before freezing\n",
      "Total parameters: 0 after freezing\n",
      "replace ln_1\n",
      "replace c_proj\n",
      "replace ln_1\n",
      "replace c_proj\n",
      "Total parameters: 16,384 to train with LoRA\n"
     ]
    }
   ],
   "source": [
    "model,config = get_model()\n",
    "\n",
    "print_no_parameters(model, \"before freezing\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print_no_parameters(model, \"after freezing\")\n",
    "\n",
    "replace_modules = [\"ln_1\", \"c_proj\"]\n",
    "rank = 16\n",
    "alpha = 30\n",
    "\n",
    "for module in model.modules():\n",
    "    for name, submodule in module.named_children():\n",
    "        if isinstance(submodule, torch.nn.Linear):\n",
    "            if name in replace_modules:\n",
    "                print(f\"replace {name}\")\n",
    "                setattr(submodule, name, LinearLoRA(submodule, rank, alpha))\n",
    "                \n",
    "print_no_parameters(model, \"to train with LoRA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d56d0400",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while getting the str of an object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2550\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2552\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2553\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m _addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2554\u001b[0m     child_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mod_str)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:332\u001b[0m, in \u001b[0;36mModuleList.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a custom repr for ModuleList that compresses repeated module representations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     list_of_reprs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mrepr\u001b[39m(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_reprs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_name() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a custom repr for ModuleList that compresses repeated module representations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     list_of_reprs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_reprs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_name() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2550\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2552\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2553\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m _addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2554\u001b[0m     child_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mod_str)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2550\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2552\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2553\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m _addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2554\u001b[0m     child_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mod_str)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.__repr__ at line 2552 (981 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2550\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2552\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2553\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m _addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2554\u001b[0m     child_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mod_str)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2546\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;66;03m# We treat the extra repr like the sub-module, one item per line\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m     extra_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2546\u001b[0m     extra_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# empty string will be split into list ['']\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extra_repr:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:120\u001b[0m, in \u001b[0;36mLinear.extra_repr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextra_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, out_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, bias=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while getting the str of an object"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35a99f",
   "metadata": {},
   "source": [
    "## Adapters\n",
    "\n",
    "Save only the layers we have replaced as adapters. Later load the model weights\n",
    "and replace the layers with weights we have stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6726ba1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SLLM' object has no attribute 'ln_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SLLM' object has no attribute 'ln_1'"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5fe4f",
   "metadata": {},
   "source": [
    "## Prompt Tuning\n",
    "\n",
    "In the previous chapter, we did a full fine tuning of a model for a classification task. With prompt tuning we can turn any task into a generation task. Introduced in the paper {cite}`lester2021powerscaleparameterefficientprompt`. Quoting from the paper \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples\"\n",
    "\n",
    "Given a series of n tokens, {x1, x2, . . . , xn}, the\n",
    "first thing T5 does is embed the tokens, forming\n",
    "a matrix Xe ∈ Rn×e where e is the dimension of\n",
    "the embedding space. Our soft-prompts are repre-\n",
    "sented as a parameter Pe ∈ Rp×e, where p is the\n",
    "length of the prompt. Our prompt is then concate-\n",
    "nated to the embedded input forming a single ma-\n",
    "trix [Pe; Xe] ∈ R(p+n)×e which then flows though\n",
    "the encoder-decoder as normal. Our models are\n",
    "trained to maximize the probability of Y , but only\n",
    "the prompt parameters Pe are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "259eea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/gopi/Documents/small_llm/llmbook/data/simplebooks-tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([20978,  7634,   475,   260,  2432,  3235,   357,   259, 19962,   493,\n",
       "          467, 19962,    14,   199])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chapter1.simplebooks import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "prompt_tuning_init_text = \"Classify if the feedback is a complaint or no complaint.\\n\"\n",
    "pt_init_text_tokens = torch.tensor(tokenizer(prompt_tuning_init_text)[\"input_ids\"])\n",
    "pt_init_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b37a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PrefixEmbedding(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, pt_tokens, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.pt_tokens       = pt_tokens\n",
    "        no_tokens            = len(pt_tokens)\n",
    "        self.pt_embedding    = nn.Embedding(no_tokens, config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        pt = self.pt_embedding(self.pt_tokens)\n",
    "        return torch.cat((pt, x),0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2592a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration SLLMConfig(d_model=128, d_head=128, bias=False, dropout=0.0, context_window=50, n_heads=2, vocab_size=52000, n_layers=2)\n",
      "\n",
      "Total parameters: 13,698,976\n",
      "Total parameters to train after freezing: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, config = get_model()\n",
    "prefix_block = PrefixEmbedding(pt_init_text_tokens, config)\n",
    "\n",
    "# freeze all the weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters to train after freezing: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82da01df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after adding prompt tuning: 1,792\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "blocks = OrderedDict({\"token_embedding\": model.embedding_block, \"pt_embedding\": prefix_block})\n",
    "model.embedding_block = nn.Sequential(blocks)\n",
    "#nn.Sequential(model.embedding_block, prefix_block)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters after adding prompt tuning: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43545235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pt_init_text_tokens) * config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f742a42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLLM(\n",
       "  (embedding_block): Sequential(\n",
       "    (token_embedding): EmbeddingsBlock(\n",
       "      (token_embdgs): Embedding(52000, 128)\n",
       "      (pos_embdgs): Embedding(50, 128)\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (pt_embedding): PrefixEmbedding(\n",
       "      (pt_embedding): Embedding(14, 128)\n",
       "    )\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (mha): MultiHeadAttentionV1(\n",
       "        (projection_out): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (Wq): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (Wk): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (Wv): Linear(in_features=128, out_features=256, bias=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (ln_1): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=128, out_features=52000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c771259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([52000, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3087,  0.2148,  0.6442,  ..., -0.7668,  0.4021,  0.6716],\n",
      "        [-0.3794, -0.6720, -0.8701,  ..., -0.1167,  0.3699, -0.1928],\n",
      "        [-0.1089,  0.7103,  1.2057,  ..., -0.1386,  1.9621,  1.0058],\n",
      "        ...,\n",
      "        [ 1.4903,  1.5516,  0.5210,  ...,  1.2449,  1.5994,  1.8209],\n",
      "        [-0.0917,  0.5116, -1.5981,  ..., -0.7242, -1.8690,  0.0202],\n",
      "        [-0.9190,  0.5129,  1.3088,  ..., -1.1461, -0.9235, -1.7763]])\n",
      "0\n",
      "torch.Size([50, 128])\n",
      "Parameter containing:\n",
      "tensor([[-1.2415,  0.2271,  1.0032,  ..., -0.6076,  1.3255, -1.4367],\n",
      "        [ 1.5665,  1.4603,  1.2462,  ..., -1.2420,  1.5477, -0.7403],\n",
      "        [ 0.4539, -0.9131, -0.5536,  ..., -1.1211,  0.8547,  0.7545],\n",
      "        ...,\n",
      "        [ 0.1233,  0.7568,  0.8532,  ..., -0.4519,  1.8104, -1.1059],\n",
      "        [-1.0984, -0.3848,  0.4627,  ..., -1.9657, -0.5742, -1.7771],\n",
      "        [ 1.3583, -1.6562,  0.6895,  ..., -1.3460,  0.3275,  0.8947]])\n",
      "1\n",
      "torch.Size([14, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 1.7528,  0.1251, -0.1682,  ..., -0.1074,  0.6861, -0.4391],\n",
      "        [ 0.5507,  1.3164,  2.2272,  ..., -0.2855,  1.7033, -1.4604],\n",
      "        [-0.0332, -0.9328,  0.1681,  ...,  0.1810, -1.2108, -0.3354],\n",
      "        ...,\n",
      "        [ 1.1417, -0.4308, -0.3334,  ..., -0.8229, -1.2089,  0.8921],\n",
      "        [-0.4223,  0.1932,  0.1684,  ..., -0.4551,  0.2210,  0.3494],\n",
      "        [-0.9094, -0.2590, -0.3169,  ..., -0.3438,  0.3409,  1.6805]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,module in model.embedding_block.named_children():\n",
    "    for params in module.parameters():\n",
    "        print(name)\n",
    "        print(params.shape)\n",
    "        print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eabf7",
   "metadata": {},
   "source": [
    "## Prefix Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49467f3a",
   "metadata": {},
   "source": [
    "## P-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5eafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
