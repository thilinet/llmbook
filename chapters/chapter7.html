
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 7 - Rag based applications &#8212; Learn Large Language Models Fast</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/chapter7';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Chapter 6 - Fine Tuning LLMs" href="chapter6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/llm_t.png" class="logo__image only-light" alt="Learn Large Language Models Fast - Home"/>
    <script>document.write(`<img src="../_static/llm_t.png" class="logo__image only-dark" alt="Learn Large Language Models Fast - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Learn Large Language Models Fast
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">Chapter 1 - Input to LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">Chapter 2 - Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">Chapter 3 - Pre-train a tiny LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">Chapter 4 - Instruction tuning a LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">Chapter 5 - LLM applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">Chapter 6 - Fine Tuning LLMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 7 - Rag based applications</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapters/chapter7.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/chapter7.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 7 - Rag based applications</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-rag">What is RAG</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qanda-with-rag">QandA with RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-text-from-a-webpage">Extract text from a webpage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-documents-to-index">Create documents to index</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-the-documents">Index the documents</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-retrieval">Similarity based retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-a-text-generation">Q&amp;A Text Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-with-history">Chat with History</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dolly-example">Dolly Example</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-7-rag-based-applications">
<h1>Chapter 7 - Rag based applications<a class="headerlink" href="#chapter-7-rag-based-applications" title="Link to this heading">#</a></h1>
<p>Building a chatbot using RAG</p>
<section id="what-is-rag">
<h2>What is RAG<a class="headerlink" href="#what-is-rag" title="Link to this heading">#</a></h2>
</section>
<section id="qanda-with-rag">
<h2>QandA with RAG<a class="headerlink" href="#qanda-with-rag" title="Link to this heading">#</a></h2>
<section id="extract-text-from-a-webpage">
<h3>Extract text from a webpage<a class="headerlink" href="#extract-text-from-a-webpage" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.gutenberg.org/cache/epub/64317/pg64317-images.html&quot;</span> 

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">page_html</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>

<span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">page_html</span><span class="p">,</span> <span class="s2">&quot;html.parser&quot;</span><span class="p">)</span>
<span class="n">a</span>    <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;class&quot;</span> <span class="p">:</span> <span class="s2">&quot;container&quot;</span><span class="p">})</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">get_text</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="s2">&quot;*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***&quot;</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="s2">&quot;*** END OF THE PROJECT GUTENBERG EBOOK&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">(.*)</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">),</span> <span class="n">text</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-documents-to-index">
<h3>Create documents to index<a class="headerlink" href="#create-documents-to-index" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.docstore.document</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">doc</span> <span class="o">=</span>  <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">add_start_index</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">all_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">([</span><span class="n">doc</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="index-the-documents">
<h3>Index the documents<a class="headerlink" href="#index-the-documents" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_community.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="c1"># Equivalent to SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">all_splits</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/gopi/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="similarity-based-retrieval">
<h2>Similarity based retrieval<a class="headerlink" href="#similarity-based-retrieval" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="s2">&quot;similarity&quot;</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">retrieved_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>With fenders spread like wings we scattered light through half Astoria—only half, for as we twisted among the pillars of the elevated I heard the familiar “jug-jug-spat!” of a motorcycle, and a frantic policeman rode alongside.


“All right, old sport,” called Gatsby. We slowed down. Taking a white card from his wallet, he waved it before the man’s eyes.


“Right you are,” agreed the policeman, tipping his cap. “Know you next time, Mr. Gatsby. Excuse me!”


“What was that?” I inquired. “The picture of Oxford?”


“I was able to do the commissioner a favour once, and he sends me a Christmas card every year.”
</pre></div>
</div>
</div>
</div>
<section id="q-a-text-generation">
<h3>Q&amp;A Text Generation<a class="headerlink" href="#q-a-text-generation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>


<span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

<span class="c1"># HuggingFace Pipeline</span>
<span class="n">generate_text</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;databricks/dolly-v2-3b&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
                         <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">## Langchain pipeline</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">generate_text</span><span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are an assistant for question answering task.Use the following pieces of context to answer the question at the end.</span>
<span class="s2">If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.</span>
<span class="s2">Use three sentences maximum and keep the answer as concise as possible.</span>
<span class="s2">Always say &quot;thanks for asking!&quot; at the end of the answer.</span>

<span class="si">{context}</span>

<span class="s2">Question: </span><span class="si">{question}</span>

<span class="s2">Helpful Answer:&quot;&quot;&quot;</span>
<span class="n">custom_rag_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">retriever</span> <span class="o">|</span> <span class="n">format_docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">custom_rag_prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="o">%</span><span class="k">time</span> rag_chain.invoke(&quot;Where did the accident happen?&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">File</span> <span class="o">&lt;</span><span class="n">timed</span> <span class="nb">eval</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">1</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/runnables/base.py:2089,</span> in <span class="ni">RunnableSequence.invoke</span><span class="nt">(self, input, config)</span>
<span class="g g-Whitespace">   </span><span class="mi">2087</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2088</span>     <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">steps</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">2089</span>         <span class="nb">input</span> <span class="o">=</span> <span class="n">step</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2090</span>             <span class="nb">input</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2091</span>             <span class="c1"># mark each step as a child run</span>
<span class="g g-Whitespace">   </span><span class="mi">2092</span>             <span class="n">patch_config</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2093</span>                 <span class="n">config</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">run_manager</span><span class="o">.</span><span class="n">get_child</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;seq:step:</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2094</span>             <span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">2095</span>         <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2096</span> <span class="c1"># finish the root run</span>
<span class="g g-Whitespace">   </span><span class="mi">2097</span> <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:246,</span> in <span class="ni">BaseLLM.invoke</span><span class="nt">(self, input, config, stop, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">236</span> <span class="k">def</span> <span class="nf">invoke</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">237</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">238</span>     <span class="nb">input</span><span class="p">:</span> <span class="n">LanguageModelInput</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">242</span>     <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">243</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span>     <span class="n">config</span> <span class="o">=</span> <span class="n">ensure_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span>     <span class="k">return</span> <span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">246</span>         <span class="bp">self</span><span class="o">.</span><span class="n">generate_prompt</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)],</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>             <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>             <span class="n">callbacks</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;callbacks&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span>             <span class="n">tags</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tags&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">251</span>             <span class="n">metadata</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;metadata&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span>             <span class="n">run_name</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;run_name&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">253</span>             <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">255</span>         <span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">256</span>         <span class="o">.</span><span class="n">text</span>
<span class="g g-Whitespace">    </span><span class="mi">257</span>     <span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:541,</span> in <span class="ni">BaseLLM.generate_prompt</span><span class="nt">(self, prompts, stop, callbacks, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">533</span> <span class="k">def</span> <span class="nf">generate_prompt</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">534</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">535</span>     <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">PromptValue</span><span class="p">],</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">538</span>     <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">539</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMResult</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">540</span>     <span class="n">prompt_strings</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
<span class="ne">--&gt; </span><span class="mi">541</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt_strings</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:714,</span> in <span class="ni">BaseLLM.generate</span><span class="nt">(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">698</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">699</span>             <span class="s2">&quot;Asked to cache, but no cache found at `langchain.cache`.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">700</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">701</span>     <span class="n">run_managers</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">    </span><span class="mi">702</span>         <span class="n">callback_manager</span><span class="o">.</span><span class="n">on_llm_start</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span>             <span class="n">dumpd</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">712</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">713</span>     <span class="p">]</span>
<span class="ne">--&gt; </span><span class="mi">714</span>     <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_helper</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">715</span>         <span class="n">prompts</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">run_managers</span><span class="p">,</span> <span class="nb">bool</span><span class="p">(</span><span class="n">new_arg_supported</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="g g-Whitespace">    </span><span class="mi">716</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">717</span>     <span class="k">return</span> <span class="n">output</span>
<span class="g g-Whitespace">    </span><span class="mi">718</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_prompts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:578,</span> in <span class="ni">BaseLLM._generate_helper</span><span class="nt">(self, prompts, stop, run_managers, new_arg_supported, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">576</span>     <span class="k">for</span> <span class="n">run_manager</span> <span class="ow">in</span> <span class="n">run_managers</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">577</span>         <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_error</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="n">LLMResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="p">[]))</span>
<span class="ne">--&gt; </span><span class="mi">578</span>     <span class="k">raise</span> <span class="n">e</span>
<span class="g g-Whitespace">    </span><span class="mi">579</span> <span class="n">flattened_outputs</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">580</span> <span class="k">for</span> <span class="n">manager</span><span class="p">,</span> <span class="n">flattened_output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">run_managers</span><span class="p">,</span> <span class="n">flattened_outputs</span><span class="p">):</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:565,</span> in <span class="ni">BaseLLM._generate_helper</span><span class="nt">(self, prompts, stop, run_managers, new_arg_supported, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">555</span> <span class="k">def</span> <span class="nf">_generate_helper</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">556</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">557</span>     <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">561</span>     <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">562</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMResult</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">563</span>     <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">564</span>         <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">565</span>             <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">566</span>                 <span class="n">prompts</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">567</span>                 <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">568</span>                 <span class="c1"># TODO: support multiple run managers</span>
<span class="g g-Whitespace">    </span><span class="mi">569</span>                 <span class="n">run_manager</span><span class="o">=</span><span class="n">run_managers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">run_managers</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">570</span>                 <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">571</span>             <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">572</span>             <span class="k">if</span> <span class="n">new_arg_supported</span>
<span class="g g-Whitespace">    </span><span class="mi">573</span>             <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">575</span>     <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">576</span>         <span class="k">for</span> <span class="n">run_manager</span> <span class="ow">in</span> <span class="n">run_managers</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/langchain_community/llms/huggingface_pipeline.py:261,</span> in <span class="ni">HuggingFacePipeline._generate</span><span class="nt">(self, prompts, stop, run_manager, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">258</span> <span class="n">batch_prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">260</span> <span class="c1"># Process batch of prompts</span>
<span class="ne">--&gt; </span><span class="mi">261</span> <span class="n">responses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">262</span>     <span class="n">batch_prompts</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">263</span>     <span class="n">stop_sequence</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span>     <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span>     <span class="o">**</span><span class="n">pipeline_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span> <span class="c1"># Process each response in the batch</span>
<span class="g g-Whitespace">    </span><span class="mi">269</span> <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">responses</span><span class="p">):</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1177,</span> in <span class="ni">Pipeline.__call__</span><span class="nt">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1173</span> <span class="k">if</span> <span class="n">can_use_iterator</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1174</span>     <span class="n">final_iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_iterator</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1175</span>         <span class="n">inputs</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">preprocess_params</span><span class="p">,</span> <span class="n">forward_params</span><span class="p">,</span> <span class="n">postprocess_params</span>
<span class="g g-Whitespace">   </span><span class="mi">1176</span>     <span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1177</span>     <span class="n">outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">final_iterator</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1178</span>     <span class="k">return</span> <span class="n">outputs</span>
<span class="g g-Whitespace">   </span><span class="mi">1179</span> <span class="k">else</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124,</span> in <span class="ni">PipelineIterator.__next__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">121</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_batch_item</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">123</span> <span class="c1"># We&#39;re out of items within a batch</span>
<span class="ne">--&gt; </span><span class="mi">124</span> <span class="n">item</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iterator</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">125</span> <span class="n">processed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="c1"># We now have a batch of &quot;inferred things&quot;.</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125,</span> in <span class="ni">PipelineIterator.__next__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">123</span> <span class="c1"># We&#39;re out of items within a batch</span>
<span class="g g-Whitespace">    </span><span class="mi">124</span> <span class="n">item</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iterator</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">125</span> <span class="n">processed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="c1"># We now have a batch of &quot;inferred things&quot;.</span>
<span class="g g-Whitespace">    </span><span class="mi">127</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">128</span>     <span class="c1"># Try to infer the size of the batch</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1102,</span> in <span class="ni">Pipeline.forward</span><span class="nt">(self, model_inputs, **forward_params)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>     <span class="k">with</span> <span class="n">inference_context</span><span class="p">():</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>         <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>         <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">forward_params</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span> <span class="k">else</span><span class="p">:</span>

<span class="nn">File ~/.cache/huggingface/modules/transformers_modules/databricks/dolly-v2-3b/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/instruct_pipeline.py:132,</span> in <span class="ni">InstructionTextGenerationPipeline._forward</span><span class="nt">(self, model_inputs, **generate_kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span>     <span class="n">in_b</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="ne">--&gt; </span><span class="mi">132</span> <span class="n">generated_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span>     <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">134</span>     <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">135</span>     <span class="n">pad_token_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span>     <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span> <span class="n">out_b</span> <span class="o">=</span> <span class="n">generated_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115,</span> in <span class="ni">context_decorator.&lt;locals&gt;.decorate_context</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">112</span> <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">113</span> <span class="k">def</span> <span class="nf">decorate_context</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">114</span>     <span class="k">with</span> <span class="n">ctx_factory</span><span class="p">():</span>
<span class="ne">--&gt; </span><span class="mi">115</span>         <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1592,</span> in <span class="ni">GenerationMixin.generate</span><span class="nt">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1584</span>     <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1585</span>         <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1586</span>         <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1587</span>         <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1588</span>         <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1589</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1591</span>     <span class="c1"># 13. run sample</span>
<span class="ne">-&gt; </span><span class="mi">1592</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1593</span>         <span class="n">input_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1594</span>         <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1595</span>         <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1596</span>         <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1597</span>         <span class="n">pad_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1598</span>         <span class="n">eos_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1599</span>         <span class="n">output_scores</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1600</span>         <span class="n">output_logits</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1601</span>         <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1602</span>         <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1603</span>         <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1604</span>         <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1605</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1607</span> <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">BEAM_SEARCH</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1608</span>     <span class="c1"># 11. prepare beam search scorer</span>
<span class="g g-Whitespace">   </span><span class="mi">1609</span>     <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1610</span>         <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1611</span>         <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1616</span>         <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1617</span>     <span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2696,</span> in <span class="ni">GenerationMixin.sample</span><span class="nt">(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2693</span> <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2695</span> <span class="c1"># forward pass to get next token</span>
<span class="ne">-&gt; </span><span class="mi">2696</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2697</span>     <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2698</span>     <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2699</span>     <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2700</span>     <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2701</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2703</span> <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2704</span>     <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1509</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1510</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1511</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1515</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1518</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1519</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1520</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1036,</span> in <span class="ni">GPTNeoXForCausalLM.forward</span><span class="nt">(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)</span>
<span class="g g-Whitespace">    </span><span class="mi">995</span><span class="w"> </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">996</span><span class="sd"> past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="g g-Whitespace">    </span><span class="mi">997</span><span class="sd">     Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">   </span><span class="mi">1032</span><span class="sd"> &gt;&gt;&gt; prediction_logits = outputs.logits</span>
<span class="g g-Whitespace">   </span><span class="mi">1033</span><span class="sd"> ```&quot;&quot;&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1034</span> <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
<span class="ne">-&gt; </span><span class="mi">1036</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpt_neox</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1037</span>     <span class="n">input_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1038</span>     <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1039</span>     <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1040</span>     <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1041</span>     <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1042</span>     <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1043</span>     <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1044</span>     <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1045</span>     <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1046</span>     <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1047</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span> <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1509</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1510</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1511</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1515</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1518</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1519</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1520</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:927,</span> in <span class="ni">GPTNeoXModel.forward</span><span class="nt">(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)</span>
<span class="g g-Whitespace">    </span><span class="mi">916</span>     <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">917</span>         <span class="n">layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">918</span>         <span class="n">hidden_states</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">924</span>         <span class="n">output_attentions</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">925</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">926</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">927</span>     <span class="n">outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">928</span>         <span class="n">hidden_states</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">929</span>         <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">930</span>         <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">931</span>         <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
<span class="g g-Whitespace">    </span><span class="mi">932</span>         <span class="n">layer_past</span><span class="o">=</span><span class="n">layer_past</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">933</span>         <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">934</span>         <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">935</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">936</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">937</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1509</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1510</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1511</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1515</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1518</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1519</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1520</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:708,</span> in <span class="ni">GPTNeoXLayer.forward</span><span class="nt">(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">attention_layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="g g-Whitespace">    </span><span class="mi">705</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_parallel_residual</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">706</span>     <span class="c1"># pseudocode:</span>
<span class="g g-Whitespace">    </span><span class="mi">707</span>     <span class="c1"># x = x + attn(ln1(x)) + mlp(ln2(x))</span>
<span class="ne">--&gt; </span><span class="mi">708</span>     <span class="n">mlp_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">709</span>     <span class="n">mlp_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_mlp_dropout</span><span class="p">(</span><span class="n">mlp_output</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">710</span>     <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mlp_output</span> <span class="o">+</span> <span class="n">attn_output</span> <span class="o">+</span> <span class="n">hidden_states</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1509</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1510</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1511</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1515</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1518</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1519</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1520</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:661,</span> in <span class="ni">GPTNeoXMLP.forward</span><span class="nt">(self, hidden_states)</span>
<span class="g g-Whitespace">    </span><span class="mi">659</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_h_to_4h</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">660</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">661</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_4h_to_h</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">662</span> <span class="k">return</span> <span class="n">hidden_states</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1509</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1510</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1511</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1515</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1518</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1519</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1520</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:116,</span> in <span class="ni">Linear.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">115</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">116</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="chat-with-history">
<h2>Chat with History<a class="headerlink" href="#chat-with-history" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">MessagesPlaceholder</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>


<span class="n">contextualize_q_system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Given a chat history and the latest user question </span><span class="se">\</span>
<span class="s2">which might reference context in the chat history, formulate a standalone question </span><span class="se">\</span>
<span class="s2">which can be understood without the chat history. Do NOT answer the question, </span><span class="se">\</span>
<span class="s2">just reformulate it if needed and otherwise return it as is.&quot;&quot;&quot;</span>
<span class="n">contextualize_q_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">contextualize_q_system_prompt</span><span class="p">),</span>
        <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;chat_history&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">contextualize_q_chain</span> <span class="o">=</span> <span class="n">contextualize_q_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="n">qa_system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are an assistant for question-answering tasks. </span><span class="se">\</span>
<span class="s2">Use the following pieces of retrieved context to answer the question. </span><span class="se">\</span>
<span class="s2">If you don&#39;t know the answer, just say that you don&#39;t know. </span><span class="se">\</span>
<span class="s2">Use three sentences maximum and keep the answer concise.</span><span class="se">\</span>

<span class="si">{context}</span><span class="s2">&quot;&quot;&quot;</span>
<span class="n">qa_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">qa_system_prompt</span><span class="p">),</span>
        <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;chat_history&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">contextualized_question</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;chat_history&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">contextualize_q_chain</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>


<span class="n">rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="n">context</span><span class="o">=</span><span class="n">contextualized_question</span> <span class="o">|</span> <span class="n">retriever</span> <span class="o">|</span> <span class="n">format_docs</span>
    <span class="p">)</span>
    <span class="o">|</span> <span class="n">qa_prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chat_history</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;Who is Daisy?&quot;</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;chat_history&quot;</span><span class="p">:</span> <span class="n">chat_history</span><span class="p">})</span>
<span class="n">chat_history</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">question</span><span class="p">),</span> <span class="n">ai_msg</span><span class="p">])</span>

<span class="n">second_question</span> <span class="o">=</span> <span class="s2">&quot;How did she die?&quot;</span>
<span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">second_question</span><span class="p">,</span> <span class="s2">&quot;chat_history&quot;</span><span class="p">:</span> <span class="n">chat_history</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Tom Buchanan was in love with Daisy for several years before he knew that she was involved in some criminal activity. At first, he suspected nothing, but one day he saw Daisy leave her home with another woman and he discovered that Daisy had a driver’s license. When Tom confronted Daisy about it, she attempted to run away and tore open the back of her neck, killing her instantly.&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="nn">gr</span>

<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">message</span>
    <span class="n">ai_msg</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;chat_history&quot;</span><span class="p">:</span> <span class="n">chat_history</span><span class="p">})</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">question</span><span class="p">),</span> <span class="n">ai_msg</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ai_msg</span>


<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">echo</span><span class="p">,</span> <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hello&quot;</span><span class="p">,</span> <span class="s2">&quot;hola&quot;</span><span class="p">,</span> <span class="s2">&quot;merhaba&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;The Great Gatsby Bot&quot;</span><span class="p">)</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running on local URL:  http://127.0.0.1:7861

To create a public link, set `share=True` in `launch()`.
</pre></div>
</div>
<div class="output text_html"><div><iframe src="http://127.0.0.1:7861/" width="100%" height="500" allow="autoplay; camera; microphone; clipboard-read; clipboard-write;" frameborder="0" allowfullscreen></iframe></div></div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dolly-example">
<h2>Dolly Example<a class="headerlink" href="#dolly-example" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generate_text</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;databricks/dolly-v2-3b&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
                         <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="c1"># template for an instrution with no input</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{instruction}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># template for an instruction with input</span>
<span class="n">prompt_with_context</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{instruction}</span><span class="se">\n\n</span><span class="s2">Input:</span><span class="se">\n</span><span class="si">{context}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">hf_pipeline</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">generate_text</span><span class="p">)</span>

<span class="n">llm_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">hf_pipeline</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">llm_context_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">hf_pipeline</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_with_context</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">llm_chain</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="s2">&quot;Explain to me the difference between nuclear fission and fusion.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lstrip</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fission breaks down an atom into smaller atoms, while fusion together the smaller atomic nuclei to form one larger atomic nucleus.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,</span>
<span class="s2">and Founding Father who served as the first president of the United States from 1789 to 1797.&quot;&quot;&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">llm_context_chain</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="s2">&quot;When was George Washington president?&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span><span class="o">.</span><span class="n">lstrip</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>George Washington was president from 1789 to 1797.
</pre></div>
</div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Improve the quality of the embeddings: You&#39;re currently using the sentence-transformers/all-mpnet-base-v2 model for embeddings. While this is a good general-purpose model, you might get better results with a model that&#39;s more specialized for your specific task. For example, you could try using a model that&#39;s been fine-tuned on a dataset of similar documents to your operation manual.

Optimize the search: You&#39;re currently using the FAISS vector store for similarity search. While FAISS is a good choice for large-scale similarity search, it might not be the most efficient choice for your specific task. You could try using a different vector store, such as Pinecone or Weaviate, to see if they provide better performance.

Optimize the text splitting: You&#39;re currently splitting the text into chunks of 1000 characters with an overlap of 200 characters. This might be too fine-grained, resulting in a lot of redundant computations. You could try increasing the chunk size and reducing the overlap to improve efficiency.

Optimize the prompt: You&#39;re currently using a generic prompt template. You might get better results by customizing the prompt to better match the style and content of your operation manual.
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 6 - Fine Tuning LLMs</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-rag">What is RAG</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qanda-with-rag">QandA with RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-text-from-a-webpage">Extract text from a webpage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-documents-to-index">Create documents to index</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-the-documents">Index the documents</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-retrieval">Similarity based retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-a-text-generation">Q&amp;A Text Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-with-history">Chat with History</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dolly-example">Dolly Example</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gopi Subramanian
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>