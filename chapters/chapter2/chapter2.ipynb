{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65a1db7-efa1-4060-aa44-04704dd5bbe9",
   "metadata": {},
   "source": [
    "# Chapter 2 - Transformer Architecture\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "At the heart of any large language model lies the transformers. Proposed by in the seminal paper Attention is all you need by {cite:p}`vaswani2023attentionneed`, it has become the backbone for modern LLM. Original Transformer articulated in the paper is a neural network, consisting of the following modules\n",
    "\n",
    "1. Encoder Module\n",
    "    This is made of several self attention module heads followed by a fully conntected layers.\n",
    "    \n",
    "2. Decoder Module\n",
    "    Similar to an encoder module with attention heads followed by fully connected layers. In addtion to user input, this layer also takes in the embedding output from encoding module.\n",
    "    \n",
    " \n",
    " ```{figure} ../../images/chapter2/Transformer-full.png\n",
    "---\n",
    "height: 150px\n",
    "name: Transformer Architecture\n",
    "---\n",
    "Transformer Architecture\n",
    "```\n",
    "Transormer architecture is a deep learning neural network trained for language translation tasks. The Attention is all you need paper gives an example of training from english to german and french. You can think of it as two neural networks, an encoder and a decoder connected in a cascading manner to perform the translation task.\n",
    "\n",
    "The encoder converts the whole input english text into an embedded vector represenation. The decoder uses this embedded vector and translates one word a time. There are three types of transformers which evolved quickly after this paper was published.\n",
    "\n",
    "1. Encoder - Decoder transformers\n",
    "    They include both encoders and decoders. Translation models uses these architecture.  T5, BART etc. Their pre-training is task dependent. Tasks that involve both understanding and generating data. They first encode an input sequence into an internal represenation and then decode this represenation into an output sequence.\n",
    "\n",
    "2. Encoder only Transformers  \n",
    "    Models like BERT are encoder only transfomer.Their task involves only understanding. They are trained to do masked word prediction. Given a sentence, one of the word will be masked. Encoder only models are heavily used in classification tasks.\n",
    "    \n",
    "3. Decoder only Transformers\n",
    "    In this book we will be looking at decoder only models. Decoder only models are trained to perform text generation. These models are also called as autoregressive models.\n",
    "    \n",
    "\n",
    "The secret sauce in encoder / decoder architecture is the multi-head attention module. It is this attention mechanism module which claims to provide better contextual information and long term dependency features present in the input text data to the model. Let us lookat the following two examples\n",
    "\n",
    "1. The river bank is not accessible to the tourist.\n",
    "2. The robbers had planned a heist of major banks in the city.\n",
    "\n",
    "\n",
    "Both the example sentences have the word \"bank\". The context in the first sentence is \"river\" and the second sentence is \"heist\".  We want the model to treat the word \"bank\" with respect to their context in the sentence. Another example,\n",
    "\n",
    "1. I went to the library yesterday, there i forgot my book. I am returning there today.\n",
    "\n",
    "As an English reader, if I ask you a question \"Were am I returning?\", you know that I am returning to the library. The previous sentence has the clue. This is a trivial example for long term dependency.\n",
    "\n",
    "\n",
    "This is where attention mechanism comes to rescue. Using attention mechanism we inject contextual information and long term dependency in to the model. We saw word embeddings in the previous chapter. Each word or a token had a unique position in the embedding space. However as we saw in the previous examples, these embeddings need to account for the contextual information in the input text. Attention mechanism adds these contextual information to the vector representaion of the token. More about attention in the subsequent sections.\n",
    "\n",
    "\n",
    "\n",
    "ALl the examples in this book are decoder only architecture. It is the widely used architecture for building causal LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c2b8e-8c24-4800-90bb-4904b0e7796c",
   "metadata": {},
   "source": [
    "## Why LLMs are decoder Transformers\n",
    "\n",
    "Why most of the LLMs are decoder only models and not encoder-decoder model?\n",
    "\n",
    "According to {cite}'wang2022language' Decoder only models trained on next word prediction objective exibit the strongest zero-shot generalization after purely self-supervised pretraining.\n",
    "\n",
    "Models trained with masked language modeling objective, followed by mutitask finetuning perform the best among our experiments.\n",
    "\n",
    "* Factors to consider while choosing between Decoder only or Encoder Decoder models.\n",
    "\n",
    "  1 cost of training\n",
    "      Decoder only models are cheaper to train. They can be trained on large unsupervised corpus\n",
    "      Encoder Decoder needs a lot of labelled data for  multitask finetuning.\n",
    "  2 Emerging Abilities\n",
    "      phenomenon where models display new, sophisticated capabilities not explicitly taught during training, arising naturally as the model scales in size\n",
    "  and complexity.https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\n",
    "\n",
    "  https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f\n",
    "\n",
    "  emergent abilities reduce the performance gap achieved by encoder decoder models over decoder-only ones with multitask finetuning.\n",
    "\n",
    "  3. In-context learning from prompts\n",
    " \n",
    "      prompt engineering methods provide few-shot examples to help LLM understand the context or task. In context information can be seen to have a similar effect as gradient descent that updates the attention weight of the zero-shot prompt.\n",
    "\n",
    "  4. Efficiency Optimization\n",
    " \n",
    "     In decoder only models, the key and value matrices from previous tokens can be reused for subsequent tokens during the decoding process. Since each position only attends to previous tokens the K and V matrices for those tokens remain unchaged. This caching mechanism improves effieciency by avoiding the recomputation of K and V matrices for tokens that have already been processed, facilitates faster generation and lower computation cost during inference.\n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d21f9e",
   "metadata": {},
   "source": [
    "## Scaled Self-Attention\n",
    "\n",
    "Sometime while performing average of numbers, we assign weights to different numbers; this is weighted averaging. Varied attention is given to different numbers.In the example below we show a weighted average example by restricting the contribution of the three entities, to ten, thirty and sixty percentages. Let us take some liberty here for the sake for quickly grasping the concept of self-attention. Instead of calling these as weights, lets call them as attention. Rewriting the previous sentence, we have restricted the attention the average function can give to these three entities as 10, 30 and 60 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ff4a646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Normal 103.333 Weighted 35.00\n"
     ]
    }
   ],
   "source": [
    "weights  = (0.1, 0.3, 0.6)\n",
    "entities = (90.,120.,100.)\n",
    "\n",
    "normal_average = sum(entities) / len(entities)\n",
    "\n",
    "weighted_sum   = (x*y for x,y in zip(weights, entities))\n",
    "weighted_average = sum(weighted_sum) / len(entities)\n",
    "\n",
    " \n",
    "print(f\"Average Normal {normal_average:.3f} \\\n",
    "    Weighted {weighted_average:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6765b7",
   "metadata": {},
   "source": [
    "So, what is self attention. Say we have six tokens in our input, \"The boat left the river bank.\" \n",
    "For each token we have a vector represenation in the form of embedding. If we take a dot product of the vectors for \"The\" and \"boat\", it gives us the strength of their relationship. Refer to previous chapter for vector represenation of words. So say we have an initial embedding for word \"The\". Now we do a dot product of word \"the\" and \"boat\", followed by dot product of \"The\" and \"left\" and so on. Let us call them attention weights, the strength of relationship between those words. \n",
    "\n",
    "When we start training a transformer for a language task, we choose to initialize these word embeddings in a random fashion leaving us with no control over the scale of the embedding values. Attention paper proposes to scale these values using square root of the embedding dimension. Hence we call this attention as scaled attention. After scaling we normalize normalize these weights to fall between zero and one. Normalizing the values help in stabilized training of LLM. \n",
    "\n",
    "Finally we will enrich the embedding representation of each word with these scaled normalized attention weights. We take the embedding for the word \"The\", we multiply it with attention weight for \"The\". The embedding for \"The\" is now enriched with its relationship with itself. To this add the next enriched vector, the embedding vector for \"The\" multipled by attention weight for the word \"boat\" and so on. The final vector called context vector for the word \"The\" is thus formed by addition. Through this weighted sum, the context vector for each word is now enriched by their strength of relationship with their neighbors. The input is enriched by its constituents and hence the name self-attention.\n",
    "\n",
    "To sum up scaled self-attention, the input is enriched by its constituents and hence self, the contribution of the words in the input to each other is the attention and finally these attention weights are scaled by square root of embedding dimension and hence scaled is used in naming this.\n",
    "\n",
    "\n",
    "```{note}\n",
    "dot product explanation comes here.\n",
    "\n",
    "```\n",
    "\n",
    "To demonstrate this concept, let us create some dummy word embeddings. For the purpose of illustration we we will stick with word tokens and not their integer encoding. In the previous chapter we studied about integer encoding followed by token embedding. For bruety purpose let us skip directly to token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6236a71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40340279,  0.03305275,  0.99138182, -0.57605323],\n",
       "       [ 0.25784043,  0.02112612,  0.63365529, -0.36819233],\n",
       "       [ 0.0519119 ,  0.00425339,  0.12757601, -0.07412943],\n",
       "       [ 0.09087871,  0.00744613,  0.22333881, -0.12977345],\n",
       "       [ 0.10743083,  0.00880233,  0.26401646, -0.15340965],\n",
       "       [ 0.05611193,  0.00459752,  0.13789777, -0.08012701]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_tokens = [\"The\", \"boat\", \"left\", \"the\", \"river\", \"bank\"]\n",
    "embd_dim =4\n",
    "\n",
    "generate_embdgs = np.random.normal(size=(embd_dim,))\n",
    "token_embedding = np.array([generate_embdgs * np.random.rand() \\\n",
    "                            for i in range(len(input_tokens))])\n",
    "token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014411d",
   "metadata": {},
   "source": [
    "With these embeddings, now let us calcuate the reltionship strength of firt word \"The\" with all the words in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "88aa3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strength betwen 'The'         and 'The' 1.48\n",
      "Strength betwen 'The'         and 'boat' 0.95\n",
      "Strength betwen 'The'         and 'left' 0.19\n",
      "Strength betwen 'The'         and 'the' 0.33\n",
      "Strength betwen 'The'         and 'river' 0.39\n",
      "Strength betwen 'The'         and 'bank' 0.21\n"
     ]
    }
   ],
   "source": [
    "first_word_initial_vector = token_embedding[0]\n",
    "\n",
    "attention_scores_first_word = np.zeros(len(input_tokens),)\n",
    "\n",
    "for idx, each_word_vector in enumerate(token_embedding):\n",
    "    attention_scores_first_word[idx] = \\\n",
    "            np.dot(first_word_initial_vector, each_word_vector)\n",
    "\n",
    "for i in range(len(attention_scores_first_word)):\n",
    "    print(f\"Strength betwen '{input_tokens[0]}' \\\n",
    "        and '{input_tokens[i]}' {attention_scores_first_word[i]:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a462138",
   "metadata": {},
   "source": [
    "The stength score array now has the relationship strengths. Using we can now created a weighted sum for the word \"The\". Let us normalize these scores using softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f5c2f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights [0.2521732  0.19313079 0.13242228 0.14222411 0.1466042  0.13344543]         sum of weights 1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "attention_scores_first_word = attention_scores_first_word * (1/np.sqrt(embd_dim))\n",
    "attention_weights_first_word = \\\n",
    "    softmax(attention_scores_first_word)\n",
    "\n",
    "print(f\"Attention weights {attention_weights_first_word} \\\n",
    "        sum of weights {sum(attention_weights_first_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93c2cb",
   "metadata": {},
   "source": [
    "Normalized attention weights sums up to 1.0. Softmax is useful in dealing with extreme values. Let us try to see the difference in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f6d36e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values [1.39336480e-05 4.77725073e-05 9.95260569e-01 0.00000000e+00\n",
      " 4.67772468e-03]\n",
      "Softmax Normalization [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray([0.14,0.48,10000,0,47])\n",
    "x_normalized = x / sum(x)\n",
    "print(f\"Normalized values {x_normalized}\")\n",
    "print(f\"Softmax Normalization {softmax(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d7b91",
   "metadata": {},
   "source": [
    "With softmax we get more numerical stability, so we can avoid underflow and overflow problems during gradient calcualtions and backprobagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "08e4e8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19456143,  0.01594136,  0.47814409, -0.27783085])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_context_vector = np.zeros(first_word_initial_vector.shape)\n",
    "\n",
    "for idx, score in enumerate(attention_weights_first_word):\n",
    "    first_word_context_vector += token_embedding[idx] * score\n",
    "\n",
    "first_word_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564da839",
   "metadata": {},
   "source": [
    "### Vectorization for all the words\n",
    "\n",
    "Repeate the same process for each word in the squence. Using matrix operation we can vectorize attention calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c4ed6e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73925077, 0.47250227, 0.09513051, 0.16653865, 0.19687104,\n",
       "        0.10282722],\n",
       "       [0.47250227, 0.30200631, 0.06080397, 0.10644546, 0.12583283,\n",
       "        0.06572343],\n",
       "       [0.09513051, 0.06080397, 0.01224187, 0.02143103, 0.02533436,\n",
       "        0.01323232],\n",
       "       [0.16653865, 0.10644546, 0.02143103, 0.03751788, 0.04435117,\n",
       "        0.02316495],\n",
       "       [0.19687104, 0.12583283, 0.02533436, 0.04435117, 0.05242904,\n",
       "        0.02738408],\n",
       "       [0.10282722, 0.06572343, 0.01323232, 0.02316495, 0.02738408,\n",
       "        0.01430291]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = np.dot(token_embedding, token_embedding.T)\n",
    "attention_scores = attention_scores * (1/math.sqrt(embd_dim))\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1fefa553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73925077, 0.47250227, 0.09513051, 0.16653865, 0.19687104,\n",
       "       0.10282722])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_first_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e542ed",
   "metadata": {},
   "source": [
    "Compare the first row with our prevoius strength_score array.The first row here represents the strength score between word \"The\" and all the other words in the sequence. The second row represents the relationship between word \"boat\" and all hte other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ef806235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2521732 , 0.19313079, 0.13242228, 0.14222411, 0.1466042 ,\n",
       "        0.13344543],\n",
       "       [0.21871958, 0.18443452, 0.144907  , 0.15167402, 0.15464327,\n",
       "        0.14562162],\n",
       "       [0.17637897, 0.17042723, 0.16234867, 0.16384739, 0.16448819,\n",
       "        0.16250955],\n",
       "       [0.18392583, 0.17319868, 0.15908282, 0.16166266, 0.16277113,\n",
       "        0.15935889],\n",
       "       [0.1871979 , 0.17436104, 0.15768977, 0.16071723, 0.16202074,\n",
       "        0.15801333],\n",
       "       [0.17718185, 0.17072819, 0.16199763, 0.16361471, 0.16430648,\n",
       "        0.16217115]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = softmax(attention_scores,axis=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff5f6f",
   "metadata": {},
   "source": [
    "We have all the attention weights. You can compare the attention weights for the first word \"The\" with the first row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2b4be68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2521732 , 0.19313079, 0.13242228, 0.14222411, 0.1466042 ,\n",
       "       0.13344543])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_first_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bc983d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = np.dot(attention_weights, token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da9908",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Another important concept to grasp before we move further is Masking. Let us see what is masking and why need masking. We perform masking to avoid data leakage. While we train the transformer to perform the next word prediction, we should avoid giving any information about the next word it is supposed to predict.In the previous example, while doing the weighted sum addition, we enriched the vector representation for word \"The\" w the ith all the other words in the sequence. While we expect the transformer to predict, \"Boat\" after \"The\", we have introduced leakage by providing information about \"Boat\" to the model while training.\n",
    "\n",
    "Remember the context vector creation. We enrich the original embedding vector with attention scores. We dont want to enrich word \"The\" with the scores of \"boat\" before the model sees the word \"boat\". In the next step, when the model sees the word \"The\" and \"boat\", we will enrich it with the scores from \"boat\". We can do this by using a mask, where we will mask the upper triangle of our attenion score matrix.\n",
    "\n",
    "Let us create a mask matrix of the same shape as our attention_weights matrix. Set the lower triangle part of this matrix as 1 and the upper triangle as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6b1a6990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False],\n",
       "       [ True,  True, False, False, False, False],\n",
       "       [ True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mask = np.zeros_like(attention_scores, dtype=bool)\n",
    "mask[np.tril_indices_from(attention_scores)] = True\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f9377",
   "metadata": {},
   "source": [
    "Apply this mask over attention_scores matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "323d54a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73925077, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.47250227, 0.30200631, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09513051, 0.06080397, 0.01224187, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.16653865, 0.10644546, 0.02143103, 0.03751788, 0.        ,\n",
       "        0.        ],\n",
       "       [0.19687104, 0.12583283, 0.02533436, 0.04435117, 0.05242904,\n",
       "        0.        ],\n",
       "       [0.10282722, 0.06572343, 0.01323232, 0.02316495, 0.02738408,\n",
       "        0.01430291]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_masked = attention_scores * mask\n",
    "attention_scores_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4029653",
   "metadata": {},
   "source": [
    "We should have a masked attention scores matrix, where all the upper triangular entries are set to zero. Let us set those zero values to minus infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "52dc1d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73925077,       -inf,       -inf,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [0.47250227, 0.30200631,       -inf,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [0.09513051, 0.06080397, 0.01224187,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [0.16653865, 0.10644546, 0.02143103, 0.03751788,       -inf,\n",
       "              -inf],\n",
       "       [0.19687104, 0.12583283, 0.02533436, 0.04435117, 0.05242904,\n",
       "              -inf],\n",
       "       [0.10282722, 0.06572343, 0.01323232, 0.02316495, 0.02738408,\n",
       "        0.01430291]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_masked[attention_scores_masked == 0]= -np.inf\n",
    "attention_scores_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd2b92",
   "metadata": {},
   "source": [
    "Softmax function, will handle the negative infinity value and set it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f2d191fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.54252104, 0.45747896, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.34641517, 0.33472572, 0.31885911, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.27132906, 0.25550428, 0.23468043, 0.23848623, 0.        ,\n",
       "        0.        ],\n",
       "       [0.22232881, 0.2070829 , 0.18728298, 0.19087859, 0.19242672,\n",
       "        0.        ],\n",
       "       [0.17718185, 0.17072819, 0.16199763, 0.16361471, 0.16430648,\n",
       "        0.16217115]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_masked = softmax(attention_scores_masked,axis=-1)\n",
    "attention_weights_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244627",
   "metadata": {},
   "source": [
    "Finally using this masked attention weights, we calculate the context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "03d24ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40340279,  0.03305275,  0.99138182, -0.57605323],\n",
       "       [ 0.33681107,  0.02759656,  0.82772946, -0.48096124],\n",
       "       [ 0.24260326,  0.01987766,  0.5962092 , -0.34643387],\n",
       "       [ 0.20919026,  0.01713997,  0.51409516, -0.29872061],\n",
       "       [ 0.19082399,  0.01563513,  0.46895915, -0.27249384],\n",
       "       [ 0.1655263 ,  0.01356237,  0.40678886, -0.23636911]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = np.dot(attention_weights_masked, token_embedding)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59deb20",
   "metadata": {},
   "source": [
    "The context vector for the word \"The\" is not enriched by other words in the input. The last word has the enrichment of all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0d145-f78b-4e30-94a4-c47793aa7662",
   "metadata": {},
   "source": [
    "### Scaled dot product self-attention in Pytorch\n",
    "\n",
    "\n",
    "The below code implements the attention mechanism in Pytorch. SLLMConfig class is a datastructure to store the model parameters.\n",
    "\n",
    "We implement the attention mechanism in the class SingleHeadAttention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a34105",
   "metadata": {},
   "source": [
    "Here is the torch implementation \"torch.nn.functional.scaled_dot_product_attention\" of scaled dot product attention. Going forward we will be using this version\n",
    "inside our source code.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "\n",
    "<Need to rewrite it>\n",
    "\n",
    "The documentation warns that its a beta version and subject to change. In addition to standard implementation similar to our example, this also implements three other versions,\n",
    "\n",
    "1. Flash attention {cite}`dao2023flashattention2fasterattentionbetter`\n",
    "\n",
    "The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation.\n",
    "\n",
    "The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation.\n",
    "\n",
    "2. Memory effiecient\n",
    "\n",
    "3. C++ implementation\n",
    "\n",
    "Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:\n",
    "\n",
    "        torch.nn.attention.sdpa_kernel(): A context manager used to enable or disable any of the implementations.\n",
    "\n",
    "        torch.backends.cuda.enable_flash_sdp(): Globally enables or disables FlashAttention.\n",
    "\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(): Globally enables or disables Memory-Efficient Attention.\n",
    "\n",
    "        torch.backends.cuda.enable_math_sdp(): Globally enables or disables the PyTorch C++ implementation.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e14251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4034,  0.0331,  0.9914, -0.5761],\n",
       "        [ 0.3368,  0.0276,  0.8277, -0.4810],\n",
       "        [ 0.2426,  0.0199,  0.5962, -0.3464],\n",
       "        [ 0.2092,  0.0171,  0.5141, -0.2987],\n",
       "        [ 0.1908,  0.0156,  0.4690, -0.2725],\n",
       "        [ 0.1655,  0.0136,  0.4068, -0.2364]], dtype=torch.float64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "torch_tensor = torch.tensor(token_embedding)\n",
    "\n",
    "attn_torch = scaled_dot_product_attention(\n",
    "        query = torch_tensor\n",
    "       ,key   = torch_tensor\n",
    "       ,value = torch_tensor\n",
    "       ,attn_mask=None, dropout_p=0.0, is_causal=True, scale=None\n",
    ")\n",
    "\n",
    "print(attn_torch.shape)\n",
    "attn_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "96bdc224-0c19-4854-8dad-c06dec51fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "torch.manual_seed(4752)\n",
    "\n",
    "@dataclass\n",
    "class SLLMConfig:\n",
    "    # Embedding dimension\n",
    "    d_model: int = 512\n",
    "    # Query key Value projection dimension\n",
    "    d_head: int  = 128\n",
    "    # bias for query,key and value projection matrices\n",
    "    bias: bool = False\n",
    "    dropout: int = 0.0\n",
    "    # Number of input tokens\n",
    "    context_window: int = 32\n",
    "    # Number of attention heads\n",
    "    n_heads: int = 4\n",
    "\n",
    "    \n",
    "config = SLLMConfig()\n",
    "assert config.d_model % config.n_heads == 0\n",
    "\n",
    "    \n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements weighted self attention\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "\n",
    "        self.attn_drop = config.dropout\n",
    "        \n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        \n",
    "        context_vector = scaled_dot_product_attention(\n",
    "                                query = q\n",
    "                               ,key   = k\n",
    "                               ,value = v\n",
    "                               ,attn_mask=None\n",
    "                               ,dropout_p=self.attn_drop\n",
    "                               ,is_causal=True, scale=None)\n",
    "\n",
    "\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787d3ac",
   "metadata": {},
   "source": [
    "SingleHeadAttention class should look straightforward if you were following us till now. There are three trainable weights: Wq, Wk and Wv linear layers. Inside the forward function we see that these matrices are used to project the input token embeddings to three variables query, key and value. Let us spend some time understanding the importance of these trainable weights.\n",
    "\n",
    "Query, Key and Value terminology comes from recommonder system literature and subsequently RNN based transalation systems. Query refers to \"what are we looking for\" and Key symbolizes \"what is available\". We calculate the attention score between Query and Key. The context vector for tokens in the query is enriched by its relationship with tokens in the key.\n",
    "\n",
    "There is another way to look at this Query, Key and Value. Imaginge you are working on a curve fitting problem. A graph with lot of curves, a quadratic equation will be more suitable than a linear equation. The difference is in the number of coeffiecients. The linear equation has a single coeffiecient, the slope. However quadratic equation will have multiple coeffiecients and will be able to do better in fitting the curve.\n",
    "\n",
    "Take this analogy. We have now three different coeffiecients for our input imbeddings to extract and learn more features for our text input. During the training of our LLM, we have given the model more options, it can do gradient updates during back probagation to the following\n",
    "\n",
    "1. Embedding Layer\n",
    "2. Query weight matrix\n",
    "3. Key weight matrix\n",
    "4. Value weight matrix.\n",
    "\n",
    "\n",
    "Compare this if we dont have query, key and value and we use only the token embedding layer to tweak as we build our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "51c0ba31-b97c-480a-8764-7089cf20af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7320, -1.3121, -1.1089,  ..., -0.3185, -0.0318, -0.0945],\n",
       "        [-0.4218, -0.7940, -0.9174,  ...,  0.2992,  0.3103, -0.0832],\n",
       "        [-0.5216, -0.7165, -0.9867,  ...,  0.1792,  0.2390,  0.0536],\n",
       "        ...,\n",
       "        [-0.8741, -0.9610, -1.1674,  ...,  0.0701,  0.4750,  0.0293],\n",
       "        [-0.8710, -0.9496, -1.1936,  ...,  0.0768,  0.4816,  0.0163],\n",
       "        [-0.8807, -0.9522, -1.1772,  ...,  0.0664,  0.4743,  0.0258]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.normal(0.5,0.3,size=(config.context_window, config.d_model))\n",
    "\n",
    "sha = SingleHeadAttention(config)\n",
    "attn = sha.forward(x)\n",
    "\n",
    "print(attn.shape)\n",
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639f769",
   "metadata": {},
   "source": [
    "We have the context vectors for our input token embeddings. In transfomer architecture, we can have multiple attention module. Hence the name, single head. One head provides a context vectors. Mutliple heads can provide multiple context vectors and finally we concatenate these context vectors. The concatenated context vectors serve as input to the next layer.\n",
    "\n",
    "Below is an example of multi head implementation. \"n_heads\" in config defines the number of heads needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "e4e55bd9-23e7-4310-ba5d-eb97325c1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config) for _ in range(config.n_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attentions = []\n",
    "        for head in self.heads:\n",
    "            c_vector = head(x)\n",
    "            attentions.append(c_vector)\n",
    "\n",
    "        context_vector = torch.cat(attentions, dim=-1)\n",
    "        context_projected = self.projection_out(context_vector)\n",
    "        return context_projected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373a922",
   "metadata": {},
   "source": [
    "self.heads stores the list of singlehaead attention, number decided by n_heads. This is followed by a projection\n",
    "layer.All the concatenated context vectors are transformed through this projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "afe0c902-6bfd-4e84-a345-95d98d7c63eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1402,  0.0391,  0.3527,  ...,  0.0339, -0.1148, -0.3316],\n",
       "         [-0.2345, -0.4306,  0.1521,  ..., -0.1201, -0.0996, -0.2717],\n",
       "         [ 0.0304, -0.4214,  0.2029,  ..., -0.0579, -0.3120, -0.1975],\n",
       "         ...,\n",
       "         [-0.1216, -0.3776,  0.0353,  ..., -0.1119, -0.3448, -0.5420],\n",
       "         [-0.1336, -0.3805,  0.0261,  ..., -0.1079, -0.3413, -0.5553],\n",
       "         [-0.1342, -0.3915,  0.0220,  ..., -0.0991, -0.3558, -0.5513]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(config)\n",
    "x = torch.normal(0.5,0.3,size=(1, config.context_window, config.d_model))\n",
    "projected_output = mha.forward(x)\n",
    "projected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e23317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)    \n",
    "        \n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "\n",
    "        self.attn_drop  = config.dropout\n",
    "        self.n_heads    = config.n_heads\n",
    "        self.d_head     = config.d_head\n",
    "        self.__init_weights()\n",
    "\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, length, d = x.shape\n",
    "        is_causal = True\n",
    "        \n",
    "        if not self.train:\n",
    "            is_causal = False\n",
    "            self.attn_drop = 0.0\n",
    "        \n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        \n",
    "        q = q.view(batch, length, self.n_heads, self.d_head)\n",
    "        k = k.view(batch, length, self.n_heads, self.d_head)\n",
    "        v = v.view(batch, length, self.n_heads, self.d_head)\n",
    "\n",
    "        context_vector = scaled_dot_product_attention(\n",
    "                                query = q\n",
    "                               ,key   = k\n",
    "                               ,value = v\n",
    "                               ,attn_mask=None\n",
    "                               ,dropout_p=self.attn_drop\n",
    "                               ,is_causal=True, scale=None)\n",
    "\n",
    "        context_vector = context_vector.contiguous().view(batch, length, self.d_head * self.n_heads)\n",
    "        output = self.projection_out(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8c936443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0800, -0.3302, -0.0600,  ..., -0.2621,  0.0807, -0.0346],\n",
      "         [ 0.0257, -0.4768, -0.1681,  ..., -0.0309,  0.0832,  0.1942],\n",
      "         [ 0.0488, -0.0642, -0.0550,  ...,  0.0505,  0.1325, -0.0416],\n",
      "         ...,\n",
      "         [-0.1246, -0.1650, -0.1457,  ...,  0.0048,  0.2119,  0.1326],\n",
      "         [-0.1885, -0.2958,  0.0418,  ...,  0.0224,  0.3544,  0.1434],\n",
      "         [-0.2636, -0.1345, -0.0062,  ..., -0.1854,  0.2127,  0.0444]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionv1(config)\n",
    "projection_output = mha.forward(x)\n",
    "print(projection_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66899f",
   "metadata": {},
   "source": [
    "Look at the dimensios of query,key and values weight matrices.They are designed to include the number of heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40042e61",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Adjust the the output of the activation to have zero mean and a variance of 1. Numerical underflow and overflow are common problems while training deep learning networks. Numbers with uneven scales will not lead ot smooth backpropagation and hence trianing of deep networks will be difficult. By normalizng the values to zero mean and unit variance, we can speed up the convergence of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfa2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5976, -0.9168, -0.9624,  0.1854],\n",
      "         [-0.7473,  0.5628, -0.8835,  2.3138],\n",
      "         [-0.3804, -0.3160, -0.0926,  1.0782]],\n",
      "\n",
      "        [[-1.5180,  0.4197, -1.1423,  0.1079],\n",
      "         [ 1.2017, -1.3224,  1.7539,  0.1658],\n",
      "         [ 0.3813,  1.2058,  0.6612, -1.0390]]])\n",
      "tensor([[[-0.0539, -0.7481, -0.8472,  1.6491],\n",
      "         [-0.8229,  0.1953, -0.9287,  1.5563],\n",
      "         [-0.7667, -0.6576, -0.2792,  1.7034]],\n",
      "\n",
      "        [[-1.2077,  1.1685, -0.7469,  0.7862],\n",
      "         [ 0.6420, -1.5130,  1.1135, -0.2425],\n",
      "         [ 0.0952,  1.0895,  0.4328, -1.6176]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch = 2\n",
    "context_window = 3\n",
    "d_model = 4\n",
    "\n",
    "embedding = torch.randn(batch, context_window, d_model)\n",
    "print(embedding)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "normalized = layer_norm(embedding)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e68158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[-0.5728,  0.3114,  0.0723],\n",
      "        [-0.5332,  0.4498,  0.3023]])\n",
      "Var tensor([[0.2819, 2.2072, 0.4649],\n",
      "        [0.8866, 1.8291, 0.9168]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.mean(embedding,dim=-1)}\")\n",
    "print(f\"Var {torch.var(embedding,dim=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb184a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[0., 0., 0.],\n",
      "        [0., -0., -0.]], grad_fn=<RoundBackward0>)\n",
      "Var  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.round(torch.mean(normalized,dim=-1))}\")\n",
    "print(f\"Var  {torch.round(torch.var(normalized,dim=-1))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e86883",
   "metadata": {},
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd4e5219-2dcf-4abc-9b7d-af0a7f42195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d246f4",
   "metadata": {},
   "source": [
    "### Activation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ea3e5",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20638b56-bd41-4333-a3a3-dc322829c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = LayerNorm(config.d_model, bias=config.bias)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln2 = LayerNorm(config.d_head, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b5704f8-5df6-4589-bd2e-637674ed8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embdgs = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embdgs   = nn.Embedding(config.context_window, config.d_model)\n",
    "        self.droput       = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "        [TransformerBlock(config) for _ in config.n_layers]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(config.d_head)\n",
    "        self.out_head = nn.Linear(config.d_head, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_length = x.shape\n",
    "        token_embds = self.token_embdgs(x)\n",
    "        pos_embds = self.pos_embds(torch.arange(seq_length, device=x.device))\n",
    "        x = token_embds + pos_embds\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "                                   \n",
    "                                   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41971474-76f1-4d01-a90c-020aa16ed7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
