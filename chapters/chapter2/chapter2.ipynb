{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f85593e",
   "metadata": {},
   "source": [
    "# Chapter 2 - Transformer Architecture\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "At the heart of any large language model lies the transformer architecture. Introduced in the paper Attention is all you need by {cite:p}`vaswani2023attentionneed`, transformers is a set of two deep neural networks, an encoder and a decoder connected in a cascading manner to perform the translation task. Both encoders and decoders translate their inputs into an internal representation which embodies the meaning of the given text. Attention mechanism is used to create these representation. In a transalation example,\n",
    "the encoder converts the whole input english text into an embedded vector,an internal represenation which captures the context of these words. The decoder during training, takes the translated german sentence, encode it into its internal representation,further uses embedded vector from encoder and translates one word a time. The below figure {numref}`Transformer-Architecture` shows the components of encoder and decoder. \n",
    "\n",
    "\n",
    " ```{figure} ../../images/chapter2/Transformer-full.png\n",
    "---\n",
    "height: 750px\n",
    ":name: Transformer-Architecture\n",
    "---\n",
    "Transformer Architecture\n",
    "```\n",
    "\n",
    "There are three types of transformers which evolved quickly after this paper was published\n",
    "\n",
    "1. Encoder - Decoder transformers\n",
    "    They include both encoders and decoders. Translation models uses these architecture.T5, BART etc. Their pre-training is task dependent. Tasks that involve both understanding and generating data. They first encode an input sequence into an internal represenation and then decode this represenation into an output sequence.\n",
    "\n",
    "2. Encoder only Transformers  \n",
    "    Models like BERT are encoder only transfomer.Their task involves only understanding. They are trained to do masked word prediction. Given a sentence, one of the word will be masked. Encoder only models are heavily used in classification tasks.\n",
    "    \n",
    "3. Decoder only Transformers\n",
    "    In this book we will be looking at decoder only models. Decoder only models are trained to perform text generation. These models are also called as autoregressive models.\n",
    "    \n",
    "\n",
    "The secret sauce in encoder / decoder architecture is the multi-head attention module. It is this attention mechanism module which claims to provide better contextual information and long term dependency features present in the input text data to the model. Let us lookat the following two examples\n",
    "\n",
    "1. The river bank is not accessible to the tourist.\n",
    "2. The robbers had planned a heist of major banks in the city.\n",
    "\n",
    "\n",
    "Both the example sentences have the word \"bank\". The context in the first sentence is \"river\" and the second sentence is \"heist\".  We want the model to treat the word \"bank\" with respect to their context in the sentence. Another example,\n",
    "\n",
    "1. I went to the library yesterday, there i forgot my book. I am returning there today.\n",
    "\n",
    "As an English reader, if I ask you a question \"Were am I returning?\", you know that I am returning to the library. The previous sentence has the clue. This is a trivial example for long term dependency.\n",
    "\n",
    "\n",
    "This is where attention mechanism comes to rescue. Using attention mechanism we inject contextual information and long term dependency in to the model. We saw word embeddings in the previous chapter. Each word or a token had a unique position in the embedding space. However as we saw in the previous examples, these embeddings need to account for the contextual information in the input text. Attention mechanism adds these contextual information to the vector representaion of the token. More about attention in the subsequent sections.\n",
    "\n",
    "\n",
    "\n",
    "ALl the examples in this book are decoder only architecture. It is the widely used architecture for building causal LLMs.\n",
    "\n",
    "In this chapter we will see in detail about the components inside a decoder only transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cdf39",
   "metadata": {},
   "source": [
    "## Decoder only Transformers\n",
    "\n",
    "\n",
    "The below figure {numref}`GPT2` depicts the a GPT2 like decoder only transformer architecture It is a decoder only transfomer. As you can see there is no encoder module.\n",
    "\n",
    "\n",
    "\n",
    " ```{figure} ../../images/chapter2/GPT2.drawio.png\n",
    "---\n",
    "height: 750px\n",
    ":name: GPT2\n",
    "---\n",
    "GPT2 Like Architecture\n",
    "```\n",
    "\n",
    "The input, combined word and position embeddings is fed to a series of transformer blocks. The final transformer block output is fed to a linear layer producing logits. This logits is further passed through a softmax functiont to get the probability distribution over the vocabulary.\n",
    "\n",
    "The transformer block consist of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d21f9e",
   "metadata": {},
   "source": [
    "## Scaled Self-Attention\n",
    "\n",
    "Sometime while performing average of numbers, we assign weights to different numbers; this is weighted averaging. Varied attention is given to different numbers.In the example below we show a weighted average example by restricting the contribution of the three entities, to ten, thirty and sixty percentages. Let us take some liberty here for the sake for quickly grasping the concept of self-attention. Instead of calling these as weights, lets call them as attention. Rewriting the previous sentence, we have restricted the attention the average function can give to these three entities as 10, 30 and 60 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4a646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Normal 103.333     Weighted 35.00\n"
     ]
    }
   ],
   "source": [
    "weights  = (0.1, 0.3, 0.6)\n",
    "entities = (90.,120.,100.)\n",
    "\n",
    "normal_average = sum(entities) / len(entities)\n",
    "\n",
    "weighted_sum   = (x*y for x,y in zip(weights, entities))\n",
    "weighted_average = sum(weighted_sum) / len(entities)\n",
    "\n",
    " \n",
    "print(f\"Average Normal {normal_average:.3f} \\\n",
    "    Weighted {weighted_average:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6765b7",
   "metadata": {},
   "source": [
    "So, what is self attention. Say we have six tokens in our input, \"The boat left the river bank.\" \n",
    "For each token we have a vector represenation in the form of embedding. If we take a dot product of the vectors for \"The\" and \"boat\", it gives us the strength of their relationship. Refer to previous chapter for vector represenation of words. So say we have an initial embedding for word \"The\". Now we do a dot product of word \"the\" and \"boat\", followed by dot product of \"The\" and \"left\" and so on. Let us call them attention weights, the strength of relationship between those words. \n",
    "\n",
    "When we start training a transformer for a language task, we choose to initialize these word embeddings in a random fashion leaving us with no control over the scale of the embedding values. Attention paper proposes to scale these values using square root of the embedding dimension. Hence we call this attention as scaled attention. After scaling we normalize normalize these weights to fall between zero and one. Normalizing the values help in stabilized training of LLM. \n",
    "\n",
    "Finally we will enrich the embedding representation of each word with these scaled normalized attention weights. We take the embedding for the word \"The\", we multiply it with attention weight for \"The\". The embedding for \"The\" is now enriched with its relationship with itself. To this add the next enriched vector, the embedding vector for \"The\" multipled by attention weight for the word \"boat\" and so on. The final vector called context vector for the word \"The\" is thus formed by addition. Through this weighted sum, the context vector for each word is now enriched by their strength of relationship with their neighbors. The input is enriched by its constituents and hence the name self-attention.\n",
    "\n",
    "To sum up scaled self-attention, the input is enriched by its constituents and hence self, the contribution of the words in the input to each other is the attention and finally these attention weights are scaled by square root of embedding dimension and hence scaled is used in naming this.\n",
    "\n",
    "\n",
    "```{note}\n",
    "dot product explanation comes here.\n",
    "\n",
    "```\n",
    "\n",
    "To demonstrate this concept, let us create some dummy word embeddings. For the purpose of illustration we we will stick with word tokens and not their integer encoding. In the previous chapter we studied about integer encoding followed by token embedding. For bruety purpose let us skip directly to token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6236a71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33480422,  0.66683943, -0.81312097, -0.23173285],\n",
       "       [-1.00952375,  0.50433631, -0.61497028, -0.17526152],\n",
       "       [-0.83232717,  0.41581272, -0.50702767, -0.14449875],\n",
       "       [-0.36488208,  0.18228722, -0.22227475, -0.06334649],\n",
       "       [-0.74279219,  0.37108297, -0.45248576, -0.12895475],\n",
       "       [-0.20128171,  0.10055601, -0.12261452, -0.03494414]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_tokens = [\"The\", \"boat\", \"left\", \"the\", \"river\", \"bank\"]\n",
    "embd_dim =4\n",
    "\n",
    "generate_embdgs = np.random.normal(size=(embd_dim,))\n",
    "token_embedding = np.array([generate_embdgs * np.random.rand() \\\n",
    "                            for i in range(len(input_tokens))])\n",
    "token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014411d",
   "metadata": {},
   "source": [
    "With these embeddings, now let us calcuate the reltionship strength of firt word \"The\" with all the words in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88aa3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strength betwen 'The'         and 'The' 2.94\n",
      "Strength betwen 'The'         and 'boat' 2.22\n",
      "Strength betwen 'The'         and 'left' 1.83\n",
      "Strength betwen 'The'         and 'the' 0.80\n",
      "Strength betwen 'The'         and 'river' 1.64\n",
      "Strength betwen 'The'         and 'bank' 0.44\n"
     ]
    }
   ],
   "source": [
    "first_word_initial_vector = token_embedding[0]\n",
    "\n",
    "attention_scores_first_word = np.zeros(len(input_tokens),)\n",
    "\n",
    "for idx, each_word_vector in enumerate(token_embedding):\n",
    "    attention_scores_first_word[idx] = \\\n",
    "            np.dot(first_word_initial_vector, each_word_vector)\n",
    "\n",
    "for i in range(len(attention_scores_first_word)):\n",
    "    print(f\"Strength betwen '{input_tokens[0]}' \\\n",
    "        and '{input_tokens[i]}' {attention_scores_first_word[i]:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a462138",
   "metadata": {},
   "source": [
    "The stength score array now has the relationship strengths. Using we can now created a weighted sum for the word \"The\". Let us normalize these scores using softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c2f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights [0.2919817  0.20403941 0.16785268 0.10029128 0.15208529 0.08374964]         sum of weights 1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "attention_scores_first_word = attention_scores_first_word * (1/np.sqrt(embd_dim))\n",
    "attention_weights_first_word = \\\n",
    "    softmax(attention_scores_first_word)\n",
    "\n",
    "print(f\"Attention weights {attention_weights_first_word} \\\n",
    "        sum of weights {sum(attention_weights_first_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93c2cb",
   "metadata": {},
   "source": [
    "Normalized attention weights sums up to 1.0. Softmax is useful in dealing with extreme values. Let us try to see the difference in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d36e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values [1.39336480e-05 4.77725073e-05 9.95260569e-01 0.00000000e+00\n",
      " 4.67772468e-03]\n",
      "Softmax Normalization [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray([0.14,0.48,10000,0,47])\n",
    "x_normalized = x / sum(x)\n",
    "print(f\"Normalized values {x_normalized}\")\n",
    "print(f\"Softmax Normalization {softmax(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d7b91",
   "metadata": {},
   "source": [
    "With softmax we get more numerical stability, so we can avoid underflow and overflow problems during gradient calcualtions and backprobagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e4e8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90184891,  0.45054428, -0.54937814, -0.15656829])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_context_vector = np.zeros(first_word_initial_vector.shape)\n",
    "\n",
    "for idx, score in enumerate(attention_weights_first_word):\n",
    "    first_word_context_vector += token_embedding[idx] * score\n",
    "\n",
    "first_word_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564da839",
   "metadata": {},
   "source": [
    "### Vectorization for all the words\n",
    "\n",
    "Repeate the same process for each word in the squence. Using matrix operation we can vectorize attention calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ed6e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.47062148, 1.11224349, 0.91701703, 0.40200909, 0.81837181,\n",
       "        0.22176227],\n",
       "       [1.11224349, 0.84119918, 0.69354775, 0.30404288, 0.61894154,\n",
       "        0.16772068],\n",
       "       [0.91701703, 0.69354775, 0.57181284, 0.25067577, 0.51030187,\n",
       "        0.13828152],\n",
       "       [0.40200909, 0.30404288, 0.25067577, 0.1098932 , 0.22371012,\n",
       "        0.06062093],\n",
       "       [0.81837181, 0.61894154, 0.51030187, 0.22371012, 0.45540775,\n",
       "        0.12340632],\n",
       "       [0.22176227, 0.16772068, 0.13828152, 0.06062093, 0.12340632,\n",
       "        0.03344063]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "attention_scores = np.dot(token_embedding, token_embedding.T)\n",
    "attention_scores = attention_scores * (1/math.sqrt(embd_dim))\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fefa553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47062148, 1.11224349, 0.91701703, 0.40200909, 0.81837181,\n",
       "       0.22176227])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_first_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e542ed",
   "metadata": {},
   "source": [
    "Compare the first row with our prevoius strength_score array.The first row here represents the strength score between word \"The\" and all the other words in the sequence. The second row represents the relationship between word \"boat\" and all hte other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef806235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2919817 , 0.20403941, 0.16785268, 0.10029128, 0.15208529,\n",
       "        0.08374964],\n",
       "       [0.25869083, 0.19727315, 0.17019382, 0.11528796, 0.1579584 ,\n",
       "        0.10059583],\n",
       "       [0.24118358, 0.19288402, 0.17077624, 0.12386808, 0.16058818,\n",
       "        0.11069989],\n",
       "       [0.19761292, 0.17917159, 0.16986039, 0.14755401, 0.1653412 ,\n",
       "        0.1404599 ],\n",
       "       [0.23253037, 0.19048826, 0.17087818, 0.12829844, 0.16175078,\n",
       "        0.11605397],\n",
       "       [0.18338032, 0.17373318, 0.16869317, 0.15608815, 0.1662024 ,\n",
       "        0.15190277]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = softmax(attention_scores,axis=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff5f6f",
   "metadata": {},
   "source": [
    "We have all the attention weights. You can compare the attention weights for the first word \"The\" with the first row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4be68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2919817 , 0.20403941, 0.16785268, 0.10029128, 0.15208529,\n",
       "       0.08374964])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_first_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc983d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = np.dot(attention_weights, token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da9908",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Another important concept to grasp before we move further is Masking. Let us see what is masking and why need masking. We perform masking to avoid data leakage. While we train the transformer to perform the next word prediction, we should avoid giving any information about the next word it is supposed to predict.In the previous example, while doing the weighted sum addition, we enriched the vector representation for word \"The\" w the ith all the other words in the sequence. While we expect the transformer to predict, \"Boat\" after \"The\", we have introduced leakage by providing information about \"Boat\" to the model while training.\n",
    "\n",
    "Remember the context vector creation. We enrich the original embedding vector with attention scores. We dont want to enrich word \"The\" with the scores of \"boat\" before the model sees the word \"boat\". In the next step, when the model sees the word \"The\" and \"boat\", we will enrich it with the scores from \"boat\". We can do this by using a mask, where we will mask the upper triangle of our attenion score matrix.\n",
    "\n",
    "Let us create a mask matrix of the same shape as our attention_weights matrix. Set the lower triangle part of this matrix as 1 and the upper triangle as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1a6990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False],\n",
       "       [ True,  True, False, False, False, False],\n",
       "       [ True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mask = np.zeros_like(attention_scores, dtype=bool)\n",
    "mask[np.tril_indices_from(attention_scores)] = True\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f9377",
   "metadata": {},
   "source": [
    "Apply this mask over attention_scores matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "323d54a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.47062148, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.11224349, 0.84119918, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.91701703, 0.69354775, 0.57181284, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.40200909, 0.30404288, 0.25067577, 0.1098932 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.81837181, 0.61894154, 0.51030187, 0.22371012, 0.45540775,\n",
       "        0.        ],\n",
       "       [0.22176227, 0.16772068, 0.13828152, 0.06062093, 0.12340632,\n",
       "        0.03344063]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_masked = attention_scores * mask\n",
    "attention_scores_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4029653",
   "metadata": {},
   "source": [
    "We should have a masked attention scores matrix, where all the upper triangular entries are set to zero. Let us set those zero values to minus infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52dc1d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.47062148,       -inf,       -inf,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [1.11224349, 0.84119918,       -inf,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [0.91701703, 0.69354775, 0.57181284,       -inf,       -inf,\n",
       "              -inf],\n",
       "       [0.40200909, 0.30404288, 0.25067577, 0.1098932 ,       -inf,\n",
       "              -inf],\n",
       "       [0.81837181, 0.61894154, 0.51030187, 0.22371012, 0.45540775,\n",
       "              -inf],\n",
       "       [0.22176227, 0.16772068, 0.13828152, 0.06062093, 0.12340632,\n",
       "        0.03344063]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_masked[attention_scores_masked == 0]= -np.inf\n",
    "attention_scores_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd2b92",
   "metadata": {},
   "source": [
    "Softmax function, will handle the negative infinity value and set it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d191fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.56734926, 0.43265074, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.39875346, 0.31889888, 0.28234766, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.28466325, 0.25809835, 0.24468547, 0.21255293, 0.        ,\n",
       "        0.        ],\n",
       "       [0.26305946, 0.21549761, 0.19331291, 0.14514285, 0.18298717,\n",
       "        0.        ],\n",
       "       [0.18338032, 0.17373318, 0.16869317, 0.15608815, 0.1662024 ,\n",
       "        0.15190277]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_masked = softmax(attention_scores_masked,axis=-1)\n",
    "attention_weights_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244627",
   "metadata": {},
   "source": [
    "Finally using this masked attention weights, we calculate the context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d24ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33480422,  0.66683943, -0.81312097, -0.23173285],\n",
       "       [-1.19407139,  0.59653233, -0.72739093, -0.20730049],\n",
       "       [-1.08919942,  0.54414056, -0.66350621, -0.18909386],\n",
       "       [-0.92174124,  0.46048206, -0.56149592, -0.16002176],\n",
       "       [-0.91846389,  0.45884477, -0.55949946, -0.15945279],\n",
       "       [-0.77155538,  0.38545244, -0.47000739, -0.13394828]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = np.dot(attention_weights_masked, token_embedding)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59deb20",
   "metadata": {},
   "source": [
    "The context vector for the word \"The\" is not enriched by other words in the input. The last word has the enrichment of all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0d145-f78b-4e30-94a4-c47793aa7662",
   "metadata": {},
   "source": [
    "### Scaled dot product self-attention in Pytorch\n",
    "\n",
    "\n",
    "The below code implements the attention mechanism in Pytorch. SLLMConfig class is a datastructure to store the model parameters.\n",
    "\n",
    "We implement the attention mechanism in the class SingleHeadAttention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a34105",
   "metadata": {},
   "source": [
    "Here is the torch implementation \"torch.nn.functional.scaled_dot_product_attention\" of scaled dot product attention. Going forward we will be using this version\n",
    "inside our source code.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "\n",
    "<Need to rewrite it>\n",
    "\n",
    "The documentation warns that its a beta version and subject to change. In addition to standard implementation similar to our example, this also implements three other versions,\n",
    "\n",
    "1. Flash attention {cite}`dao2023flashattention2fasterattentionbetter`\n",
    "\n",
    "The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation.\n",
    "\n",
    "The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation.\n",
    "\n",
    "2. Memory effiecient\n",
    "\n",
    "3. C++ implementation\n",
    "\n",
    "Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:\n",
    "\n",
    "        torch.nn.attention.sdpa_kernel(): A context manager used to enable or disable any of the implementations.\n",
    "\n",
    "        torch.backends.cuda.enable_flash_sdp(): Globally enables or disables FlashAttention.\n",
    "\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(): Globally enables or disables Memory-Efficient Attention.\n",
    "\n",
    "        torch.backends.cuda.enable_math_sdp(): Globally enables or disables the PyTorch C++ implementation.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3348,  0.6668, -0.8131, -0.2317],\n",
       "        [-1.1941,  0.5965, -0.7274, -0.2073],\n",
       "        [-1.0892,  0.5441, -0.6635, -0.1891],\n",
       "        [-0.9217,  0.4605, -0.5615, -0.1600],\n",
       "        [-0.9185,  0.4588, -0.5595, -0.1595],\n",
       "        [-0.7716,  0.3855, -0.4700, -0.1339]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "torch_tensor = torch.tensor(token_embedding)\n",
    "\n",
    "attn_torch = scaled_dot_product_attention(\n",
    "        query = torch_tensor\n",
    "       ,key   = torch_tensor\n",
    "       ,value = torch_tensor\n",
    "       ,attn_mask=None, dropout_p=0.0, is_causal=True, scale=None\n",
    ")\n",
    "\n",
    "print(attn_torch.shape)\n",
    "attn_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96bdc224-0c19-4854-8dad-c06dec51fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(4752)\n",
    "\n",
    "@dataclass\n",
    "class SLLMConfig:\n",
    "    # Embedding dimension\n",
    "    d_model: int = 512\n",
    "    # Query key Value projection dimension\n",
    "    d_head: int  = 128\n",
    "    # bias for query,key and value projection matrices\n",
    "    bias: bool = False\n",
    "    dropout: int = 0.0\n",
    "    # Number of input tokens\n",
    "    context_window: int = 32\n",
    "    # Number of attention heads\n",
    "    n_heads: int = 4\n",
    "\n",
    "    \n",
    "config = SLLMConfig()\n",
    "assert config.d_model % config.n_heads == 0\n",
    "\n",
    "    \n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements weighted self attention\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head, bias=config.bias)\n",
    "\n",
    "        self.attn_drop = config.dropout\n",
    "        \n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        \n",
    "        context_vector = F.scaled_dot_product_attention(\n",
    "                                query = q\n",
    "                               ,key   = k\n",
    "                               ,value = v\n",
    "                               ,attn_mask=None\n",
    "                               ,dropout_p=self.attn_drop\n",
    "                               ,is_causal=True, scale=None)\n",
    "\n",
    "\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787d3ac",
   "metadata": {},
   "source": [
    "SingleHeadAttention class should look straightforward if you were following us till now. There are three trainable weights: Wq, Wk and Wv linear layers. Inside the forward function we see that these matrices are used to project the input token embeddings to three variables query, key and value. Let us spend some time understanding the importance of these trainable weights.\n",
    "\n",
    "Query, Key and Value terminology comes from recommonder system literature and subsequently RNN based transalation systems. Query refers to \"what are we looking for\" and Key symbolizes \"what is available\". We calculate the attention score between Query and Key. The context vector for tokens in the query is enriched by its relationship with tokens in the key.\n",
    "\n",
    "There is another way to look at this Query, Key and Value. Imaginge you are working on a curve fitting problem. A graph with lot of curves, a quadratic equation will be more suitable than a linear equation. The difference is in the number of coeffiecients. The linear equation has a single coeffiecient, the slope. However quadratic equation will have multiple coeffiecients and will be able to do better in fitting the curve.\n",
    "\n",
    "Take this analogy. We have now three different coeffiecients for our input imbeddings to extract and learn more features for our text input. During the training of our LLM, we have given the model more options, it can do gradient updates during back probagation to the following\n",
    "\n",
    "1. Embedding Layer\n",
    "2. Query weight matrix\n",
    "3. Key weight matrix\n",
    "4. Value weight matrix.\n",
    "\n",
    "\n",
    "Compare this if we dont have query, key and value and we use only the token embedding layer to tweak as we build our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c0ba31-b97c-480a-8764-7089cf20af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7320, -1.3121, -1.1089,  ..., -0.3185, -0.0318, -0.0945],\n",
       "        [-0.4218, -0.7940, -0.9174,  ...,  0.2992,  0.3103, -0.0832],\n",
       "        [-0.5216, -0.7165, -0.9867,  ...,  0.1792,  0.2390,  0.0536],\n",
       "        ...,\n",
       "        [-0.8741, -0.9610, -1.1674,  ...,  0.0701,  0.4750,  0.0293],\n",
       "        [-0.8710, -0.9496, -1.1936,  ...,  0.0768,  0.4816,  0.0163],\n",
       "        [-0.8807, -0.9522, -1.1772,  ...,  0.0664,  0.4743,  0.0258]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.normal(0.5,0.3,size=(config.context_window, config.d_model))\n",
    "\n",
    "sha = SingleHeadAttention(config)\n",
    "attn = sha.forward(x)\n",
    "\n",
    "print(attn.shape)\n",
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639f769",
   "metadata": {},
   "source": [
    "We have the context vectors for our input token embeddings. In transfomer architecture, we can have multiple attention module. Hence the name, single head. One head provides a context vectors. Mutliple heads can provide multiple context vectors and finally we concatenate these context vectors. The concatenated context vectors serve as input to the next layer.\n",
    "\n",
    "Below is an example of multi head implementation. \"n_heads\" in config defines the number of heads needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e55bd9-23e7-4310-ba5d-eb97325c1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config) for _ in range(config.n_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attentions = []\n",
    "        for head in self.heads:\n",
    "            c_vector = head(x)\n",
    "            attentions.append(c_vector)\n",
    "\n",
    "        context_vector = torch.cat(attentions, dim=-1)\n",
    "        context_projected = self.projection_out(context_vector)\n",
    "        return context_projected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373a922",
   "metadata": {},
   "source": [
    "self.heads stores the list of singlehaead attention, number decided by n_heads. This is followed by a projection\n",
    "layer.All the concatenated context vectors are transformed through this projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe0c902-6bfd-4e84-a345-95d98d7c63eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4857,  0.1058,  0.3330,  ...,  0.0266, -0.2390, -0.5778],\n",
       "         [-0.3779, -0.1576,  0.2441,  ...,  0.1952, -0.2516, -0.4957],\n",
       "         [-0.3296, -0.3279,  0.2102,  ...,  0.1217, -0.2000, -0.5549],\n",
       "         ...,\n",
       "         [-0.3132, -0.1365,  0.0805,  ..., -0.0436, -0.1455, -0.3998],\n",
       "         [-0.2954, -0.1391,  0.0757,  ..., -0.0342, -0.1323, -0.4010],\n",
       "         [-0.2838, -0.1309,  0.0842,  ..., -0.0362, -0.1253, -0.4080]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(config)\n",
    "x = torch.normal(0.5,0.3,size=(1, config.context_window, config.d_model))\n",
    "projected_output = mha.forward(x)\n",
    "projected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e23317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)    \n",
    "        \n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "\n",
    "        self.attn_drop  = config.dropout\n",
    "        self.n_heads    = config.n_heads\n",
    "        self.d_head     = config.d_head\n",
    "        self.__init_weights()\n",
    "\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, length, d = x.shape\n",
    "        is_causal = True\n",
    "        \n",
    "        if not self.train:\n",
    "            is_causal = False\n",
    "            self.attn_drop = 0.0\n",
    "        \n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        \n",
    "        q = q.view(batch, length, self.n_heads, self.d_head)\n",
    "        k = k.view(batch, length, self.n_heads, self.d_head)\n",
    "        v = v.view(batch, length, self.n_heads, self.d_head)\n",
    "\n",
    "        context_vector = scaled_dot_product_attention(\n",
    "                                query = q\n",
    "                               ,key   = k\n",
    "                               ,value = v\n",
    "                               ,attn_mask=None\n",
    "                               ,dropout_p=self.attn_drop\n",
    "                               ,is_causal=True, scale=None)\n",
    "\n",
    "        context_vector = context_vector.contiguous().view(batch, length, self.d_head * self.n_heads)\n",
    "        output = self.projection_out(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c936443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3404, -0.2685,  0.0146,  ..., -0.3770, -0.1301, -0.0377],\n",
      "         [ 0.3261, -0.2009, -0.0938,  ..., -0.5215, -0.2404,  0.1223],\n",
      "         [ 0.4130, -0.1474, -0.1370,  ..., -0.2388,  0.0324, -0.0026],\n",
      "         ...,\n",
      "         [ 0.1320, -0.3051, -0.2318,  ..., -0.5390, -0.1414, -0.3503],\n",
      "         [ 0.5967, -0.1873, -0.1388,  ..., -0.2002, -0.1709, -0.0088],\n",
      "         [ 0.1941, -0.4433,  0.0967,  ..., -0.2179, -0.0894,  0.0209]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionv1(config)\n",
    "projection_output = mha.forward(x)\n",
    "print(projection_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66899f",
   "metadata": {},
   "source": [
    "Look at the dimensios of query,key and values weight matrices.They are designed to include the number of heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40042e61",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Adjust the the output of the activation to have zero mean and a variance of 1. Numerical underflow and overflow are common problems while training deep learning networks. Numbers with uneven scales will not lead ot smooth backpropagation and hence trianing of deep networks will be difficult. By normalizng the values to zero mean and unit variance, we can speed up the convergence of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecfa2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1266,  1.6377, -0.0248,  0.7473],\n",
      "         [ 1.3856,  1.2638, -0.2143,  0.7376],\n",
      "         [ 0.5705,  1.0342, -0.6157, -0.2444]],\n",
      "\n",
      "        [[-1.8936, -0.6040,  1.6915,  0.8249],\n",
      "         [-1.9924, -1.1201,  0.6283, -0.8409],\n",
      "         [-2.1389, -0.3379,  1.4214,  0.1690]]])\n",
      "tensor([[[-0.7569,  1.5534, -0.9885,  0.1920],\n",
      "         [ 0.9394,  0.7463, -1.5977, -0.0881],\n",
      "         [ 0.5904,  1.3027, -1.2317, -0.6614]],\n",
      "\n",
      "        [[-1.3871, -0.4448,  1.2325,  0.5993],\n",
      "         [-1.2304, -0.3061,  1.5467, -0.0102],\n",
      "         [-1.4993, -0.0909,  1.2848,  0.3054]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch = 2\n",
    "context_window = 3\n",
    "d_model = 4\n",
    "\n",
    "embedding = torch.randn(batch, context_window, d_model)\n",
    "print(embedding)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "normalized = layer_norm(embedding)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e68158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[ 0.6217,  0.7932,  0.1862],\n",
      "        [ 0.0047, -0.8312, -0.2216]])\n",
      "Var tensor([[0.5704, 0.5302, 0.5651],\n",
      "        [2.4973, 1.1873, 2.1804]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.mean(embedding,dim=-1)}\")\n",
    "print(f\"Var {torch.var(embedding,dim=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb184a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tensor([[0., -0., 0.],\n",
      "        [-0., 0., 0.]], grad_fn=<RoundBackward0>)\n",
      "Var  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean {torch.round(torch.mean(normalized,dim=-1))}\")\n",
    "print(f\"Var  {torch.round(torch.var(normalized,dim=-1))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e86883",
   "metadata": {},
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd4e5219-2dcf-4abc-9b7d-af0a7f42195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.d_head, config.d_head, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d246f4",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "\n",
    "Non linear transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ea3e5",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20638b56-bd41-4333-a3a3-dc322829c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = LayerNorm(config.d_model, bias=config.bias)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln2 = LayerNorm(config.d_head, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b7594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embdgs = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embdgs   = nn.Embedding(config.context_window, config.d_model)\n",
    "        self.droput       = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_embds = self.token_embdgs(x)\n",
    "        pos_embds = self.pos_embds(torch.arange(seq_length, device=x.device))\n",
    "        x = token_embds + pos_embds\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b5704f8-5df6-4589-bd2e-637674ed8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_block    = EmbeddingsBlock(config)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.n_layers)]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(config.d_head)\n",
    "        self.out_head = nn.Linear(config.d_head, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_length = x.shape\n",
    "        x = self.embedding_block(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "                                   \n",
    "                                   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa656b1",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "1. Rotary Embeddings\n",
    "2. Grouped Query Attention\n",
    "3. RMSNorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789f054",
   "metadata": {},
   "source": [
    "## Rotary Embeddings\n",
    "\n",
    "Before we see the details of rotary embeddings, let us understand a little bit about vector rotation in 2d spaces. \n",
    "Say we have a point $(x_1,y_1)$, if we rotate this vector anticlockwise by $\\theta$ degrees, the new co-ordinates $(x_2,y_2)$ are\n",
    "\n",
    "$$\n",
    "x_2=\\cos{\\theta}{x_1} - \\sin{\\theta}{y_1} \\\\\n",
    "y_2=\\sin{\\theta}{x_1} + \\cos{\\theta}{y_1}\n",
    "$$\n",
    "\n",
    "We can perform the above above operation as matrix multiplication.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos{\\theta} & -\\sin{\\theta} \\\\\n",
    "\\sin{\\theta} &  \\cos{\\theta} \\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Rotation only changes the orientation of vector and not its magnitude. This description is based on the blog \n",
    "https://matthew-brett.github.io/teaching/rotation_2d.html\n",
    "\n",
    "The below python code snippet demonstrated vector rotation in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0dd59d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random 2D tensor([[0.8357],\n",
      "        [0.9378]]) magnitude 1.26\n",
      "x rotated by 1 degress tensor([[-0.3376],\n",
      "        [ 1.2099]]) magnitude 1.26\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(55)\n",
    "x = torch.rand((2, 1))\n",
    "print(f\"A random 2D {x} magnitude {torch.norm(x):.2f}\")\n",
    "\n",
    "theta = torch.tensor(1)\n",
    "rotation_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)]\n",
    "                                   ,[torch.sin(theta), torch.cos(theta)]]\n",
    "                                  )\n",
    "x_rotated = rotation_matrix @ x \n",
    "\n",
    "print(f\"x rotated by {theta} degress {x_rotated} magnitude {torch.norm(x_rotated):.2f}\")\n",
    "\n",
    "# Plot the vectors\n",
    "ax = plt.axes()\n",
    "\n",
    "# extract numpy arrays\n",
    "x_numpy  = x.squeeze(0).detach().numpy()\n",
    "xr_numpy = x_rotated.squeeze(0).detach().numpy()\n",
    "\n",
    "# reshape column vectors to row vectors and \n",
    "# remove extra dimension\n",
    "x_numpy =  x_numpy.reshape(1,2).squeeze(0)\n",
    "xr_numpy = xr_numpy.reshape(1,2).squeeze(0)\n",
    "\n",
    "ax.arrow(0,0,*x_numpy,width=0.01,color=\"blue\")\n",
    "ax.annotate(\"Original x\", xy=(x_numpy/2), xytext=(x_numpy/2),\n",
    "            arrowprops=None)\n",
    "\n",
    "\n",
    "ax.arrow(0,0,*xr_numpy,width=0.01,color=\"green\")\n",
    "ax.annotate(\"Rotated x\", xy=(xr_numpy/2), xytext=(xr_numpy/2),\n",
    "            arrowprops=None)\n",
    "    \n",
    "plt.xlim(-1.,1.3)\n",
    "plt.ylim(-1.,1.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65325a4c",
   "metadata": {},
   "source": [
    "As you can see the vector x, is rotated by an angle of $\\theta=1$. The orginal is shown by blue colored vector and the rotated is shown by green vector in the graph. The length of the vector remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225aa27",
   "metadata": {},
   "source": [
    "In the previous chapter we demonstrted fixed position embedding and learned position embedding. In both the cases, we created an embedding vector for position of our token and added it to our word embedding.\n",
    "In the previous section in this chapter, we saw that we used weight matrices to project the combined (token and positoin) embeddings into query, key and value matrices. \n",
    "\n",
    "Below python snippet mimics creation of a token embedding vector, followed by affine-transformation using a query weight matrix and subsequently deriving the query vector. For demonstration purposes we restrict the context length to three and embedding dimension to four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2f539228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(55)\n",
    "context_length = 3\n",
    "d_model = 4\n",
    "\n",
    "\n",
    "def get_query_matrix(context_length, d_model):\n",
    "    x = torch.rand((context_length, d_model))\n",
    "    Wq = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "    q = Wq(x)\n",
    "    return q\n",
    "\n",
    "q = get_query_matrix(context_length, d_model)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c4453",
   "metadata": {},
   "source": [
    "Below is a figure from the paper RoFormer: Enhanced Transformer with Rotary Position Embedding {cite}\n",
    "\n",
    "`su2023roformerenhancedtransformerrotary`.\n",
    "\n",
    "```{figure} ../../images/chapter2/rope.png\n",
    "---\n",
    "height: 150px\n",
    "name: rope-fig\n",
    "---\n",
    "Rotary Position Embedding\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720fc39",
   "metadata": {},
   "source": [
    "The embedding dimension is four. Each dimension is a feature. We have *two pairs of features* given our embedding dimension is four. Context length is set to three, hence we have four tokens, in position 1,2,and 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac6ac5",
   "metadata": {},
   "source": [
    "Each pair of the features will be rotated by a different $\\theta$. \n",
    "\n",
    "$$\n",
    "\\Theta=\\{\\theta_i = 10000^{{-2\\dot(i-1)}\\over{d}}:i\\in{1,2,\\dots,\\frac{d}{2}} \\}\n",
    "$$\n",
    "\n",
    "So for our embedding dimension size of 4, we will have two $\\theta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7cc7d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0100])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_dimensions = torch.arange(0, d_model // 2, dtype=torch.float32)\n",
    "\n",
    "theta = 10000**((-2.*sliced_dimensions)/d_model)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016bb47",
   "metadata": {},
   "source": [
    "Say our embedded dimension in the example of size 4 is represented as ${x_1,x_2,x_3,x_4}$, we rotate dimensions $(x_1,x_2)$ by $\\theta=1$ and dimension $(x_3,x_4)$ by $\\theta=0.01$. Now comes the position of the token.With a context length of three, we have three positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "538b4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = torch.arange(1, context_length+1).unsqueeze(1)\n",
    "positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7150441",
   "metadata": {},
   "source": [
    "Each position will scale these $\\theta$ values as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "affacd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0100],\n",
       "        [2.0000, 0.0200],\n",
       "        [3.0000, 0.0300]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_theta = positions * theta\n",
    "m_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e4c54",
   "metadata": {},
   "source": [
    "You can see that the first row represents the scaled theta values for feature pairs $(x_1,x_2)$ and $(x_3,x_4)$. The figure from the paper explains this. In the figure the feature pair $(x_1,x_2)$ is rotated by $m\\theta_1$, where $m$ is the position index and $\\theta_1$ is the rotation angle for that feature pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e9218",
   "metadata": {},
   "source": [
    "Let us get the rotation matrix. For position, m = 1, the first token, for features $(x_1,x_2)$ the rotation matrix when multiplied by query/key vector looks like the following\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos{m\\theta} & -\\sin{m\\theta} \\\\\n",
    "\\sin{m\\theta} &  \\cos{m\\theta} \\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    ":m=1,\\theta=1.0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed87e4",
   "metadata": {},
   "source": [
    "We calculate the sine and cosine values for all the tokens and their embedding pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3f0e3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5403,  0.9999],\n",
       "        [-0.4161,  0.9998],\n",
       "        [-0.9900,  0.9996]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_values = torch.cos(m_theta)\n",
    "sin_values = torch.sin(m_theta)\n",
    "cos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e5d27",
   "metadata": {},
   "source": [
    "Let us proceed to create a rotation matrix. We have one rotation matrix for each token. Fill the rotation matrix with sin and cosine values we have derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "334353ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix = torch.zeros((context_length,d_model, d_model))\n",
    "rotation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7062db",
   "metadata": {},
   "source": [
    "We will create our rotation matrix in such a way that, when we multiply our query/key matrix, the correct $\\theta$ values will be applied.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cos{m\\theta} & -\\sin{m\\theta} & 0 & 0 \\\\\n",
    "\\sin{m\\theta} &  \\cos{m\\theta} & 0 & 0 \\\\\n",
    " 0 & 0 & \\cos{m\\theta} & -\\sin{m\\theta}  \\\\\n",
    " 0 & 0 & \\sin{m\\theta} &  \\cos{m\\theta} \n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4\n",
    "\\end{pmatrix}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "=\n",
    "x_1\\cdot\\cos{m\\theta} - x_1\\cdot\\sin{m\\theta} + 0 + 0 \\\\\n",
    "x_2\\cdot\\sin{m\\theta} + x_2\\cdot\\cos{m\\theta} + 0 + 0 \\\\\n",
    "0 + 0 + x_3\\cdot\\cos{m\\theta} - x_3\\cdot\\sin{m\\theta} \\\\\n",
    "0 + 0 + x_4\\cdot\\sin{m\\theta} + x_4\\cdot\\cos{m\\theta} \n",
    "$$\n",
    "\n",
    "Let us now fill our rotation matrix. We know that cosine values are present in 0th row 0th column in all the matrices, similarly in 2nd row and 2nd column. And likewise for other entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7c9838b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_matrix[:,[0,2],[0,2]]= cos_values\n",
    "rotation_matrix[:,[0,2],[1,2]]= -sin_values\n",
    "rotation_matrix[:,[1,3],[0,2]]= sin_values\n",
    "rotation_matrix[:,[1,3],[1,3]]= cos_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0b39934a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5403, -0.8415,  0.0000,  0.0000],\n",
       "         [ 0.8415,  0.5403,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0100,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0100,  0.9999]],\n",
       "\n",
       "        [[-0.4161, -0.9093,  0.0000,  0.0000],\n",
       "         [ 0.9093, -0.4161,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0200,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0200,  0.9998]],\n",
       "\n",
       "        [[-0.9900, -0.1411,  0.0000,  0.0000],\n",
       "         [ 0.1411, -0.9900,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0300,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0300,  0.9996]]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbaeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can now rotate our query and key matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4e7aeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "        self.projection_out = nn.Linear(config.n_heads * config.d_head, config.d_head)    \n",
    "        \n",
    "        self.Wq =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wk =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        self.Wv =  nn.Linear(config.d_model, config.d_head * config.n_heads, bias=config.bias)\n",
    "        \n",
    "        self.rotation_matrix = torch.zeros(config.context_window, config.d_head, config.d_head)\n",
    "        self.rotation_buffer = self.register_buffer(\"rotation_matrix\", self.rotation_matrix)\n",
    "        self.__init_rotation_matrix(config)\n",
    "        \n",
    "        self.attn_drop  = config.dropout\n",
    "        self.n_heads    = config.n_heads\n",
    "        self.d_head     = config.d_head\n",
    "        self.__init_weights()\n",
    "\n",
    "\n",
    "    def __init_weights(self):\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq.weight)\n",
    "        nn.init.xavier_uniform_(self.Wk.weight)\n",
    "        nn.init.xavier_uniform_(self.Wv.weight)\n",
    "    \n",
    "    def __init_rotation_matrix(self, config):\n",
    "        \n",
    "        positions = torch.arange(1, config.context_window + 1).unsqueeze(1)\n",
    "        sliced_dimensions = torch.arange(0, config.d_head // 2, dtype=torch.float32)\n",
    "        theta = 10000**((-2.*sliced_dimensions.float())/config.d_head)\n",
    "        m_theta = positions * theta\n",
    "        \n",
    "        sin_values = torch.sin(m_theta)\n",
    "        cos_values = torch.cos(m_theta)\n",
    "        \n",
    "        self.rotation_matrix[:,2*sliced_dimensions, 2*sliced_dimensions] = cos_values\n",
    "        self.rotation_matrix[:,2*sliced_dimensions, 2*sliced_dimensions+1] = -sin_values\n",
    "        self.rotation_matrix[:,2*sliced_dimensions+1, 2*sliced_dimensions] = sin_values\n",
    "        self.rotation_matrix[:,2*sliced_dimensions+1, 2*sliced_dimensions+1] = cos_values\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, length, d = x.shape\n",
    "        is_causal = True\n",
    "        \n",
    "        if not self.train:\n",
    "            is_causal = False\n",
    "            self.attn_drop = 0.0\n",
    "        \n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        \n",
    "        q = q.view(batch, length, self.n_heads, self.d_head)\n",
    "        k = k.view(batch, length, self.n_heads, self.d_head)\n",
    "        v = v.view(batch, length, self.n_heads, self.d_head)\n",
    "\n",
    "        context_vector = scaled_dot_product_attention(\n",
    "                                query = q\n",
    "                               ,key   = k\n",
    "                               ,value = v\n",
    "                               ,attn_mask=None\n",
    "                               ,dropout_p=self.attn_drop\n",
    "                               ,is_causal=True, scale=None)\n",
    "\n",
    "        context_vector = context_vector.contiguous().view(batch, length, self.d_head * self.n_heads)\n",
    "        output = self.projection_out(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77a2b6",
   "metadata": {},
   "source": [
    "## Llama architecture \n",
    "\n",
    "\n",
    "https://medium.com/@pranjalkhadka/llama-explained-a70e71e706e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe918eb8",
   "metadata": {},
   "source": [
    "## Mistral Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfe15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A shared input/output embedding matrix [Press and Wolf, 2016] to\n",
    "reduce memory usage for parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
