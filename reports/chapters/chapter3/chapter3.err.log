Traceback (most recent call last):
  File "/home/gopi/.local/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/usr/lib/python3/dist-packages/nbclient/client.py", line 1093, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/usr/lib/python3/dist-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/usr/lib/python3/dist-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/usr/lib/python3/dist-packages/nbclient/client.py", line 559, in async_execute
    await self.async_execute_cell(
  File "/usr/lib/python3/dist-packages/nbclient/client.py", line 854, in async_execute_cell
    self._check_raise_for_error(cell, exec_reply)
  File "/usr/lib/python3/dist-packages/nbclient/client.py", line 756, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
device = "cuda"

loss_fn = LLMLoss()

model = model.to(device)
model.eval()
batch_no = 1
for batch in train_loader:
    
    features, target = batch
    loss = batch_loss(loss_fn, features, target, model, device)
    
    print(f"Batch {batch_no} Loss {loss}")
    batch_no+=1
    
    if batch_no > 2:
        break


------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [0;32mIn[7], line 5[0m
[1;32m      1[0m device [38;5;241m=[39m [38;5;124m"[39m[38;5;124mcuda[39m[38;5;124m"[39m
[1;32m      3[0m loss_fn [38;5;241m=[39m LLMLoss()
[0;32m----> 5[0m model [38;5;241m=[39m [43mmodel[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m
[1;32m      6[0m model[38;5;241m.[39meval()
[1;32m      7[0m batch_no [38;5;241m=[39m [38;5;241m1[39m

File [0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
[1;32m   1171[0m         [38;5;28;01melse[39;00m:
[1;32m   1172[0m             [38;5;28;01mraise[39;00m
[0;32m-> 1174[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

File [0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    778[0m [38;5;28;01mif[39;00m recurse:
[1;32m    779[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 780[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    782[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    783[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    784[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    785[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    790[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    791[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    801[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
[1;32m    802[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
[1;32m    803[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
[1;32m    804[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 805[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
[1;32m    806[0m p_should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
[1;32m    808[0m [38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors[39;00m

File [0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
[1;32m   1153[0m     [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
[1;32m   1154[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(
[1;32m   1155[0m             device,
[1;32m   1156[0m             dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1157[0m             non_blocking,
[1;32m   1158[0m             memory_format[38;5;241m=[39mconvert_to_format,
[1;32m   1159[0m         )
[0;32m-> 1160[0m     [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m
[1;32m   1161[0m [43m        [49m[43mdevice[49m[43m,[49m
[1;32m   1162[0m [43m        [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m
[1;32m   1163[0m [43m        [49m[43mnon_blocking[49m[43m,[49m
[1;32m   1164[0m [43m    [49m[43m)[49m
[1;32m   1165[0m [38;5;28;01mexcept[39;00m [38;5;167;01mNotImplementedError[39;00m [38;5;28;01mas[39;00m e:
[1;32m   1166[0m     [38;5;28;01mif[39;00m [38;5;28mstr[39m(e) [38;5;241m==[39m [38;5;124m"[39m[38;5;124mCannot copy out of meta tensor; no data![39m[38;5;124m"[39m:

File [0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:314[0m, in [0;36m_lazy_init[0;34m()[0m
[1;32m    312[0m [38;5;28;01mif[39;00m [38;5;124m"[39m[38;5;124mCUDA_MODULE_LOADING[39m[38;5;124m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m os[38;5;241m.[39menviron:
[1;32m    313[0m     os[38;5;241m.[39menviron[[38;5;124m"[39m[38;5;124mCUDA_MODULE_LOADING[39m[38;5;124m"[39m] [38;5;241m=[39m [38;5;124m"[39m[38;5;124mLAZY[39m[38;5;124m"[39m
[0;32m--> 314[0m [43mtorch[49m[38;5;241;43m.[39;49m[43m_C[49m[38;5;241;43m.[39;49m[43m_cuda_init[49m[43m([49m[43m)[49m
[1;32m    315[0m [38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();[39;00m
[1;32m    316[0m [38;5;66;03m# we need to just return without initializing in that case.[39;00m
[1;32m    317[0m [38;5;66;03m# However, we must not let any *other* threads in![39;00m
[1;32m    318[0m _tls[38;5;241m.[39mis_initializing [38;5;241m=[39m [38;5;28;01mTrue[39;00m

[0;31mRuntimeError[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.
RuntimeError: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.

